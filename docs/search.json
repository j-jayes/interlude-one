[
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Jayes",
    "section": "",
    "text": "‚ÄúI enjoy some of the old and I enjoy the new. And if I can find a balance between it, that‚Äôs where I find my satisfaction.‚Äù\n‚Äì Anderson .Paak - The Dreamer (feat. Talib Kweli & Timan Family Choir)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jonathan Jayes",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n  \n    \n  \n\nLund UniversityMaster's degree, Economics 2021  Exchange semester \n    \n  \n\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \n    \n  \n\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \n    \n  \n\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jonathan Jayes",
    "section": "Experience",
    "text": "Experience\nLaboratory for the Economics of Africa‚Äôs Past | Researcher under Prof Johan Fourie| January 2020 - July 2021\nFTI Consulting | Junior Economist in Economic and Financial Consulting Practice | March 2019 - January 2020"
  },
  {
    "objectID": "content/publications.html",
    "href": "content/publications.html",
    "title": "Publications",
    "section": "",
    "text": "I have one paper that I‚Äôm proud to have published in World Development with my master‚Äôs supervisor Johan Fourie. Have a look at it below."
  },
  {
    "objectID": "content/teaching.html",
    "href": "content/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Check out my site for a set of Stata labs I taught in the Spring of 2023"
  },
  {
    "objectID": "content/teaching.html#screencasts",
    "href": "content/teaching.html#screencasts",
    "title": "Teaching",
    "section": "Screencasts",
    "text": "Screencasts\nHave a look at a set of screencasts I am working on for learning R."
  },
  {
    "objectID": "content/teaching.html#data-visualization",
    "href": "content/teaching.html#data-visualization",
    "title": "Teaching",
    "section": "Data visualization",
    "text": "Data visualization\nI am working on adapting a course from Claus Wilke for presentation in Lund."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interlude One",
    "section": "",
    "text": "Electricity generation per capita\n\n\n\n\n\nA post on electricity generation per capita across the world since 2000.\n\n\n\n\n\n\nJan 30, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nOur World In Data choropleth\n\n\n\n\n\nA post on how recreate the Our World In Data characteristic choropleth in R.\n\n\n\n\n\n\nJan 29, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT cooks me dinner\n\n\n\n\n\nüç≤ Master the kitchen like a pro with the help of ChatGPT! üç¥ I tried it out and the results will blow your mind! ü§Ø Find out what happened when I asked a language model for recipe ideas and cooking instructions. üò≤ You won‚Äôt believe how delicious my Gammon and Vegetable Stew with Mint turned out! üçΩÔ∏è Don‚Äôt miss out on this must-read blog post! üëÄ\n\n\n\n\n\n\nJan 20, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT fills in for me as a blog post author\n\n\n\n\n\nI ask ChatGPT to write a blog post in the same manner as one I wrote last week. The results will amaze you!\n\n\n\n\n\n\nDec 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nCoefficient plots in ggplot\n\n\n\n\n\nDrawing coefficient plots in R and ggplot.\n\n\n\n\n\n\nDec 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nLinkedIn style tables for personal websites\n\n\n\n\n\nA post on how to us the {gt} table package to make tables for the about page of a pesonal website or blog.\n\n\n\n\n\n\nMay 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nProgrammatically generate tabs in Rmarkdown\n\n\n\n\n\nA walk-though for writing code that writes it‚Äôs own code.\n\n\n\n\n\n\nMar 4, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nFlavours of cocoa\n\n\n\n\n\nAn exploration of chocolate bar reviews\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nR vs Python? R & Python!\n\n\n\n\n\nMining the twitter archive of Fathom Data\n\n\n\n\n\n\nJan 17, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nStudents Speak\n\n\n\n\n\nText analysis of students‚Äô diary entries during the Covid-19 lockdown in South Africa\n\n\n\n\n\n\nJul 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nResources for Learning R and Mastering ggplot\n\n\n\n\n\nA collection of videos and resources to get you started using R and the tidyverse for exploratory data analysis and visualization.\n\n\n\n\n\n\nMay 21, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nMapping Sweden‚Äôs Regional GDP\n\n\n\n\n\nVisualizing the relationship between population and regional GDP share in 1571, based on data from Enflo and Missiaia (2018).\n\n\n\n\n\n\nMay 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nShipping and Scraping - Part 1 in a Series on Shipping\n\n\n\n\n\nIn this post I walk through scraping data on cargo ships from Wikipedia as part of a series on shipping. I make use of R, the rvest package for webscraping and the SelectorGadget tool for CSS selection.\n\n\n\n\n\n\nApr 8, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nSwedish trade in goods and exchange students\n\n\n\n\n\nI wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund Univesity from there, this semester.\n\n\n\n\n\n\nFeb 20, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nSouth African Car Colour Exploration\n\n\n\n\n\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\n\n\n\n\n\n\nNov 24, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nCar Price Comparison Tool\n\n\n\n\n\nThis web app helps South Africans gauge the median price for a second hand car. You can compare price by odometer reading, make and model, and province. Ad links are included in a table beside the output.\n\n\n\n\n\n\nOct 10, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nRecreating Our World In Data‚Äôs Beautiful Transistor Count Graphic\n\n\n\n\n\nScraping data from Wikipedia and recreating OWID‚Äôs figure with ggplot2.\n\n\n\n\n\n\nApr 8, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to Interlude One\n\n\n\n\n\n‚ÄúLike all good fruit, the balance of life is in the ripe and ruin‚Äù\n\n\n\n\n\n\nMar 13, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html",
    "href": "posts/2021-07-06-students-speak/students-speak.html",
    "title": "Students Speak",
    "section": "",
    "text": "Hi Johan - here are some visualizations I have put together from your students‚Äô diary entries. I think that they tell quite a nice story. I hope that some are useful. I‚Äôve done them in black and white - I‚Äôm not sure where you want to publish them in the end. If you‚Äôd like some colour I can add it easily.\nI‚Äôve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\n\n\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\n\n\n\nThe students‚Äô diary entries have been ingested to form a dataset such that each row is one student‚Äôs observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\n\n\n\n\n\n\n\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\n\n\n\n\n\n\nWe start with a word cloud which shows the words used by the students in their diary entries.\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\n\n\n\n\n\n\n\n\nWord cloud\n\n\n\n\nWe can see that common positive words include ‚Äúsupport‚Äù, ‚Äúprivileged‚Äù, ‚Äúhealthy‚Äù, ‚Äúproductive‚Äù, and ‚Äúexcited‚Äù. Common negative words are dominated by ‚Äúvirus‚Äù, followed by ‚Äúdifficult‚Äù, ‚Äústruggling‚Äù, and ‚Äúinfected‚Äù.\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students‚Äô diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\nTable @ref(tab:excluded) shows some common words in the students‚Äô diary entries which are excluded from the wordcloud in Figure @ref(fig:wordcloud).\n\n\n\nCommon words excluded from wordcloud\n\n\nWord\nNumber of uses\n\n\n\n\nLockdown\n251\n\n\nPeople\n211\n\n\nDay\n183\n\n\nTime\n172\n\n\nFamily\n111\n\n\nFeel\n92\n\n\nSouth\n90\n\n\nHome\n80\n\n\nWorld\n78\n\n\nDays\n71\n\n\nAfrica\n61\n\n\nLife\n54\n\n\nOnline\n51\n\n\n19\n50\n\n\nFriends\n50\n\n\n\n\n\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure @ref(fig:image-grobs).\n\n\n\n\n\n\nConventional word cloud and comparison cloud\n\n\n\n\n\nFigure @ref(fig:sentiment) below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students‚Äô diary entries in a particular week.\n\n\n\n\n\n\n\n\nEvolution of sentiment\n\n\n\n\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn √Örup Nielsen in 2009-2011. An example of the scores assigned to words in the students‚Äô diary entries is shown in Table @ref(tab:afinn) below.\n\n\n\nAFINN sentiment scores\n\n\nWord\nSentiment score\n\n\n\n\nBullshit\n-4\n\n\nCatastrophic\n-4\n\n\nPanic\n-3\n\n\nFake\n-3\n\n\nWorse\n-3\n\n\nFunny\n4\n\n\nFun\n4\n\n\nWonderful\n4\n\n\nThrilled\n5\n\n\n\n\n\n\n\n\nThe wordcloud in Figure @ref(fig:wordcloud) showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin‚Äô Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\nTable @ref(tab:words) below shows the words most specific to each week.\n\n\n\n\n\n\nWeek specific words calculated with weighted log odds\n\n\n\n\n\n\n\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\n\n\n\n\nAfrican leaders\nLockdown starts\nPrivilege\nEaster Sunday\nZoom\nClothing bank\n\n\nChurch\nVirus\nConspiracy theories\nPayment\nR500 billion\nLockdown restrictions\n\n\nAirports\nTownships\nAn obligation\nHot Cross buns\nSUN Learn\nLevel 5\n\n\nNSFAS\nCases recorded\nContinues to rise\nExtension\nEconomic stimulus\nLivelihoods\n\n\n\n\n\nNice! We can see that we capture some elements of the experience in each week of lockdown.\n\n\n\n\n\n\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students‚Äô sentiment across the weeks.\n\n\n\n\n\nSentiment and week-words figure\n\n\n\n\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\n\n\n\nFigure @ref(fig:context) shows the evolution of the sentiment of the students‚Äô diary entries beside the rising Covid-19 case numbers in South Africa.\n\n\n\n\n\n\n\n\nComparison figure\n\n\n\n\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students‚Äô reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\n\n\n\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\nI think option two conveys the message clearly and without clutter."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "title": "Students Speak",
    "section": "Option 1",
    "text": "Option 1\nHere we have a two panel plot of the Covid-19 statistics and number of diary entries recorded by the students."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "title": "Students Speak",
    "section": "Option 2",
    "text": "Option 2\nNext we have a single panel with the period of diary entries superimposed on the Covid-19 statistics."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "title": "Students Speak",
    "section": "Option 3",
    "text": "Option 3\nAlternatively we can annotate a thick line to show where the diary entries occur."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "title": "Students Speak",
    "section": "Option 4",
    "text": "Option 4\nAlternatively we can have a legend variant of option 2."
  },
  {
    "objectID": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html",
    "href": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html",
    "title": "South African Car Colour Exploration",
    "section": "",
    "text": "While driving to Cape Town this past weekend in my beautiful white Hyundai i10, a German friend asked why every second car in South Africa was white in colour. While I didn‚Äôt have a compelling reason, I did have a some data to assess whether it was true that half of cars on the road are white.\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\nI show a useful trick for filling in missing data that creates gaps in the area chart using dplyr‚Äôs pivot_wider and pivot_longer commands.\nHere is the final interactive graphic:\n\ndf_plotly <- read_rds(\"data/car_colour/df_plotly.rds\")\n\np_load(plotly)\n\ng <- df_plotly %>% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\ndf <- readRDS(\"data/car_colour/gumtree_ads_2020-10-01-11-33-am.rds\")\n\ndf <- df %>% filter(!is.na(make),\n                    !is.na(colour),\n                    !is.na(model)) %>% \n             # Removing the duplicated province names   \n             mutate(province = str_remove(province, \"\\\\,.*\"))\n\n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n44644\n\n\nNumber of columns\n30\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n22\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nlink\n0\n1.00\n68\n86\n0\n2218\n0\n\n\nad_url\n0\n1.00\n78\n193\n0\n38392\n0\n\n\nsite\n0\n1.00\n7\n7\n0\n1\n0\n\n\nseller_type\n0\n1.00\n19\n30\n0\n2\n0\n\n\ntitle\n0\n1.00\n4\n100\n0\n26140\n0\n\n\ntext\n0\n1.00\n10\n3885\n0\n38343\n0\n\n\nad_date\n0\n1.00\n19\n19\n0\n42416\n0\n\n\nseller_name\n0\n1.00\n1\n50\n0\n8872\n0\n\n\nlocation\n0\n1.00\n24\n61\n0\n428\n0\n\n\nseller_age\n0\n1.00\n17\n22\n0\n79\n0\n\n\nn_all_time_ads\n0\n1.00\n1\n7\n0\n3481\n0\n\n\nscrape_time\n0\n1.00\n19\n19\n0\n35812\n0\n\n\nlocation_2\n0\n1.00\n11\n47\n0\n428\n0\n\n\nfor_sale_by\n226\n0.99\n5\n6\n0\n2\n0\n\n\nmake\n0\n1.00\n3\n13\n0\n37\n0\n\n\nmodel\n0\n1.00\n1\n20\n0\n459\n0\n\n\nbody_type\n226\n0.99\n3\n12\n0\n11\n0\n\n\ntransmission\n206\n1.00\n6\n9\n0\n2\n0\n\n\nfuel_type\n104\n1.00\n6\n8\n0\n4\n0\n\n\ncolour\n0\n1.00\n3\n8\n0\n16\n0\n\n\nna\n44644\n0.00\nNA\nNA\n0\n0\n0\n\n\nprovince\n0\n1.00\n7\n13\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npage\n0\n1.00\n14.51\n12.66\n1\n4.00\n11.0\n22.00\n50\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\nad_number\n0\n1.00\n23570.95\n14487.85\n1\n11471.75\n22889.5\n34258.25\n56454\n‚ñá‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nprice\n229\n0.99\n218706.68\n692268.63\n1\n109900.00\n165000.0\n259900.00\n140000000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nn_photos\n1099\n0.98\n8.63\n3.08\n1\n7.00\n9.0\n12.00\n12\n‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñá\n\n\nn_views\n0\n1.00\n588.85\n1281.69\n0\n91.00\n233.5\n580.00\n46956\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nn_active_ads\n0\n1.00\n130.20\n225.27\n1\n5.50\n29.0\n100.00\n997\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nyear\n100\n1.00\n2013.76\n6.66\n1900\n2011.00\n2016.0\n2018.00\n2021\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nkilometers\n1189\n0.97\n92267.35\n157493.13\n1\n26890.50\n72000.0\n130000.00\n9999999\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nThe dataset has some really nice information on the cars advertised, including their characteristics, as well as the location and name of the seller.\nFor this graphic, we only need the car‚Äôs year and colour.\n\n\n\nWe start by plotting the number of cars in each colour advertised on Gumtree. In this plot I show the 12 most common colours.\nIt is evident that white and silver cars are very common. In as far as our data reflects cars on the road, white cars make up nearly 1/3 of all cars advertised! A little less than the half quoted in the discussion above, but certainly the dominant colour.\n\ndf %>%\n  mutate(colour = fct_lump(colour, 12)) %>%\n  select(colour) %>% \n  count(colour, sort = T) %>% \n  mutate(colour = fct_reorder(colour, n)) %>% \n  ggplot(aes(n, colour)) +\n  geom_segment(aes(xend = 0, yend = colour)) +\n  geom_point(size = 4, colour = \"orange\") +\n  geom_text(aes(label = scales::comma(n)), hjust = -.3) +\n  scale_x_continuous(labels = scales::comma_format(), limits = c(0, 16000)) +\n  labs(x = \"Number of vehicles\",\n       title = \"What are the most common vehicle colours advertised on Gumtree?\",\n       subtitle = \"Data from 45,000 adverts from across the country\",\n       y = \"\")\n\n\n\n\n\n\n\nNext we want to know if some colours have become more popular over time. Given that our data source is a snapshot of cars for sale at one point in time and on one online website, this is unlikely to be representative of the entire population of cars in South Africa. However, it will be interesting to see the trends that do exist in this data.\nWe start with a simple plot of the number of cars advertised in the none more common colours, by year.\n\ndf_colours <- df %>% \n  group_by(year, colour) %>% \n  summarise(n = n()) %>%\n  ungroup() %>% \n  filter(between(year, 1990, 2020)) \n\nIt makes sense that the majority of adverts are for cars that are produced recently, and it is striking that white and silver cars see such a dramatic increase, even relative to the third most common car colour, grey.\n\ndf_colours %>% \n  mutate(colour = fct_lump(colour, 10)) %>%\n  filter(colour != \"Other\") %>% \n  mutate(colour = fct_reorder(colour, n, .fun = sum, .desc = T)) %>% \n  ggplot(aes(year, n, fill = colour)) +\n  geom_area() +\n  facet_wrap(~ colour)\n\n\n\n\n\n\n\nIn this plot we show the evolution of car colour between 1990 and the present.\nThere are a few problems with this plot. First, there are too many colours, the least common of which have just a few entries. Second, the missing values for the less common colours create unappealing white gaps in the bottom left of the plot.\n\ndf_colours %>% \n  group_by(year) %>% \n  add_tally(n) %>% \n  ungroup() %>% \n  mutate(colour = fct_reorder(colour, nn, .fun = sum, .desc = T)) %>% \n  mutate(share = n / nn) %>% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\nTo solve this problem, we can fill in the missing values by making the dataset wider and then long gain.\nUsing dplyr‚Äôs pivot_wider and the values_fill = 0 option, we create a wide dataset with one row for each year, shown below.\nWe go from a long dataset like this:\n\nhead(df_colours)\n\n# A tibble: 6 √ó 3\n   year colour     n\n  <dbl> <chr>  <int>\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      3\n\n\nTo a wide one, like this:\n\ndf_colours <- df_colours %>% \n  mutate(colour = fct_lump(colour, 10)) %>%\n    group_by(year, colour) %>% \n    mutate(n = sum(n)) %>% \n  ungroup() %>% \n  distinct() %>% \n  pivot_wider(names_from = colour, values_from = n, values_fill = 0) \n\nhead(df_colours)\n\n# A tibble: 6 √ó 11\n   year Black  Blue  Gold Green  Grey Other   Red Silver White Burgundy\n  <dbl> <int> <int> <int> <int> <int> <int> <int>  <int> <int>    <int>\n1  1990     2    14     3     3     5     4    10      3    10        0\n2  1991     3    11     2     4     4     3     9      3    12        1\n3  1992     1    16     2     4     5     7    11      3    14        1\n4  1993     0    13     2     1     4     1    13      7    18        1\n5  1994     2    15     0     8     7     1    13      6    41        6\n6  1995     6    36     0    17     6     3    32      7    34        7\n\n\nNow we have a ‚Äòwide‚Äô dataset with the missing values filled in with zeros\nWe can then make it long again, with the pivot_longer command, specifying that we want the original column names back.\n\ndf_colours <- df_colours %>% \n  pivot_longer(!year, names_to = \"colour\", values_to = \"n\")\n\nhead(df_colours)\n\n# A tibble: 6 √ó 3\n   year colour     n\n  <dbl> <chr>  <int>\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      4\n\n\n\n\n\nNow we have a plot with no white gaps, and an appropriate number of colours. However, it would be nice to have the colours on the plot match the colours of the cars, and have some labels to make it clear to the reader how to interpret the graphic.\n\ndf_colours %>% \n  group_by(year) %>% \n  add_tally(n) %>% \n  ungroup() %>% \n  mutate(share = n / nn) %>% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\nWe use ggplot‚Äôs scale_fill_manual to specify the hex codes for the colours in the order that they appear in the plot.\nWe add in a title and axes labels, and make the plot interactive with ggplotly.\n\ng <- df_colours %>% \n  filter(year > 1994) %>% \n  group_by(year) %>% \n  add_tally(n) %>% \n  ungroup() %>% \n  mutate(share = n / nn) %>% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\n\n\nWow! Blue and Green cars really lost out to white and Silver ones over time. Of the cars advertised that were produced in 2020, nearly 42% are white!\nBlack cars seemed to have a peak in popularity around 2007, and then decline again, coinciding with the first round of load-shedding in South Africa.\nThanks for reading along, and I hope you enjoyed learning about ggplot as well as the cars on South Africa‚Äôs roads!\n\n\n\nHere is the static figure with accompanying caption.\n\n# jpeg(\n#     filename=\"figures/stacked_area_2.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_colours %>%\n  filter(year > 1994) %>%\n  group_by(year) %>%\n  add_tally(n) %>%\n  ungroup() %>%\n  mutate(share = n / nn) %>%\n  mutate(colour = fct_relevel(colour, \"Silver\")) %>%\n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#95A5A6\", \"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#EAECEE\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\n\n\n# dev.off()\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html",
    "title": "Swedish trade in goods and exchange students",
    "section": "",
    "text": "In a presentation last week organized by the Laboratory for the Economics of Africa‚Äôs Past I learned about the persistence of economic connectivity between areas from the Roman era until today. It was fascinating. The presenter, Prof.¬†Erik Hornung, mentioned offhand that the choice of where to spend a semester exchange was likely a function of the economic and social connections between your home country and your host country ‚Äì whether you know someone there, have family, or are familiar due to cultural similarities.\nI am in Sweden this semester, on an exchange at Lund University, in the South of the country. This is due in part to my supervisor‚Äôs connections with the economic historians here.\nI wanted to see if I could back up this assertion about exchange students with some data. More specifically, I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund from there, this semester.\nAs you see in the graphic below, there appears to be a strong positive correlation between Swedish imports of goods and imports of exchange students at Lund.\n\ndf_plotly <- read_rds(\"data/Sweden_trade_exchange/plotly_data.rds\")\n\nc <- df_plotly %>% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent)) +\n  geom_smooth(group = 1, method = \"lm\", se = F) +\n  geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(title = \"Correlation between Sweden's imports of goods and exchange students\",\n       x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"Population\")\n\nc\n\n\n\n\nThe most populous European nations of Germany and France lead the rankings, with 46 and 37 exchange students respectively. The largest non-European senders are China and India, followed by the United States. South Africa has the largest representation for Africa, with 4 students.\nRead on to see the creation of the graphic."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#data",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#data",
    "title": "Swedish trade in goods and exchange students",
    "section": "Data",
    "text": "Data\nTo answer my question, I link two pieces of data: trade statistics on Sweden‚Äôs imports, and the nationalities of the incoming exchange students at Lund.\n\nTrade data\nThe first I downloaded from an amazing trade tool called the Observatory of Economic Complexity (Linked here). It has wonderful visualizations of trade by type of goods and by country. Have a look at this tree map below for an example.\n\n\n\n\nTree Map\n\n\nThe tree map shows the origins of Sweden‚Äôs imports by value in 2018. Sweden cares about limiting carbon emissions, and so it makes sense that the majority of their imports are sourced within Europe.\n\n\n\nData on exchange students\nThe second data source is a rough proxy for the nationalities of my colleagues. I collected phone numbers from a big WhatsApp group called ‚ÄúLund University ‚Äô21‚Äù and extract the international dialing codes. There are several shortcomings to this data source ‚Äì self-selection into WhatsApp use may differ by country, as might the desire to be part of a large group. Further, WhatsApp groups are limited in size at 256 members, just more than half the total number of exchange students at Lund this semester. My sample is unlikely to perfectly represent my population of interest, but it is a good enough starting point.\n\nData processing\nIn the chunks of code below I scrape a list of international dialing codes from the web, along with other country level information including GDP per capita and land area.\n\n# website with data on dialing codes\nurl <- \"https://countrycode.org/\"\n\n# scrape table with rvest\ntable <- read_html(url) %>% \n  html_nodes(\"table\") %>% \n  html_table()\n\n# processing\ntable <- table[[1]] %>% as_tibble(.name_repair = \"minimal\") %>% unnest()\n\ntable <- table %>%  as_tibble() %>% janitor::clean_names()\n\n# cleaning names and formatting columns as numbers rather than characters\ntable_df <- table %>% \n  mutate(population = parse_number(population),\n         area_km2 = parse_number(area_km2),\n         gdp_usd = parse_number(gdp_usd),\n         gdp_usd = gdp_usd*10e9)\n\nHere is a table of the data scraped from countrycode.org.\n\n# display table\ndatatable(table_df)\n\n\n\n\n\n\nWe can show the regions of the world by the first digit of their dialing code in a neat map. Expand the chunk by clicking code to see how easy it is to make an interactive graphic with ggplotly.\n\n# packages for map plotting and matching countries.\np_load(rnaturalearth, countrycode)\n\n# extracting first digit of dialing code\ntable_df <- table_df %>% \n  mutate(iso_a3 = countrycode(country, origin = \"country.name\", destination = \"iso3c\")) %>% \n  mutate(first_digit = substring(country_code, 1, 1))\n\n# creating dataframe with mapping geometry\nworld <- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# joining up to table of dialing codes\nmap_df <- world %>% as_tibble() %>% left_join(table_df, by = \"iso_a3\")\n\n# creating plot\na <- map_df %>% \n  filter(!is.na(first_digit)) %>% \n  ggplot(aes(geometry = geometry, fill = first_digit)) +\n  geom_sf() +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(title = \"Countries coloured by first digit of international dialing code\",\n       fill = \"First digit\")\n\n# display interactive plot\nggplotly(a)\n\n\n\n\n\n\n\nDialing codes\nThe trickiest part was matching a country name with an international dialing code from the number alone. I used Google‚Äôs open source library for parsing, formatting, and validating international phone numbers. It is written in Java but someone has kindly written a wrapper for R. I show the process in the code chunk below, but do not display the phone numbers themselves for privacy reasons.\n\n# Sys.setenv(JAVA_HOME='C:\\\\Program Files\\\\Java\\\\jre1.8.0_281')\n# install.packages(\"dialrjars\", INSTALL_opts = c(\"--no-multiarch\"))\n\nlibrary(dialrjars)\nlibrary(dialr)\n\ndf <- read_excel(\"data/Sweden_trade_exchange/Whatsapp numbers.xlsx\")\n\n# uniform formatting of numbers\ndf <- df %>% \n  mutate(clean = ifelse(str_detect(raw, \"\\\\+.*\"), raw, str_c(\"+\", raw))) %>% select(number = clean)\n\ndf <- df %>% \n  mutate(phone = phone(number, \"SE\"))\n\n# get the region from number\ndf <- df %>% \n  mutate(region = get_region(phone))\n\n# count number of phone numbers per country\ncounts <- df %>% count(region, sort = T)\n\n# match name of country to two letter country code\ncounts <- counts %>% \n  mutate(name = countrycode(region, origin = \"iso2c\", destination = \"country.name\"),\n         iso_a3 = countrycode(region, origin = \"iso2c\", destination = \"iso3c\"))\n\ndatatable(counts)\n\n\n\n\n\n\nA quick plot of counts. Wow! Look at that. Go Germany!\n\ndf_col <- counts %>% \n  left_join(world %>% as_tibble(), by = \"iso_a3\") %>% \n  rename(name = name.x)\n\ndf_col %>%   \n  filter(name != \"Sweden\") %>% \n  mutate(name = fct_reorder(name, n),\n         continent = factor(continent, levels = c(\"Europe\", \"Asia\", \"North America\", \"Africa\", \"South America\"))) %>% \n  ggplot(aes(n, name, fill = continent)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Number of exchange students in Lund Uni exchange WhatsApp group\",\n       subtitle = \"By country\",\n       y = \"\",\n       x = \"Number of exchange students\",\n       fill = \"Continent\")"
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#findings",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#findings",
    "title": "Swedish trade in goods and exchange students",
    "section": "Findings",
    "text": "Findings\n\nPlotting the correlation between imports of goods to Sweden and exchange students to Lund\nI plot the correlation between the value of Sweden‚Äôs imports in 2018 on the x-axis, and the number of exchange students on the y-axis. Both axes are on a log scale.\nBelow is an interactive version of the static plot above.\n\nc <- df_plotly %>% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent, text=paste(\"Country:\", name))) +\n  geom_smooth(group = 1, method = \"lm\", se = T) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"\",\n       colour = \"\")\n\n\nggplotly(c)\n\n\n\n\n# static figure\n\n\n# map_df_short <- map_df %>%\n#   select(iso_a3, continent, pop_est, gdp_md_est)\n# \n# jpeg(\n#   filename= \"data/Sweden_trade_exchange/reg_2.jpg\",\n#   width = 8,\n#   height = 6,\n#   units = \"in\",\n#   res = 500\n# )\n# \n# df %>%\n#   inner_join(map_df_short) %>%\n#   filter(iso_a3 != \"SWE\",\n#          Continent != \"Antarctica\") %>%\n#   rename(exchange_students = n) %>%\n#   ggplot(aes(trade_value, exchange_students)) +\n#   geom_point(aes(size = pop_est, colour = Continent)) +\n#   geom_smooth(group = 1, method = \"lm\", se = F) +\n#   geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n#   scale_y_log10() +\n#   scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n#   scale_size_continuous(labels = scales::comma_format()) +\n#   scale_color_brewer(palette = \"Paired\") +\n#   geom_hline(yintercept = 0, lty = 2) +\n#   labs(x = \"Value of imports into Sweden in 2018 (USD)\",\n#        y = \"Number of exchange students in Lund Whatsapp group\",\n#        size = \"Population\")\n# \n# dev.off()\n\nThank you for reading along!"
  },
  {
    "objectID": "posts/2021-03-18-car-price-explorer/car-price-explorer.html",
    "href": "posts/2021-03-18-car-price-explorer/car-price-explorer.html",
    "title": "Car Price Comparison Tool",
    "section": "",
    "text": "Here‚Äôs a link to the web app, along with an image of what it looks like.\nThe app has three components. In the scatter plot you can see the price of each car advertised on Gumtree on the y-axis against its odometer reading on the x. Below this is a box and whisker plot of price to show the distribution of prices for the cars in the range. To the right is a table showing the details of each advert along with a link. You can choose the make, model, province, odometer range and year of manufacture in the panel on the left.\n\n\n\n\nWeb App\n\n\n\nI hope that you find it useful!"
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html",
    "href": "posts/2021-04-03-container-ships/container-ships.html",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "This week we learned about a data leak at Facebook which took place in 2019, where more than 500 million phone numbers, email addresses and names were scraped from the site and leaked online. Then, on Thursday we heard about another 500 million records including names, email addresses and more personal details were scraped from Linkedin, though the company argues that this was not a data breach. If you want to learn more about scraping, and get in on the (non-nefarious) action yourself, read along. As a bonus you will learn about how cargo ships have become so large.\nIn this post I want to show how easy it is to scrape data from the internet. It is the first post in a series looking at ships. I walk through scraping data from Wikipedia, one of the best places on the internet to ingest tabular data from.\nBefore we begin with the scraping walkthrough, I want to visualize the data and show how cargo ships have become larger over time.\n\n\nThe small multiples plot below shows the evolution of cargo ship size from 1870 to today. On the x-axis is the length of the average cargo ship per decade from Wikipedia‚Äôs list of cargo ships1. On the y-axis is the average ship beam, or width at the widest point. The colour of the rectangle shows the deadweight tonnage of the average ship, or amount of cargo that the ship can carry.\nDeadweight tonnage or tons deadweight is a measure of how much weight a ship can carry. It is the sum of the weights of cargo, fuel, fresh water, ballast water, provisions, passengers, and crew.\n\n\n\n\n\nCargo ships have increased dramatically in size over time! The oldest ship in our dataset is the R. J. Hackett, one of the first Great Lakes freighters. It was just 63m long and 10m wide, with a wooden hull. According to historian Mark Thompson, the R. J. Hackett‚Äôs boxy hull, hatch-lined deck, and placement of the deckhouses meant the ship was ideally suited for moving cargo through inland waterways. This steamer greatly influenced the development of cargo ships which followed.\nToday, container ships like the Ever Given are nearly 400m long, 60m wide, and can carry more than 20,000 twenty-foot containers. That‚Äôs enough space for 745 million bananas!\nTEUs, or twenty-foot equivalent units, is the measure that shipping companies use to compare volume. A TEU is 6.1m long, 2.4m wide and usually 2.6m high. Source: Wikipedia\n\n\n\nIn the plots below we focus only on container ships built after 1970. This era saw the construction of the first ships purpose built to carry ISO containers, which could be loaded and unloaded rapidly at port, repacked and shipped onward on any compatible ship, truck or train. The ISO standard container transformed the shipping industry and replaced the prior status quo of break bulk carriers. One of the consequences was a dramatic reduction in demand for ‚Äústevedores‚Äù or ‚Äúlongshoremen‚Äù; workers who would manually unload cargo from break bulk carriers.\nIf you‚Äôre interested in containerization, I highly reccomend this episode from the podcast Trade Talks, and this eight-part series from Alexis Madrigal.\n\n\n\n\n\n\n\nHow have cargo ship deadweight tonnages, or how much cargo a ship can carry, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships can carry more cargo today than ever before. It‚Äôs hard to get my mind around 220 000 tons of cargo!\n\n\n\nHow has the length of cargo ships changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nThe Ever Given is among the longest container ships operating today at 400m in length. The linear fit line shows that there has been a steady increase in container ship length over time.\n\n\n\nHow have cargo ship beams, or widths of ships at their widest point, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships have also become wider, with lumping at beams of 32m, 40m and 59m.\n\n\n\n\nSo it certainly seems that cargo ships have been becoming larger over time. Interestingly, it appears that while the largest container ships continue to get larger and carry more cargo, there is still a need for relatively small ships. There are a significant number of container ships that can carry less than 50 000 tons launched since 2010, shown in the density plot below. We could say that there has been a bifurcation in ship capacity, with a few enormous ships, and a greater number of smaller ships operating in tandem today.\n\n\n\n\n\n\n\n\nEconomies of scale describes a decrease in the per unit cost as scale of an operation increases. This perfectly fits the shipping industry‚Äôs relentless path towards upsizing ships, cranes and ports. One of the reasons has to do with fluid dynamics. Hull resistance is one of the key factors impacting fuel consumption. For vessels in water, drag loss is less than proportional to increasing cargo carried. In other words, making a ship larger usually results in less fuel consumption per ton of cargo, holding all else constant. As we have seen in the figures above, container ships have become larger and larger as they carry more cargo per ship, in an effort to save fuel.\nOther methods of drag reduction include improved hull design, injecting air around the hull surface and reducing hull roughness from slime and weeds. See Resistence and powering of ships \nAccording to Marc Levinson, author of The Box: How the Shipping Container Made the World Smaller and the World Economy Bigger, the shippers applied this idea to every element of the industry. He says:\n\nBigger ships lowered the cost of carrying each container. Bigger ports with bigger cranes lowered the cost of handling each ship. Bigger containers ‚Äî the 20-foot box, shippers‚Äô favorite in the early 1970s, was yielding to the 40-footer ‚Äî cut down on crane movements and reduced the time needed to turn a vessel around in port, making more efficient use of capital. A virtuous circle had developed: lower costs per container permitted lower rates, which drew more freight, which supported yet more investments in order to lower unit costs even more. If ever there was a business in which economies of scale mattered, container shipping was it.\n\nThe consequences of containerization are fascinating ‚Äì including rapidly falling costs of trade, increasingly intermeshed global supply chains, a proliferation of robots at ports, and the environmental challenges associated with ships, trucks and trains meeting at transshipping nodes around the world.\nIn the remainder of this post I walk through scraping some of the data presented above."
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html",
    "href": "posts/2021-04-21-chips/chips.html",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "",
    "text": "In this document I walk through the process to recreate a data visualization from Our World in Data. I use the web scraping package rvest to get the data from Wikipedia and some neat tidyverse tools.\nThe graphic depicts the metal‚Äìoxide‚Äìsilicon (MOS) transistor count on commercially available microprocessors over time. This progression is known as Moore‚Äôs law, which observed that the transistor count doubles approximately every two years.\n\n\n\n\nGraphic showing Moore‚Äôs Law - Our World In Data\n\n\n\nFirst, I show the wikipedia page that contains the information that I want to scrape (and contains the Our World in Data graphic).\n\n\n\n\nWikipedia article\n\n\n\nThe Wikipedia article on transistor counts includes a table containing 193 microprocessors released between 1971 and 2020, shown below. Crucially, it specifies for each chip the release year and transistor count.\n\n\n\n\nTable showing transistor count"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#scraping",
    "href": "posts/2021-04-21-chips/chips.html#scraping",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Scraping",
    "text": "Scraping\nThis chunk of code grabs the HTML of the Wikipedia page, selects all the tables on the page and saves the one we want to analyze as a tibble, a very handy dataframe alternative. Rvest‚Äôs html_table function returns all of the elements of a web table, neat!\n\nurl <- \"https://en.wikipedia.org/wiki/Transistor_count\"\n\nhtml <- read_html(url)\n\nhtml_tables <- html %>% \n  html_table()\n\ndf_transistors <- html_tables[[4]]"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#data-processing",
    "href": "posts/2021-04-21-chips/chips.html#data-processing",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Data processing",
    "text": "Data processing\nAs visible in the screen capture of the table above, a few cells also hyper-link to sources for the stats. These square brackets prevent R from reading the column as a set of numbers, instead the cell contents are returned as characters. In the chunk below I use readr‚Äôs parse_number function to extract the numbers from each cell and store them as a double.\n\ndf_transistors <- df_transistors %>% \n  janitor::clean_names() %>% \n  mutate(processor = str_squish(str_remove(processor, \"\\\\(.*\"))) %>% \n  mutate_at(c(\"transistor_count\", \"date_ofintroduction\", \"process_nm\", \"area_mm2\"), parse_number)"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#data-viz-preparation",
    "href": "posts/2021-04-21-chips/chips.html#data-viz-preparation",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Data viz preparation",
    "text": "Data viz preparation\nIn this chunk I lump together the chip designers into the five largest groups and an ‚Äòother‚Äô category with forcats‚Äô fct_lump function. It returns these categories as factors. To make the graphic readable, I sample 50 of the processors from the table of 193 to annotate onto the plot I also store some graph labels.\n\ndf_transistors %>% \n  count(designer, sort = T)\n\n# A tibble: 44 √ó 2\n   designer              n\n   <chr>             <int>\n 1 Intel                64\n 2 Apple                22\n 3 AMD                  21\n 4 IBM                  19\n 5 Fujitsu              10\n 6 Huawei                7\n 7 Motorola              7\n 8 Qualcomm              7\n 9 DEC                   5\n10 Texas Instruments     5\n# ‚Ä¶ with 34 more rows\n\ndf_transistors <- df_transistors %>% \n  mutate(designer = fct_lump(designer, 5))\n\nprocessor_labels <- sample(df_transistors$processor, 50)\n\ntitle <- \"Moore's Law\"\n\nsubtitle <- \"The number of transistors on integrated circuit chips (1971-2020)\"\n\ncaption <- \"Source: Wikipedia - Transistor Count: https://en.wikipedia.org/wiki/Transistor_count\""
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#plotting",
    "href": "posts/2021-04-21-chips/chips.html#plotting",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Plotting",
    "text": "Plotting\nHere I draw the graphic using ggplot2.\n\n# jpeg(\n#     filename=\"C:/Users/User/Documents/Recon/jjayes_distill/_posts/2021-04-21-chips/figures/Transistors_colour.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_transistors %>% \n  ggplot(aes(date_ofintroduction, transistor_count)) +\n  geom_smooth(group = 1, alpha = .1) +\n  geom_point(aes(colour = designer, shape = designer), cex = 3) +\n  geom_text_repel(data = df_transistors %>% \n                    filter(processor %in% processor_labels), \n                  aes(date_ofintroduction, transistor_count, label = processor),\n                  cex = 3, colour = \"#505050\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  scale_shape_manual(values = seq(15,20, by = 1)) +\n  scale_y_log10(labels = scales::comma_format()) +\n  labs(x = \"Year\",\n       y = \"Transistor Count (log scale)\",\n       shape = \"Chip Designer\",\n       colour = \"Chip Designer\",\n       caption = caption,\n       title = title,\n       subtitle = subtitle)\n\n\n\n# dev.off()\n\nI include a smoothed line of best fit (with geom_smooth) below the points to show how progression in transistor count has slowed since 2005, despite the number of chips designed each year increasing.\nI use geom_text_repel to prevent the labels overlapping each other, and put the y-axis on a log scale to draw attention to the approximate doubling of transistor count each year."
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#conclusion",
    "href": "posts/2021-04-21-chips/chips.html#conclusion",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Conclusion",
    "text": "Conclusion\nThanks for reading along! I hope you learned some neat tricks, and see how easy it is to grab data from the web to visualize."
  },
  {
    "objectID": "posts/2021-05-06-swedenmap/swedenmap.html#what-was-the-relationship-between-population-and-regional-gdp-in-sweden-in-1571",
    "href": "posts/2021-05-06-swedenmap/swedenmap.html#what-was-the-relationship-between-population-and-regional-gdp-in-sweden-in-1571",
    "title": "Mapping Sweden‚Äôs Regional GDP",
    "section": "What was the relationship between population and regional GDP in Sweden in 1571?",
    "text": "What was the relationship between population and regional GDP in Sweden in 1571?\n\nScatter plot and interactive map\nThe scatter plot below shows the relationship between population in the Swedish counties in 1571 (x-axis) and regional GDP share (y-axis).\nHovering over a point shows the name of the county, and highlights its location on the map of Sweden to the right (shown with their current borders).\nClicking on a county on the map will take you to it‚Äôs page on Wikipedia.\n\n\n\n\n\n\n\n\n\nSource: Enflo and Missiaia (2018), Data accessible here.\nFigure inspiration and code from Claus Wilke\n\n\nWhat do we learn?\nIn the scatter plot above we can see that the two northernmost counties, though large in land are are very sparsely populated. The line of best fit shows that V√§stmanland County has a high relative share of GDP compared to it‚Äôs population, and that Kalmar L√§n has a high population relative to it‚Äôs share of GDP in 1571."
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "",
    "text": "Data visualisation is super fun and a great tool for communication. I have put together some slides for talk on data visualisation for economic historians at the Laboratory for the Economics of Africa‚Äôs Past. These resources are collected here to help when you think ‚Äòwhere do I start with R?‚Äô.\nI hope that you find these resources useful. They are divided up into different sections in the tabs below.\n\n\nThere are a multitude of different pieces of software and different workflows that you can use to create beautiful and intelligible data visualizations.\nI think R is a very good choice. There are three reasons I love R for data viz.\n\nggplot2 is a fantastic library for making data visualizations - it is customizable and logical.\nThe R community is very supportive and makes learning and improving a pleasure.\nThe entire process of importing and cleaning data, visualizing it, and then sharing it is made easy in Rstudio."
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#resources",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#resources",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "Resources",
    "text": "Resources\nIn the tabs below, I have collected some resources that I hope will be beneficial on your journey.\n\n\nInstalling R\n\nGetting started\nBefore we can do the fun stuff, we need to install R and RStudio.\nThe Modern Dive textbook has a great primer on getting started in R with an explanation of the difference between R and RStudio:\n\n\n\nModern Dive analogy\n\n\nThe Rstudio Education team also have a great beginner guide to R that covers installation.\n\n\n\nLearning R\n\nVideos and textbooks for learning how to use R\n\nThe R bible\nThe definitive guide to using R in a modern manner is R for Data Science by Hadley Wickham and Garrett Grolemund. I am not sure that is the best place to start, but it is certainly a helpful resource to come back to once you‚Äôre up and running.\n\n\n\nThe R for Data Science textbook\n\n\n\n\nRStudio Education\nRstudio Education have put together a set of instructions for learning R at different levels of competency. These include great guides for installing R, R Studio and the tidyverse packages right at the beginning.\nThe beginner, intermediate and expert start guides are linked here.\nThe RStudio team have put a lot of effort into creating resources that are useful for many R users, from helping teachers take their course work online, to financial analysis, to creating a personal website and academic publishing. Have a look around their website for more.\n\n\n\nRstudio Education graphics\n\n\n\n\nOther great video resources\nI am a visual learner and as a consequence I like to watch how things are done and copy them. Because Youtube allows for pausing and recapping at your own pace, I think this is a great way to learn. I‚Äôve collected some videos in these resources which I think are a good place to start.\nDavid Robinson‚Äôs Youtube Channel\nDavid Robinson does a weekly screencast where he shows how to do exploratory data analysis and makes plots in R.\nHere is a video about analysing text from reviews of ramen noodles. It has some useful tips for drawing tie-fighter plots and interpreting linear regressions.\n\n\nHe also has a talk about tricks in the tidyverse.\n\n\nJulia Silge‚Äôs blog\nJulia Silge‚Äôs blog is mostly about doing modelling in R, and text modelling in particular. It is a very useful place to look if you want to understand the tidymodels framework which provides access to a host of machine learning techniques in a very easy to use manner. She also has a Youtube channel.\nHere is a video from Julia looking at sentiment analysis in R:\n\n\nThe tidy text mining textbook\nWritten by Julia Silge and David Robinson, this book gives a lovely introduction to text mining in R. It has lots of examples and includes all the code necessary. As a bonus it is free!\n\n\n\nTidy Text Mining Textbook\n\n\nAndrew Couch‚Äôs Youtube Channel has a marvelous set of walk throughs of innovative and exciting packages in R. He‚Äôs a young data scientist who is creating fantastic content focussed on R and the tidyverse.\nHere is one of his videos on creating functions with the tidyverse.\n\n\n\n\nAnother resource collection!\nThe LearnR4Free website compiled by Mine Dogucu has a great set of resources that are free to use for the learner.\n\n\n\n\nDatasets\n\nUseful sources of data to practice your skills on\nHaving great datasets available helps to keep me interested in learning new skills. Here are three sources of interesting data:\nThe Tidy Tuesday Project\nTidy Tuesday is a project run by the R for Data Science foundation - each week they share a real world data set and encourage R users to practice their skills. It has a really fantastic community aspect and if you share your work on Twitter people are always helpful and kind. Others share their work and their code so you can see how their plots were made. In their words,\n\nThe intent of Tidy Tuesday is to provide a safe and supportive forum for individuals to practice their wrangling and data visualization skills independent of drawing conclusions.\n\n\n\n\nTidy Tuesday\n\n\nData is Plural\n‚ÄúData is Plural is a weekly newsletter of useful/curious datasets, published by Jeremy Singer-Vine‚Äù. This sums it up - really interesting stuff from all over the internet.\n\n\n\nData is plural\n\n\nNaked Data\n‚ÄúNaked Data is a weekly newsletter which rounds up the best in data-driven storytelling created in or about the Global South.‚Äù Get the best bits of twitter without having to spend all day scrolling. Really great newsletter based in SA.\n\n\n\nNaked Data\n\n\n\n\n\nVisualization\n\nGreat places to get inspiration, figures and code from\n\nFundementals of data visualization\nThere are many books on the theory and practice of data visualization. I think Fundamentals of Data Visualization by Claus Wilke is an excellent choice to start with. The preface reads:\n\nIf you are a scientist, an analyst, a consultant, or anybody else who has to prepare technical documents or reports, one of the most important skills you need to have is the ability to make compelling data visualizations, generally in the form of figures. Figures will typically carry the weight of your arguments. They need to be clear, attractive, and convincing.\n\nThe book provides great advice for matching your data to a good chart type. For how to make the chosen chart in R, have a look at his course, linked below.\n\n\n\nThe best course in data viz\nClause Wilke‚Äôs Course on Data Visualization in R is a fantastic accompaniment to the book linked above. The description reads:\n\nIn this class, students will learn how to visualize data sets and how to reason about and communicate with data visualizations. A substantial component of this class will be dedicated to learning how to program in R. In addition, students will learn how to compile analyses and visualizations into reports, how to make the reports reproducible, and how to post reports on a website or blog.\n\nIt has wonderful sets of slides including this delightful primer on visualizing uncertainty, and accompnaying code that can be downloaded and worked through. Bonus: all of the slides are hosted on github such that you can see how they‚Äôre put together.\n\n\nThe ggplot2 bible\nThe definitive guide to ggplot2, The ggplot2 Book is a fantastic resource for novices and experts alike. It is easy to navigate and has helpful examples with code used to create each figure. (I love to consult the section on annotations as I never remember exactly how they work for some reason.)\n\n\n\nThe ggplot2 book\n\n\n\n\nOther great viz resources\nThe Storytelling with Data blog has an excellent selection of articles that give tips and tricks on how to design graphics that are easy to understand.\nTheir Chart Guide is also a fantastic starting point for how to pick a chart.\n\n\n\nSWD Chart Guide\n\n\nJonathan Tan‚Äôs exploration of Household Solar Energy in the US is an amazing data storytelling piece that shows all of the code required to create each figure. It makes great use of annotations to explain each component of the visualizations.\n\n\n\nJonathan Tan‚Äôs ggridge plot\n\n\nThe R Graph Gallery is a useful website which showcases different kinds of visualizations and provides the code that you need to use to recreate the figure yourself.\n\n\n\nR Graph Gallery\n\n\n\n\n\n\nCustom themes\n\nFor style that is consistent and graphics that are clear\nIf you want to create a style of graphics that communicate clearly to the general public (or are consistent across your organization), have a look at the BBC Visual and Data Journalism cookbook for R graphics. The guide is filled with fantastic examples of visualizations you might like to emulate, and code to do so. They also walk through customizing fonts, colours and annotations for consistency over time.\n\n\n\nBBC style graphics\n\n\nIf you want to create your own theme, Thomas Mock has a lovely guide on graphs that communicate well and look good.\nHe walks through a great example of where adding custom images on the plot makes sense:\n\n\n\nNFL team images example\n\n\n\n\n\nAesthetics\n\nIf you‚Äôre interested in learning more about how to make your charts look great.\nWilliam Chase‚Äôs talk on ‚ÄòThe Glamour of Graphics‚Äô is a fantastic resource on choosing colours and how to make your graphs pop.\n\n\nStephen Tracy‚Äôs breakdown of DataViz Sins You Should Never Commit is a walk-through on how not to confuse your audience. It explains the use of colour and secondary axes in Microsoft Excel plots.\nIf you are interested in learning more about fonts, Practical Typography is a great place to start. If you‚Äôre interested in choosing a font for a website, have a look at Google‚Äôs free fonts here.\n\n\n\nLato is a free Google font\n\n\nThis page from the R Graph Gallery gives a breakdown of the RcolorBrewer package and when to use its different sets of palettes.\n\nThe color brewer webpage gives fantastic advice on how to choose colours for maps, using the same theory as the package above. It also gives you the hex codes for each colour that you may copy to use yourself.\n\nThis video on colour theory is a useful primer on colour in data viz.\n\n\n\n\n\nEconometrics in R\n\nIf you‚Äôre looking to implement econometric techqniques in R\nGrant McDermott‚Äôs Data science for economists course is a great place to start. He has one lecture on regression that is super useful.\n\n\n\nGrant McDermott\n\n\n\n\n\nR Shiny\n\nIf you‚Äôre looking to make interactive web pages with R and R Markdown\nThe simplest way to get a quick and dirty interactive dashboard together is to use the flexdashboard package. Here is a link to the package. One of the great advantages of flexdashboard is that you can get amazing interactivity with just htmlwidgets, you do not need to use a shiny-runtime. In other words, you can create and host a static html page anywhere, for example on GitHub Pages.\n\n\n\nFlexdashboard\n\n\nIf you want more interactivity, David Robinson‚Äôs video on global crop yields shows a simple Shiny app, with code here.\n\n\nAnother video example of R Shiny comes from Andrew Couch‚Äôs YouTube channel.\n\n\nMastering Shiny is a more advanced authoritative book on Shiny, written by Hadley Wickham. It‚Äôs a great place to get an understanding of the core of the package.\n\n\n\nMastering Shiny\n\n\n\n\n\nRecreating Published Figures\n\nWhen you can‚Äôt get the authors of an article to share their data\nSometimes you may want to recreate a figure for your own paper from a journal article, but the data is not publicly accessible.\nOne option is to snip the image from the article and cite it in your own. The disadvantage of this is that the figure will not fit your colour scheme and may be horribly pixalated.\nAn alternative is to recreate the figure yourself by extracting the data from the image.\nA truly fantastic free tool for this purpose is WebPlotDigitizer. This web application allows you to upload a figure and grab the data from it.\nHave a look at a short video I made on how to use it here:"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "Welcome to the first in a series of data screencasts where I attempt to show you how great the R language is.\nThis post follows along with the data screencast and includes the code methodically.\nIt begins by reading in the data, then tidying it up, analysing it, making some visualizations and then performing some predictive modelling.\n\n\nI‚Äôm signed up to a fantastic newsletter called ‚ÄúData is Plural‚Äù curated by journalist Jeremy Singer-Vine. Truly, it is a treasure trove of interesting public datasets from all over the internet. You can sign up to the newsletter here if this sounds up your alley.\nThis week it included a link to a fun selection of chocolate bar reviews, which Jeremy described as:\n\nChocolate bar reviews. The Manhattan Chocolate Society‚Äôs Brady Brelinski has reviewed 2,500+ bars of craft chocolate since 2006, and compiles his findings into a copy-paste-able table that lists each bar‚Äôs manufacturer, bean origin, percent cocoa, ingredients, review notes, and numerical rating.\n\nA live link to the database is shown below.\n\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")\n\n\n\n\n\n\nI‚Äôve copied and pasted the data into an Excel spreadsheet, accessible on my Github if you want to download it an analyse it yourself.\nThe underlying data is served in a Javascript container on the website rather than vanilla HTML. This makes it a little bit more difficult to scrape with a package like rvest, for example. Hence the Excel spreadsheet.\nTo begin the analysis, we‚Äôll read in the Excel file using the readxl package, and the here package that helps us with file paths.1\nWe don‚Äôt need to load the packages via the library(readxl) command because we‚Äôre only going to use them once or twice. Instead we can call the name of the package followed by two colons and the command, as shown below.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# read in the data\ndf <- readxl::read_excel(here::here(\"posts\", \n                                    \"2022-01-17-flavours-of-cocoa\", \n                                    \"data\", \n                                    \"chocolate-bar-ratings.xlsx\"))\n\n# display the first six rows of tibble\nhead(df)\n\n# A tibble: 6 √ó 10\n    REF Company‚Ä¶¬π Compa‚Ä¶¬≤ Revie‚Ä¶¬≥ Count‚Ä¶‚Å¥ Speci‚Ä¶‚Åµ Cocoa‚Ä¶‚Å∂ Ingre‚Ä¶‚Å∑ Most ‚Ä¶‚Å∏ Rating\n  <dbl> <chr>     <chr>     <dbl> <chr>   <chr>     <dbl> <chr>   <chr>    <dbl>\n1  2454 5150      U.S.A.     2019 Tanzan‚Ä¶ Kokoa ‚Ä¶    0.76 3- B,S‚Ä¶ rich c‚Ä¶   3.25\n2  2458 5150      U.S.A.     2019 Domini‚Ä¶ Zorzal‚Ä¶    0.76 3- B,S‚Ä¶ cocoa,‚Ä¶   3.5 \n3  2454 5150      U.S.A.     2019 Madaga‚Ä¶ Bejofo‚Ä¶    0.76 3- B,S‚Ä¶ cocoa,‚Ä¶   3.75\n4  2542 5150      U.S.A.     2021 Fiji    Matasa‚Ä¶    0.68 3- B,S‚Ä¶ chewy,‚Ä¶   3   \n5  2546 5150      U.S.A.     2021 Venezu‚Ä¶ Sur de‚Ä¶    0.72 3- B,S‚Ä¶ fatty,‚Ä¶   3   \n6  2546 5150      U.S.A.     2021 Uganda  Semuli‚Ä¶    0.8  3- B,S‚Ä¶ mildly‚Ä¶   3.25\n# ‚Ä¶ with abbreviated variable names ¬π‚Äã`Company (Manufacturer)`,\n#   ¬≤‚Äã`Company Location`, ¬≥‚Äã`Review Date`, ‚Å¥‚Äã`Country of Bean Origin`,\n#   ‚Åµ‚Äã`Specific Bean Origin or Bar Name`, ‚Å∂‚Äã`Cocoa Percent`, ‚Å∑‚ÄãIngredients,\n#   ‚Å∏‚Äã`Most Memorable Characteristics`\n\n\nThis gives us a tibble (similar to a dataframe) with 10 columns (4 numeric and 6 character) and 2,530 individual reviews.\nThe column names are a big ugly though:\n\n# show column names\ndf %>% colnames()\n\n [1] \"REF\"                              \"Company (Manufacturer)\"          \n [3] \"Company Location\"                 \"Review Date\"                     \n [5] \"Country of Bean Origin\"           \"Specific Bean Origin or Bar Name\"\n [7] \"Cocoa Percent\"                    \"Ingredients\"                     \n [9] \"Most Memorable Characteristics\"   \"Rating\"                          \n\n\nWe can use the janitor package to make the column names snake case (lower case with words separated by an underscore).\n\n# clean names\ndf <- df %>% \n    janitor::clean_names()\n\n# show names again\ndf %>% colnames()\n\n [1] \"ref\"                              \"company_manufacturer\"            \n [3] \"company_location\"                 \"review_date\"                     \n [5] \"country_of_bean_origin\"           \"specific_bean_origin_or_bar_name\"\n [7] \"cocoa_percent\"                    \"ingredients\"                     \n [9] \"most_memorable_characteristics\"   \"rating\"                          \n\n\nNow that we have a nice tibble with clean names, we can ask what the data itself looks like. There are many ways to get summary statistics of a dataset. I love the skim function from the skimr package.\n\n# skim the dataset\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nref\n0\n1\n1429.80\n757.65\n5.00\n802.0\n1454.00\n2079.00\n2712\n‚ñÜ‚ñá‚ñá‚ñá‚ñá\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006.00\n2012.0\n2015.00\n2018.00\n2021\n‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ\n\n\ncocoa_percent\n0\n1\n0.72\n0.06\n0.42\n0.7\n0.70\n0.74\n1\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\nrating\n0\n1\n3.20\n0.45\n1.00\n3.0\n3.25\n3.50\n4\n‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñá\n\n\n\n\n\nGreat! Our reviews are almost all complete.\n\nOnly 3 percent are missing information on the ingredients.\nThe reviews begin in 2006, the mean review is from 2014, and the latest is from 2021.\nThe percent of the bar comprising of cocoa ranges from 42 to 100, with a mean of 72.\nWe have 62 unique countries of origin for the beans, and 67 countries of manufacture.\nThere are 21 unique combinations of ingredients, comprising of seven elements in total.\n\n\n\n\nLet‚Äôs have a look at that ingredients column.\n\n# count elements of ingredients column\ndf %>% \n    count(ingredients, sort = T)\n\n# A tibble: 22 √ó 2\n   ingredients      n\n   <chr>        <int>\n 1 3- B,S,C       999\n 2 2- B,S         718\n 3 4- B,S,C,L     286\n 4 5- B,S,C,V,L   184\n 5 4- B,S,C,V     141\n 6 <NA>            87\n 7 2- B,S*         31\n 8 4- B,S*,C,Sa    20\n 9 3- B,S*,C       12\n10 3- B,S,L         8\n# ‚Ä¶ with 12 more rows\n\n\nSo we have a number of ingredients, a dash, and then a key for what the ingredients are. Consulting the website reveals that there are seven possible ingredients:\n\n\n\n\n\nkey\nvalue\n\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\nThese key and value combinations are very sensible - if we have a lot of data we can save space by using the keys instead of the whole string. However, I would prefer to have them written out, because we‚Äôre going to split them into their own columns a little bit later.\nWe can use the str_replace_all function from the stringr package to replace items in the list of ingredients with names.\n\ndf <- df %>% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nLet‚Äôs look again at our ingredients column:\n\ndf %>% \n    count(ingredients, sort = T)\n\n# A tibble: 22 √ó 2\n   ingredients                                        n\n   <chr>                                          <int>\n 1 3- beans,sugar,cocoa_butter                      999\n 2 2- beans,sugar                                   718\n 3 4- beans,sugar,cocoa_butter,lecithin             286\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\n 5 4- beans,sugar,cocoa_butter,vanilla              141\n 6 <NA>                                              87\n 7 2- beans,non_sugar_sweetener                      31\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\n10 3- beans,sugar,lecithin                            8\n# ‚Ä¶ with 12 more rows\n\n\nFantastic! Now we have the number of ingredients, a dash, and then each ingredient by name in one column. Let‚Äôs separate this information into two columns so that we can use the number of ingredients as a feature.\nThe separate function from the tidyr package is made just for this purpose. It takes three arguments:\n\nthe name of the column to separate.\nnew column names corresponding to the number of elements.\nthe separator between elements.\n\n\ndf <- df %>% \n  separate(col = ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"),\n           sep = \"-\") %>% \n    # parse_number looks for a number inside a character column and discards the rest\n  mutate(n_ingredients = parse_number(n_ingredients),\n         # str_squish removes whitespace around the elements in the ingredients column\n         ingredients = str_squish(ingredients))\n\ndf %>% \n  select(n_ingredients, ingredients)\n\n# A tibble: 2,530 √ó 2\n   n_ingredients ingredients                      \n           <dbl> <chr>                            \n 1             3 beans,sugar,cocoa_butter         \n 2             3 beans,sugar,cocoa_butter         \n 3             3 beans,sugar,cocoa_butter         \n 4             3 beans,sugar,cocoa_butter         \n 5             3 beans,sugar,cocoa_butter         \n 6             3 beans,sugar,cocoa_butter         \n 7             3 beans,sugar,cocoa_butter         \n 8             4 beans,sugar,cocoa_butter,lecithin\n 9             4 beans,sugar,cocoa_butter,lecithin\n10             4 beans,sugar,cocoa_butter,lecithin\n# ‚Ä¶ with 2,520 more rows\n\n\nNow we have a numeric column with the number of ingredints and a column called ingredients with each element separated by a comma.\nFinally, let‚Äôs break the ingredients from a comma separated list into a binary variable for each ingredient. We can use the recipes package that is part of the tidymodels metapackage - a framework for doing statistical modelling in a tidy manner.\nFirst we break our ingredients into 6 columns. The problem we run into is that for bars that contain different ingredients, the order of the ingredients split into the columns is not constant.\n\n# df <- df %>% \n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\n#            sep = \",\") \n\n# df %>% \n#     select(company_manufacturer , starts_with(\"ingredient_\")) %>%\n#     slice(c(1L, 51L, 54L))\n\nPerhaps there is a better way to do this? Separate rows and pivot wider?\n\ndf %>% \n    separate_rows(ingredients, sep = \",\") %>%\n    count(ingredients)\n\n# A tibble: 8 √ó 2\n  ingredients             n\n  <chr>               <int>\n1 beans                2443\n2 cocoa_butter         1668\n3 lecithin              493\n4 non_sugar_sweetener    76\n5 salt                   37\n6 sugar                2360\n7 vanilla               353\n8 <NA>                   87\n\ndf <- df %>% \n    separate_rows(ingredients, sep = \",\") %>%\n    filter(!is.na(ingredients)) %>% \n    pivot_wider(names_from = ingredients, values_from = ingredients) %>% \n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nTidymodels\n\n# library(tidymodels)\n# dummy_multi_choice_rec <- recipe(~ ., data = df) %>%\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %>%\n#   prep()\n# \n# df <- bake(dummy_multi_choice_rec, new_data = NULL)\n\n\n\n\n\n\n\ndf %>% \n  count(rating, sort = T)\n\n# A tibble: 12 √ó 2\n   rating     n\n    <dbl> <int>\n 1   3.5    552\n 2   3      506\n 3   3.25   453\n 4   2.75   320\n 5   3.75   295\n 6   2.5    156\n 7   4      111\n 8   2       29\n 9   2.25    14\n10   1.5      5\n11   1        1\n12   1.75     1\n\n\nScore range between 1 and 4, and the modal value is 3.5.\n\n# histogram\ndf %>% \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\n    labs(x = \"Chocolate bar rating\",\n         y = \"Number of bars\")\n\n\n\n\n\n\n\n\ndf %>% \n    group_by(review_date) %>% \n    summarise(mean_rating = mean(rating)) %>% \n    ungroup() %>% \n    knitr::kable(digits = 2)\n\n\n\n\nreview_date\nmean_rating\n\n\n\n\n2006\n3.06\n\n\n2007\n3.17\n\n\n2008\n3.04\n\n\n2009\n3.09\n\n\n2010\n3.19\n\n\n2011\n3.27\n\n\n2012\n3.21\n\n\n2013\n3.21\n\n\n2014\n3.20\n\n\n2015\n3.24\n\n\n2016\n3.24\n\n\n2017\n3.36\n\n\n2018\n3.19\n\n\n2019\n3.13\n\n\n2020\n3.26\n\n\n2021\n3.32\n\n\n\n\n\nIt certainly seems like the mean rating is increasing over time. What is driving this?\nWe can make a plot of the figures above to see the increasing trend.\n\ndf %>% \n    group_by(review_date) %>% \n    summarise(mean_rating = mean(rating)) %>% \n    ungroup() %>% \n    ggplot(aes(review_date, mean_rating)) +\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\n    labs(x = \"Date of review\",\n         y = \"Mean rating\")\n\n\n\n\nLet‚Äôs make a boxplot to see how the spread of scores has changed over time.\n\ndf %>%\n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_jitter(alpha = .2) +\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\n\n\n\n\nIt seems as if the share of bars with very low scores has decreased over time, while the median value has remained relatively stable over time, shown by the bar in the centre of the boxplots.\nWhat about making a joy plot or ridgeline plot with the ggridges package? This allows us to see how the spread of values has changed over time.\n\nlibrary(ggridges)\n\ndf %>%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\nThis confirms what we saw in the boxplots above: fewer low scores in more recent years mean that the mean has increased, while the top of the distributions remain largely the same.\n\n\n\n\ndf %>% \n  mutate(cocoa_percent = round(cocoa_percent, 1)) %>% \n  count(cocoa_percent, n_ingredients) %>% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\n\n\n\n\ndf %>% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 √ó 2\n   country_of_bean_origin     n\n   <chr>                  <int>\n 1 Venezuela                246\n 2 Peru                     231\n 3 Dominican Republic       220\n 4 Ecuador                  201\n 5 Madagascar               171\n 6 Blend                    144\n 7 Nicaragua                100\n 8 Bolivia                   79\n 9 Colombia                  78\n10 Tanzania                  78\n# ‚Ä¶ with 52 more rows\n\ndf %>% \n  add_count(country_of_bean_origin) %>%\n  # only include countries with more than 60 bars\n  filter(n > 60) %>% \n  group_by(country_of_bean_origin) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %>% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\n\n\n\n\nlibrary(tmap)\ndata(\"World\")\n\nworld <- World %>% as_tibble()\n\nTo join our data on chocolate to this map, we need to get coutnry codes, using the countrycode package.\n\nlibrary(countrycode)\n\ndf <- df %>% \n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\n\n\nlibrary(sf)\n\ndf_map <- df %>% \n    group_by(iso_a3) %>%\n    add_count() %>% \n    summarise(mean_rating = mean(rating),\n              n = n) %>% \n    ungroup() %>% \n    distinct() %>% \n    left_join(world, by = \"iso_a3\")\n\n\ndf_map %>% \n    filter(n > 3) %>% \n    st_as_sf() %>% ggplot() +\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n    geom_sf(aes(fill = mean_rating)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    labs(fill = \"Mean country rating\")\n\n\n\n\n\n\n\n\n\ndf_characteristics <- df %>% \n  select(c(most_memorable_characteristics, rating)) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\n\ndf_characteristics %>% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 √ó 2\n   most_memorable_characteristics     n\n   <chr>                          <int>\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# ‚Ä¶ with 938 more rows\n\n\nWe can start with a naive analysis that looks only at average score per word. These are the highest scoring words.\n\n# df_characteristics %>% \n#   group_by(most_memorable_characteristics) %>% \n#   add_count() %>% \n#   mutate(avg_rating = mean(rating)) %>% \n#   ungroup() %>% \n#   slice_max(avg_rating, n = 12, with_ties = F)\n\ndf_characteristics %>% \n  group_by(most_memorable_characteristics) %>% \n  add_count() %>% \n  filter(n > 3) %>% \n  mutate(avg_rating = mean(rating)) %>% \n  ungroup() %>% \n  distinct(most_memorable_characteristics, avg_rating) %>% \n  slice_max(avg_rating, n = 12, with_ties = F) %>% \n    mutate(avg_rating = round(avg_rating, 2)) %>% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds <- vfold_cv(df_characteristics)\n\nglmnet_recipe <- \n  recipe(formula = rating ~ ., data = df_characteristics) %>% \n  step_tokenize(most_memorable_characteristics) %>% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% \n  step_tf(most_memorable_characteristics) %>% \n  step_normalize(all_predictors(), -all_nominal())\n\n\nglmnet_recipe %>% prep() %>% juice()\n\n# A tibble: 6,839 √ó 101\n   rating tf_most_memo‚Ä¶¬π tf_mo‚Ä¶¬≤ tf_mo‚Ä¶¬≥ tf_mo‚Ä¶‚Å¥ tf_mo‚Ä¶‚Åµ tf_mo‚Ä¶‚Å∂ tf_mo‚Ä¶‚Å∑ tf_mo‚Ä¶‚Å∏\n    <dbl>          <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 2   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 3   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 4   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 5   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 6   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 7   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 8   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 9   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n10   3           -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n# ‚Ä¶ with 6,829 more rows, 92 more variables:\n#   tf_most_memorable_characteristics_bitter <dbl>,\n#   tf_most_memorable_characteristics_black <dbl>,\n#   tf_most_memorable_characteristics_bland <dbl>,\n#   tf_most_memorable_characteristics_bold <dbl>,\n#   tf_most_memorable_characteristics_bright <dbl>,\n#   tf_most_memorable_characteristics_brownie <dbl>, ‚Ä¶\n\n\n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\n\nglmnet_tune %>% \n  autoplot()\n\n\n\n\n\nglmnet_model_final <- finalize_workflow(glmnet_workflow, glmnet_tune %>% \n  select_best())\n\nfinal_fit <- glmnet_model_final %>% \n  fit(df_characteristics)\n\nWhat does the fit look like? These are the terms that have the greatest effect on bar rating.\n\nfinal_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %>%\n  mutate(sign = estimate > 0) %>%\n  group_by(sign) %>%\n  mutate(estimate = abs(estimate)) %>% \n  slice_max(estimate, n = 12) %>%\n  ungroup() %>%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %>% \n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\nWow! have a look at the terms up top - ‚Äúcreamy‚Äù, ‚Äúcomplex‚Äù and ‚Äúrich‚Äù are good chocolate words. On the other side, ‚Äúbitter‚Äù, ‚Äúoff‚Äù and ‚Äúchemical‚Äù are terms that lower the score of the bar."
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Fathom data are an analytics and data science training firm which offers training in two of the most popular open source languages for statistical and general purpose programming: #Rstats and #Python. In their words:\n\n\nWe use 100% #opensource tools and we contribute to a number of Open Source projects. There's no doubt that Open Source adds value to @fathomdata, but it's hard to know what value we're creating. Some interesting perspectives on quantifying value.https://t.co/hcw5hylTwn\n\n‚Äî Fathom Data (@fathomdata) January 24, 2022\n\n\nSome of the staff state a preference for R, and I wanted to see if this preference was borne out in the tweets on their corporate twitter account.\n\n\nTo quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n# get the tweets!\ndf <- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\n\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew‚Äôs account.\n\ndf %>% \n    count(screen_name) %>% \n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\n\n\n\n\nAccount\nNumber of tweets\n\n\n\n\ndatawookie\n3198\n\n\nfathomdata\n448\n\n\n\n\n\n\n\n\nLet‚Äôs start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\n\nhashtags <- df %>%\n  filter(screen_name == \"fathomdata\") %>%\n  unnest(cols = c(hashtags)) %>%\n  filter(!is.na(hashtags)) %>%\n  mutate(hashtags = str_to_lower(hashtags)) %>%\n  count(hashtags, is_retweet) %>%\n  mutate(is_retweet = if_else(is_retweet == TRUE,\n    \"Retweet\",\n    \"@fathomdata tweet\"\n  ))\n\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\nI‚Äôve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\n\nlibrary(tidytext)\n# ggsci package for nice blue fill scale\nlibrary(ggsci)\n\nhashtags %>% \n  group_by(is_retweet) %>% \n  slice_max(n, n = 12, with_ties = F) %>% \n  ungroup() %>% \n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %>% \n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %>% \n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\n  geom_col(show.legend = F) +\n  scale_fill_material(\"blue\") +\n  scale_color_manual(values = c(\"white\", \"red\")) +\n  scale_y_reordered() +\n  facet_wrap(~ is_retweet, scales = \"free\") +\n  labs(x = \"Number of tweets tagging\",\n       y = \"Hashtag\",\n       caption = \"Note: red outline on bar indicates language\")\n\n\n\n\nColumn chart of hashtag frequency by tweet type\n\n\n\n\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\n\nhashtags %>%\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %>%\n  mutate(hashtags = fct_reorder(hashtags, n),\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %>%\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\n  geom_col(position = \"fill\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Share of tweets tagging langauge\",\n    fill = \"Language\",\n    y = NULL\n  ) +\n  scale_fill_npg(\n    guide = guide_legend(reverse = TRUE)\n  )\n\n\n\n\nStacked column chart comparing language hashtag frequency by tweet type\n\n\n\n\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\n\ndf %>%\n  select(text) %>%\n  mutate(\n    rstats = str_detect(text, \"rstats\"),\n    python = str_detect(text, \"python\")\n  ) %>%\n  summarise(across(c(rstats, python), sum)) %>%\n  pivot_longer(everything(),\n    names_to = \"language\",\n    values_to = \"n\"\n  ) %>% \n    mutate(share = scales::percent(n / (496 + 134)))\n\n# A tibble: 2 √ó 3\n  language     n share\n  <chr>    <int> <chr>\n1 rstats     496 79%  \n2 python     134 21%  \n\n\n\n\n\nIn the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\n\ndf %>%\n  filter(\n    str_detect(text, \"rstats\"),\n    screen_name == \"fathomdata\",\n    !is.na(retweet_screen_name)\n  ) %>%\n  count(retweet_name, sort = T) %>%\n  slice_head(n = 12) %>%\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\n\n\n\n\nAccout name\nNumber of retweets\n\n\n\n\nAndrew B. Collier\n43\n\n\nR-bloggers\n19\n\n\nMegan Beckett\n16\n\n\nR posts you might have missed!\n10\n\n\nsatRday South Africa\n6\n\n\nMara Averick\n5\n\n\nAstrid Radermacher\n3\n\n\nRStudio\n3\n\n\nTom Mock\n3\n\n\nFathom Data\n2\n\n\nR-Ladies Cape Town\n2\n\n\nR Weekly Live\n2\n\n\n\n\n\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap.\n\n\n\nFollowing the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\n\n# taking out URLs\nremove_bits <- \"&amp;|&lt;|&gt;\"\n\ntidy_tweets <- df %>%\n  filter(!str_detect(text, \"^RT\")) %>%\n  mutate(text = str_remove_all(text, remove_bits)) %>%\n  unnest_tokens(word, text, token = \"tweets\") %>%\n    # remove stopwords\n  filter(\n    !word %in% stop_words$word,\n    !word %in% str_remove_all(stop_words$word, \"'\"),\n    str_detect(word, \"[a-z]\")\n  ) \n\nfrequency <- tidy_tweets %>% \n  group_by(screen_name) %>% \n  count(word, sort = TRUE) %>% \n  left_join(tidy_tweets %>% \n              group_by(screen_name) %>% \n              summarise(total = n())) %>%\n  mutate(freq = n/total)\n\nfrequency <- frequency %>% \n  select(screen_name, word, freq) %>% \n  pivot_wider(names_from = screen_name, values_from = freq) %>%\n  arrange(desc(datawookie), fathomdata)\n\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew‚Äôs tweets.\n\nfrequency %>% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  theme(legend.position = \"none\") +\n    labs(x = \"Andrew Collier\",\n         y = \"Fathom Data\")\n\n\n\n\nComparing the frequency of words used by the Fathom Data account and Andrew Collier\n\n\n\n\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\n\nfrequency %>% \n  mutate(highlight = word == \"@standardbankza\") %>% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\nHighlighting Standard Bank account mentions\n\n\n\n\nThis is clearly the case! Have a look at the top terms, haha!\n\nfrequency %>%\n  filter(is.na(fathomdata)) %>%\n  select(-fathomdata) %>%\n  mutate(datawookie = scales::percent(datawookie)) %>%\n  slice_head(n = 10) %>%\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\n\n\n\n\nWord\nRelative specificity to Andrew Collier‚Äôs tweets\n\n\n\n\nkak\n0.6723%\n\n\n#customerservice\n0.6666%\n\n\n@rainsouthafrica\n0.5759%\n\n\nmind\n0.5220%\n\n\nmbps\n0.4255%\n\n\nprepared\n0.3773%\n\n\nlinux\n0.3489%\n\n\n#itcanbe\n0.3262%\n\n\n#fixit\n0.2354%\n\n\nworst\n0.2241%\n\n\n\n\n\n\n\n\nIt appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "",
    "text": "Thank you to Adam A. Kemberling for providing some useful feedback on how to explain the panelset syntax and suggesting the use of dplyr::walk() instead of a for loop. His suggestion is here.\nThis is a poor solution. In the case where there are only a small number of possible categories, it works. However, if there are many potential tab names and many are empty, having to click through and find content is not pleasing at all!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#purpose",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#purpose",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Purpose",
    "text": "Purpose\nI want to create a parametrized report in Rmarkdown. The report is populated after querying a database. In short, the Rmarkdown document must have a tab for each category of a categorical variable, and draw a ggplot in the tab. The number of categories depends on the result of the query.\nWe can simulate the problem by removing one of the islands from the palmerpenguins dataset:\nThere are three islands:\n\n\n\n\n\n\n\n\nIsland name\nNumber of penguins\n\n\n\n\nBiscoe\n168\n\n\nDream\n124\n\n\nTorgersen\n52\n\n\n\n\n\nWe will subset the data to choose only two islands:\n\ndf <- palmerpenguins::penguins %>% \n    filter(island != \"Biscoe\")\n\ndf %>%\n  distinct(island) %>%\n  knitr::kable(col.names = c(\"Remaining islands\"))\n\n\n\n\nRemaining islands\n\n\n\n\nTorgersen\n\n\nDream"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#problem",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#problem",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Problem",
    "text": "Problem\nIf I hardcode the number of tabs one of the plots is drawn but has no data!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Biscoe",
    "text": "Biscoe\n\nplot_penguins(\"Biscoe\")"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe-1",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe-1",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Biscoe",
    "text": "Biscoe\n\nplot_penguins(\"Biscoe\")"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#better-solution",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#better-solution",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Better solution",
    "text": "Better solution\nA better solution is to generate the tabs programmatically, with code that writes itself depending on the contents of the returned dataset.\nI am creating the tabs with Garrick Aden-Buie‚Äôs xaringanExtra package. Have a look at the specifics in the presentation below.\n\nknitr::include_url(\"https://pkg.garrickadenbuie.com/xaringanExtra/panelset/?panelset=r-code#2\")\n\n\n\nOther useful resources incude this Science Loft post and this Stack Overflow question.\n\nTools\nIn vanilla Rmarkdown you can create the tabs with a simple heading structure.\n\n## Here are the penguins {.panelset} \n\n### Biscoe\nplot_penguins(\"Biscoe\")\n### Dream\nplot_penguins(\"Dream\")\n\nI opt to use Pandoc‚Äôs fenced div structure instead, to avoid inadvertently creating additional tabs with a third level heading. It looks like this:\n\n::::: {.panelset}\n\n::: {.panel}\n\n[Biscoe]{.panel-name}\n\nplot_penguins(\"Biscoe\")\n:::\n    \n::: {.panel}\n\n[Dream]{.panel-name}\n\nplot_penguins(\"Dream\")\n\n:::\n\n:::::\n\nIt is worth highlighting that the tabset\n\nbegins with ::::: {.panelset} and,\nends with five colons :::::\n\nEach individual tab needs to\n\nopen with ::: {.panel},\ninclude a [Panel name specified here]{.panel-name}, and\nclose with three colons :::"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#how-does-it-work",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#how-does-it-work",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "How does it work?",
    "text": "How does it work?\nNow I can programmatically create my tabs and plots within the fences. I start with a function that makes the tabs. Then I open the tabset, get a list of the islands present, and walk across each one applying my make_tab function before closing the tabset.\n\nmake_tab <- function(isl) {         # function to make the tabs\n  cat(\"::: {.panel}\\n\")             # Open tab\n  cat(\"##\", isl, \"{.panel-name}\\n\") # Label tab\n  p <- plot_penguins(isl)           # Create plot\n  print(p)                          # Display plot\n  cat(\"\\n\")                         # Space\n  cat(\":::\\n\")                      # Close tab\n}\n\n::::: {.panelset}\n\nisl_list <- unique(df$island) %>%   # Get a list of islands present in the dataset\n    as.character()\n\ndplyr::walk(isl_list, make_tab)     # make the tabs for each island that is there\n\n:::::\n\n\nGood to remember\n\nWrap code you want executed in cat(), and print() your ggplot objects.\nSet code options to results = 'asis' for the code chunk that makes your tabs. According to the Rmarkdown Cookbook:\n\n\nThis option tells knitr not to wrap your text output in verbatim code blocks, but treat it ‚Äúas is.‚Äù This can be particularly useful when you want to generate content dynamically from R code.\n\n\nPut in lots of line breaks to separate Rmarkdown content, e.g.¬†frequent uses of cat(\"\\n\") between elements."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#does-it-work",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#does-it-work",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Does it work?",
    "text": "Does it work?\nYes! Look at that, just two tabs and no empty plots!\n\n\n\n\nisl_list <- unique(df$island) %>% as.character()\n\nwalk(isl_list, make_tab)\n\nTorgersen\n\n\n\n\n\nDream\n\n\n\n\n\nI hope that this was useful. Please create an issue on Github if you have any suggestions."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#torgersen-2",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#torgersen-2",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Torgersen",
    "text": "Torgersen"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#dream-2",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#dream-2",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Dream",
    "text": "Dream"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#postscript",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#postscript",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Postscript",
    "text": "Postscript\n\nThis is the function that plots the figures:\n\nplot_penguins <- function(isl) {\n  df %>%\n    filter(island == isl) %>%\n    ggplot(aes(bill_length_mm, \n               bill_depth_mm, \n               colour = body_mass_g, \n               shape = species)) +\n    geom_point(cex = 2) +\n    geom_smooth(method = \"lm\", se = F) +\n    scale_color_viridis_c(labels = scales::number_format()) +\n    labs(\n      x = \"Bill length (mm)\",\n      y = \"Bill depth (mm)\",\n      colour = \"Body mass (g)\",\n      shape = \"Penguin species\"\n    )\n}"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html",
    "title": "LinkedIn style tables for personal websites",
    "section": "",
    "text": "Katherine asked me if she could include the university logo in her education section on the about page on her website. The about page is made with the postcards package. I didn‚Äôt know how to do it but thought that a table might make the most sense.\nI really liked the style of the LinkedIn education sections:\n\nknitr::include_graphics(\"images/linkedin_education.PNG\")\n\n\n\n\nLinkedIn screenshot\n\n\n\n\nIn this post I walk through recreating this table with the gt package in R."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#where-to-begin",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#where-to-begin",
    "title": "LinkedIn style tables for personal websites",
    "section": "Where to begin?",
    "text": "Where to begin?\nThe education section in the LinkedIn table has six components for each entry. They are:\n\nA logo photo\nA hyperlink to the institution though the image\nName of the institution (in bold)\nTitle of the degree\nDate (start and end year)\nGrade or comment\n\nLet‚Äôs create a tibble with these components.\n\nlibrary(tidyverse)\n\neducation <- tribble(\n  ~logo, ~link, ~institution, ~degree, ~date, ~grade_comment,\n  # LU\n  \"images/lu.jfif\", \"https://ehl.lu.se/ekonomisk-historia\", \"Lund University\", \"Master's degree, Economics\", \"2021\", \"Exchange semester\",\n  # SU\n  \"images/su.jfif\", \"https://www.ekon.sun.ac.za/\", \"Stellenbosch University\", \"Master's degree, Economics\", \"2020\", \"Grade: <em>cum laude</em>\",\n  # uct hnours\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Honours degree, Economics\", \"2018\", \"Grade: 2:1\",\n  # uct undergrad\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Bachelor's degree, Economics and Economic History\", \"2015-2017\", \"Grade: 2:1\",\n)"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#building-the-table",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#building-the-table",
    "title": "LinkedIn style tables for personal websites",
    "section": "Building the table",
    "text": "Building the table\nNow lets start on the table structure. What we‚Äôre gonna do is take the individual pieces and format them to html, then stick the text together in one cell and the logos/links in another.\n\neducation <- education %>%\n  mutate(\n    logo = glue::glue(\"<a href = {link}>\n                        <img src='{logo}' width='50' height='50'>\n                      </a>\"),\n    institution = glue::glue(\"<b>{institution}</b>\"),\n    date = glue::glue(\"<span style='opacity: .7;'> {date} </span>\"),\n    grade_comment = glue::glue(\"<span style='opacity: .7;'> {grade_comment} </span>\"),\n  )\n\nIn the cell above we create an image tag for the logo, putting a hyperlink tag for the link to the university website inside it. Here we use the glue package which is useful for working with strings constructed from other columns. You can think of it as gluing together the html tag parts and the content from our dataframe. Next we put the institution in bold, and grey out the dates and grades with the html style tag, 'opacity: .7;'.1\nNow we can stick these text components together into one cell (just called text) and keep only the logo and text columns.2 Finally we use the map function from the purrr package to transform the character strings into html. Printing the tibble shows that it now contains html elements inside each cell.\n\neducation <- education %>%\n  mutate(text = str_c(\n    institution, \"<br>\",\n    degree, \"<br>\",\n    date, \"<br>\",\n    grade_comment\n  )) %>%\n  select(logo, text) %>%\n  mutate(\n    logo = purrr::map(logo, gt::html),\n    text = purrr::map(text, gt::html)\n  )\n\neducation\n\n# A tibble: 4 √ó 2\n  logo       text      \n  <list>     <list>    \n1 <html [1]> <html [1]>\n2 <html [1]> <html [1]>\n3 <html [1]> <html [1]>\n4 <html [1]> <html [1]>\n\n\nNow we create the table by calling the gt() command.\n\nlibrary(gt)\n\neducation %>% \n    gt()\n\n\n\n\n\n  \n  \n    \n      logo\n      text\n    \n  \n  \n    \n  \n\nLund UniversityMaster's degree, Economics 2021  Exchange semester \n    \n  \n\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \n    \n  \n\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \n    \n  \n\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1 \n  \n  \n  \n\n\n\n\nGreat, there it is!"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#finalizing-the-table",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#finalizing-the-table",
    "title": "LinkedIn style tables for personal websites",
    "section": "Finalizing the table",
    "text": "Finalizing the table\nWe still want to make some changes to tidy it up a bit:\n\nAdding a table header\nAligning the text to the left\nRemoving the column labels\n\n\neducation %>%\n  gt() %>%\n  tab_header(md(\"**Education**\")) %>%\n  cols_align(align = c(\"left\"), columns = text) %>%\n  cols_label(\n    logo = \"\",\n    text = \"\",\n  )\n\n\n\n\n\n  \n    \n      Education\n    \n    \n  \n  \n    \n      \n      \n    \n  \n  \n    \n  \n\nLund UniversityMaster's degree, Economics 2021  Exchange semester \n    \n  \n\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \n    \n  \n\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \n    \n  \n\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1 \n  \n  \n  \n\n\n\n\nLovely! Now we have the table we can use in the about page of a blog or personal website, with nice clickable logos that take you to the website of the institution."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "title": "Our World In Data choropleth",
    "section": "",
    "text": "I really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, ‚Äúmake progress against the world‚Äôs largest problems‚Äù.\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser last year, included below.\n\n\n\n\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "title": "Our World In Data choropleth",
    "section": "Context",
    "text": "Context\nThere is a lot of information about the OWID grapher tool. You can have a look at their github repo and an older reddit AMA if you are interested. It‚Äôs a custom system that allows for systematic changes across their website, drawing on data from a central database."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "title": "Our World In Data choropleth",
    "section": "Components",
    "text": "Components\nWhat are the parts I want to recreate? The map has:\n\na base map, where the colour fill of each country indicates it‚Äôs position in a specific measure in a particular year.\na simple line chart that appears when you hover over a country, showing how the measure has changed within a country over time.\na clear legend\na note specifying the source of the data\n\nI walk through creating each of these below.\n\nThe world map\nThe base map is sourced from the maps package. I add a three letter country code from the english name of the country using the countrycode package and filter out Antarctica, Greenland and the French Southern and Antarctic Lands.\n\n\nShow the code\n# preamble\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n# load map\nmap <- st_as_sf(maps::map(database=\"world\", plot = FALSE, fill = TRUE))\n\n# create code to match coutnry to data with\nlibrary(countrycode)\nmap <- map %>% \n  mutate(code = countrycode(ID, \"country.name\", \"iso3c\"))\n\n# remove clutter from map\ncountry_to_remove <- c(\n  'Antarctica','Greenland', 'French Southern and Antarctic Lands'\n)\n\nmap <- map %>% \n  filter(!ID %in% country_to_remove)\n\n\nThe base map is projected with the Web Mercator or WGS 84 projection, the same one Google Maps uses.\n\n\nShow the code\nmap %>% \n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\nData\nI source the data from the Our World in Data website (the WHO collects the data and the World Bank distributes it). We read in the data as a CSV file, and tidy up the column names so that they are in snake case with the clean_names() command from the very helpful janitor package.\n\n\nShow the code\ndf <- read.csv(\"data/share-of-adults-who-smoke.csv\")\n\ndf <- df %>% \n  as_tibble() %>% \n  dplyr::rename(value = Prevalence.of.current.tobacco.use....of.adults.) %>% \n  janitor::clean_names()\n\n\nNext we remove the summary groups which we cannot show on the map, including the World Bank country income groupings.\n\n\nShow the code\ndf %>% \n  filter(!code %in% map$code) %>% \n  distinct(entity)\n\n\n# A tibble: 16 √ó 1\n   entity                      \n   <chr>                       \n 1 East Asia and Pacific       \n 2 Europe and Central Asia     \n 3 European Union              \n 4 High income                 \n 5 Latin America and Caribbean \n 6 Low and middle income       \n 7 Low income                  \n 8 Lower middle income         \n 9 Middle East and North Africa\n10 Middle income               \n11 North America               \n12 South Asia                  \n13 Sub-Saharan Africa          \n14 Tuvalu                      \n15 Upper middle income         \n16 World                       \n\n\nShow the code\ndf <- df %>% \n  filter(code %in% map$code)\n\n\n\n\nCreate a colour palette\nWhat we want to do is use the scale_color_viridis_c() palette. We have to map it to the min and max of the values in our dataset for smoking so that we get a nice mapping across the colour palette.\n\n\nShow the code\ndf %>%\n  summarise(\n    min = min(value),\n    max = max(value)\n  )\n\n\n# A tibble: 1 √ó 2\n    min   max\n  <dbl> <dbl>\n1   3.5  68.5\n\n\nShow the code\nlibrary(viridisLite) \n\nvir_10 <- viridis(n = 10)\n\nsmoking_hex <- scales::gradient_n_pal(\n  colours = vir_10,\n  values = seq(0, 50, by = 5)\n)\n\nscale_smoking <- function() {\n  scale_color_gradientn(\n    colours = vir_10,\n    values = seq(0, 50, by = 5) / 50,\n    limits = c(0, 68.5),\n    name = \"value\"\n  )\n}\n\n\n\n\nHow to plot the line graph?\nThe line graph that appears when you hover over OWID map is very simple. It has just the starting value on the y-axis, and the first and last years on the x-axis, and a line that changes colour in accordance with the scale of the choropleth. The hover window which contains the graph also shows the country name, and the value of the measure in the most recent year.\nTo recreate it, we need store these four values, and draw the coloured line.\n\n\n\nA function for plotting the graph\nFirst we write a function to plot the line that is a very minimal ggplot, removing lots of superfluous elements with the theme() command.\n\n\nShow the code\n# Here the function to plot the line takes only one argument, `cd` the country code \nplot_line <- function(cd) {\n  # get axis marks\n  label_y <- df %>%\n    filter(code == cd) %>%\n    mutate(\n      min_year = min(year),\n      max_year = max(year)\n    ) %>%\n    filter(year == min(year))\n  \n  # plot the line\n  df %>%\n    filter(code == cd) %>%\n    ggplot(aes(year, value)) +\n    geom_point(cex = 3) +\n    # mapping the colour of the line segment to the value is done here\n    geom_line(aes(colour = value), cex = 2, alpha = .7) +\n    # this scale is created above, with bounds appropriate to this data\n    scale_smoking() +\n    scale_y_continuous(\n      # specifying the break on the y-axis creates the axis text\n      breaks = c(label_y$value),\n      labels = scales::percent_format(scale = 1, accuracy = .1),\n      # the limits argument here ensures the y-axis starts at zero\n      limits = c(0, NA)\n    ) +\n    scale_x_continuous(\n      # x-axis needs only two years, the first and last\n      breaks = c(label_y$min_year, label_y$max_year)) +\n    theme(\n      # removing the axis ticks and lines clears the graph of clutter\n      axis.ticks.y = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor.x = element_blank(),\n      legend.position = \"none\",\n      text = element_text(size = 20)\n    ) +\n    labs(\n      x = NULL,\n      y = NULL\n    )\n}\n\n# Test the function with India.\nplot_line(\"IND\")\n\n\n\n\n\n\n\nNext to make the table that displays the most recent value\nWe write a function to make a gt() table that contains just two cells. In the left cell of the table is our line graph from above that shows the evolution of smoking rates in each country from 2000 onwards. In the right cell is the most recent value for the metric.\n\n\nShow the code\nlibrary(gt)\nlibrary(gtExtras)\n\nmake_table <- function(cd) {\n  message(\"Making table for \", cd)\n  vars <- df %>%\n    filter(code == cd) %>%\n    filter(year == max(year))\n\n  plot <- plot_line(cd)\n\n  # the `ggplot_image` command outputs an image that can easily be put into a gt table when formatted as markdown\n  plot <- gt::ggplot_image(plot, height = px(250), aspect_ratio = 1.6)\n\n  tbl <- tibble(plot = plot, value = vars$value, context = glue::glue(\"in {vars$year}\"))\n\n  gt(tbl) %>%\n    fmt_markdown(columns = c(plot)) %>%\n    fmt_percent(value, scale_values = F, decimals = 1) %>% \n    # the `merge_stack` command joins the value and the year in one cell\n    # the `smoking_hex` function we created above makes the text coloured appropriately\n    gt_merge_stack(col1 = value, col2 = context, palette = c(smoking_hex(tbl$value), \"grey\")) %>%\n    tab_style(\n      style = cell_text(size = \"xx-large\"),\n      locations = cells_body(\n        columns = c(value)\n      )\n    ) %>%\n    tab_header(\n      # title table with coutnry name\n      title = md(glue::glue(\"**{vars$entity}**\"))) %>%\n    tab_options(column_labels.hidden = TRUE) %>% \n    as_raw_html(inline_css = F)\n}\n\n# Test on South Africa\nmake_table(\"ZAF\")\n\n\n\n\n  \n  \n  \n    \n      South Africa\n    \n    \n  \n  \n  \n    \n\n20.3%\nin 2020\n  \n  \n  \n\n\n\n\n\n\nCreating the plots for each country\nHere we use the purrr::map command to make the table in raw HTML for each country and save it inside a tibble. The output shows an HTML list in the column called gt.\n\n\nShow the code\ngt_tables <- df %>% \n  distinct(code) %>%\n  mutate(gt = purrr::map(code, make_table))\n\ngt_tables\n\n\nWe then create a tibble called df_map that selects the most recent year for each country from the dataset and joins it to the map by the country code variable we created above. Finally we join this to the tibble of tables called gt_tables.\n\n\nShow the code\ndf_map <- df %>% \n  group_by(entity) %>% \n  filter(year == max(year)) %>% \n  ungroup() %>% \n  left_join(map, by = c(\"code\"))\n\ndf_map <- df_map %>% \n  inner_join(gt_tables)\n\n\n\n\nCreating the interactive figure\nNow we are ready to create the interactive figure!\nWe begin by drawing a static map in grey, with data from the original map. Next we overlay the interactive choropleth. The grey static map will show through all the countries we don‚Äôt have data on in the dataset.\n\n\nShow the code\ng <- df_map %>%\n  ggplot(aes(geometry = geom)) +\n  geom_sf(data = map, fill = \"grey80\") +\n  geom_sf_interactive(aes(fill = value, tooltip = gt)) +\n  scale_fill_binned(type = \"viridis\", labels = scales::percent_format(scale = 1)) +\n  cowplot::theme_minimal_grid() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = .5, title.position = \"top\", label = TRUE)) +\n  labs(\n    fill = \"Share of adults who smoke, 2020\",\n    caption = \"Source: World Health Organization (via World Bank)\"\n  ) +\n  theme(\n    plot.background = element_rect(fill = \"white\", color = NA),\n    plot.title = element_text(hjust = 0.5, family = \"marker\", size = 50),\n    plot.subtitle = element_markdown(size = 20, family = \"open\", lineheight = 0.5),\n    plot.caption = element_markdown(size = 12, family = \"open\"),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid.major = element_line(color = \"grey80\", size = 0.1),\n    legend.title.align = .5\n  )\n\n\n\n\n\n\n\n\n\nShow off the interactive figure\nWow! Have a look at that! Pretty neat, and similar to the OWID explorer. I might make this more extendable in the future that you can upload your own data, but that‚Äôs for another day.\n\n\nShow the code\n# customizing the CSS makes the hover box easier to read.\ntooltip_css <- \"background-color:gray;color:white;padding:10px;border-radius:5px;text-align:center;\"\n\nggiraph(\n  ggobj = g,\n  options = list(\n    opts_tooltip(css = tooltip_css),\n    opts_sizing(width = 1)\n  )\n)\n\n\n\n\n\n\n\n\n\nWhich African country has seen the largest reduction in smoking?\nFinally, we can plot the evolution of smoking behaviour among adults in Africa, highlighting my home nation of South Africa.\n\n\nShow the code\nget_ranking <- function(continent_in) {\n  message(\"Getting ranking for \", continent_in)\n\n  continent_out <- df %>%\n    mutate(continent = countrycode(code, \"iso3c\", \"continent\")) %>%\n    filter(continent == continent_in)\n\n  df_rank <- continent_out %>%\n    select(year, country_name = entity, value) %>%\n    distinct()\n\n  df_rank <- df_rank %>%\n    group_by(year) %>%\n    mutate(rank = rank(desc(value), ties.method = \"first\")) %>%\n    ungroup()\n\n  labels_left <- df_rank %>%\n    filter(year == min(year)) %>%\n    mutate(\n      left_rank = rank,\n      left_value = value\n    ) %>%\n    select(country_name, left_rank, left_value)\n\n  labels_right <- df_rank %>%\n    filter(year == max(year)) %>%\n    mutate(\n      right_rank = rank,\n      right_value = value\n    ) %>%\n    select(country_name, right_rank, right_value)\n\n  df_rank <- df_rank %>%\n    inner_join(labels_left) %>%\n    inner_join(labels_right)\n  \n  return(df_rank)\n}\n\nafrica_rank <- get_ranking(\"Africa\")\n\n\n\n\nShow the code\nplot_levels <- function(continent_in, highlight_country_in){\n  \n  tbl <- get_ranking(continent_in)\n  \n  midpoint_in <- tbl %>% \n    summarise(mean(value)) %>% \n    pull()\n  \n  tbl %>%\n  mutate(across(contains(\"value\"), ~ round(.x, digits = 1))) %>%\n  ggplot(aes(year, value, colour = right_value, group = country_name)) +\n  geom_line(size = 2.8, aes(year, value, group = country_name), colour = \"black\") +\n  geom_line(size = 2) +\n  geom_line(size = 3, colour = \"black\", data = tbl %>% filter(country_name == highlight_country_in)) +\n  geom_text(aes(\n    x = 2000,\n    y = left_value,\n    label = paste0(country_name, \" \", left_value, \" %\")\n  ),\n  check_overlap = T,\n  colour = \"grey20\",\n  hjust = 1.1,\n  cex = 3\n  ) +\n  geom_text(aes(\n    x = 2020,\n    y = right_value,\n    label = paste0(country_name, \" \", right_value, \" %\")\n  ),\n  check_overlap = T,\n  colour = \"grey20\",\n  hjust = 0,\n  cex = 3\n  ) +\n  # scale_y_reverse() +\n  scale_color_gradient2(\n    low = \"#3C5488\",\n    high = \"#00A087\",\n    mid = \"#4DBBD5\",\n    midpoint = midpoint_in\n  ) +\n  scale_x_continuous(breaks = c(2000, 2005, 2010, 2015, 2018, 2019, 2020)) +\n  coord_cartesian(xlim = c(1995, 2025)) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percentage of adults who smoke\",\n    title = glue::glue(\"Evolution of percentage of adults who smoke in {continent_in}\"),\n    subtitle = glue::glue(\"{highlight_country_in} is highlighted in black\"),\n    caption = \"Data: WHO via World Bank\\nGraphic: @JonathanJayes\"\n  )\n  \n}\n\n\n# jpeg(\n#   filename = \"posts/2022-10-17-our-world-in-data-choropleth/images/Africa_levels.jpeg\",\n#   height = 10,\n#   width = 8,\n#   units = \"in\",\n#   res = 1000\n# )\n\nplot_levels(continent_in = \"Africa\",\n            highlight_country_in = \"South Africa\")\n\n\n\n\n\nShow the code\n# dev.off()\n\n\nHmm, South Africa has seen a drop in smoking since 2000, but the drop is relatively small in comparison to the progress that other African nations have made in the last two decades.\nWe can also plot the evolution of the ranking of who smokes the most in Africa.\n\n\nShow the code\nplot_ranking <- function(continent_in, highlight_country_in){\n  \n  tbl <- get_ranking(continent_in)\n  \n  midpoint_in <- tbl %>% \n    distinct(country_name) %>% \n    count() %>% \n    pull() / 2\n  \n  tbl %>%\n    mutate(across(contains(\"value\"), ~ scales::percent(.x, scale = 1, accuracy = 1))) %>%\n    ggplot(aes(year, rank, colour = right_rank, group = country_name)) +\n    geom_line(size = 2.8, aes(year, rank, group = country_name), colour = \"black\") +\n    geom_line(size = 2) +\n    geom_line(size = 3, colour = \"black\", data = tbl %>% filter(country_name == highlight_country_in)) +\n    geom_text(aes(\n      x = 2000,\n      y = left_rank,\n      label = paste0(left_rank, \". \", country_name, \" \", left_value)\n    ),\n    colour = \"grey20\",\n    hjust = 1.1,\n    cex = 3\n    ) +\n    geom_text(aes(\n      x = 2020,\n      y = right_rank,\n      label = paste0(right_rank, \". \", country_name, \" \", right_value)\n    ),\n    colour = \"grey20\",\n    hjust = 0,\n    cex = 3\n    ) +\n    scale_y_reverse() +\n    scale_color_gradient2(\n      low = \"blue\",\n      high = \"red\",\n      mid = \"pink\",\n      midpoint = midpoint_in\n    ) +\n    scale_x_continuous(breaks = c(2000, 2005, 2010, 2015, 2018, 2019, 2020)) +\n    coord_cartesian(xlim = c(1995, 2025)) +\n    theme(\n      legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      axis.text.y = element_blank()\n    ) +\n    labs(\n      x = NULL,\n      y = \"Ranking of adults who smoke\",\n      title = glue::glue(\"Evolution of ranking of adults who smoke in {continent_in}\"),\n      subtitle = glue::glue(\"{highlight_country_in} is highlighted in black\"),\n      caption = \"Data: WHO via World Bank\\nGraphic: @JonathanJayes\"\n    )\n}\n\n# jpeg(\n#   filename = \"posts/2022-10-17-our-world-in-data-choropleth/images/Africa_ranking.jpeg\",\n#   height = 10,\n#   width = 8,\n#   units = \"in\",\n#   res = 1000\n# )\n\nplot_ranking(\"Africa\", \"South Africa\")\n\n\n\n\n\nShow the code\n# dev.off()\n\n\nNow we can see that South Africa has climbed the rankings from 18th place in 2000 up to 7th in 2020. Interesting!\n\n\nWhat about the other continents?\nHave a look at each below, and then give my Shiny app a gander (TODO).\n\n\nShow the code\nmake_tab <- function(continent_in, highlight_country_in) { # function to make the tabs\n  cat(\"##\", continent_in) # Label tab\n  cat(\"\\n\") # Close tab\n  p <- plot_levels(\n    continent_in,\n    highlight_country_in\n  ) # Create plot in levels\n  print(p) # Display plot\n  cat(\"\\n\") # Space\n  q <- plot_ranking(\n    continent_in,\n    highlight_country_in\n  ) # Create plot in levels\n  print(q)\n  cat(\"\\n\") # Close tab\n}\n\n\n\nAmericasAsiaEuropeOceania\n\n\n\n\n\n\n\n\n\nShow the code\nmake_tab(\"Asia\", \"China\")\n\n\n\n\n\n\n\n\n\n\nShow the code\nmake_tab(\"Europe\", \"Sweden\")\n\n\n\n\n\n\n\n\n\n\nShow the code\nmake_tab(\"Oceania\", \"New Zealand\")"
  },
  {
    "objectID": "posts/2022-12-15-coef-plots/coef-plots.html",
    "href": "posts/2022-12-15-coef-plots/coef-plots.html",
    "title": "Coefficient plots in ggplot",
    "section": "",
    "text": "Recently a colleague asked how they could quickly draw a coefficient plot in R. In Stata, this is relatively simple - just run your regression and use the command coefplot afterwards. This produces a graphic that is perfectly acceptable for exploratory data analysis, but leaves something to be desired if you want to use it in a publication.\nThis post shows you how to draw coefficient plots in R and ggplot, and is extensible for use with regressions beyond the basic lm command.\n\nData\nFor this example we will use data from the lovely Palmer Penguins package from Allison Horst. The penguins dataset is a great toy dataset for exploration and visualization, based on genuine data collected by Dr.¬†Kristen Gorman at the Palmer Station in Antarctica.\n\n\n\n\n\n\n\nBasic regression\nWe begin with a basic regression where our dependent variable is penguin body weight in grams, and independent variables are the dimensions of the penguins‚Äô bills, flipper length, as well as species and sex.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n\n# read in data\ndf <- palmerpenguins::penguins\n\n# basic regression\nbasic_reg <- df %>% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .)\n\nbasic_reg\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + sex, data = .)\n\nCoefficients:\n      (Intercept)     bill_length_mm      bill_depth_mm  flipper_length_mm  \n         -1460.99              18.20              67.22              15.95  \n speciesChinstrap      speciesGentoo            sexmale  \n          -251.48            1014.63             389.89  \n\n\nThis produces a rather messy output. It includes both the regression specification and the coefficients.\n\n\nTidy with broom package\nWe can use the broom package to return a tibble, a neat data object that is easy to work with.\n\nlibrary(broom)\n\ntidy_reg <- df %>% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %>% \n    tidy()\n\ntidy_reg\n\n# A tibble: 7 √ó 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2\n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4\n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3\n6 speciesGentoo       1015.     130.        7.83 6.85e-14\n7 sexmale              390.      47.8       8.15 7.97e-15\n\n\nGreat! This output is much easier to deal with.\n\n\nCoefficient plot\nLet‚Äôs try and make a coefficient plot.\n\ntidy_reg %>%\n  filter(term != \"(Intercept)\") %>%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot\"\n  )\n\n\n\n\nWe can see that relative to Adelie penguins (the base category), Gentoo penguins weigh more and Chinstrap penguins weigh less.\nFurther, male penguins weigh more than females.\n\n\nError bars\nTo get the error bars, we specify that we want a confidence interval when we use the tidy command from the broom package, like so: tidy(conf.int = TRUE)\n\ntidy_reg_conf_int <- df %>% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %>% \n    tidy(conf.int = TRUE)\n\ntidy_reg_conf_int\n\n# A tibble: 7 √ó 7\n  term              estimate std.error statistic  p.value conf.low conf.high\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2 -2585.      -337. \n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2     4.22      32.2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4    28.4      106. \n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8    10.2       21.7\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3  -411.       -92.0\n6 speciesGentoo       1015.     130.        7.83 6.85e-14   760.      1270. \n7 sexmale              390.      47.8       8.15 7.97e-15   296.       484. \n\n\nNow in our tibble we get columns called conf.low and conf.high.\nTo plot these, we use an additional geometry in our ggplot, called geom_errorbarh. Here the h at the end specifies we want it in the horizontal direction. We map the conf.low and conf.high variables to xmin and xmax respectively.\n\ntidy_reg_conf_int %>%\n  filter(term != \"(Intercept)\") %>%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nBecause none of the error bars cross the dotted line at zero, we conclude that the point estimates of the effects are significantly different from zero at the five percent level.\n\n\nCategories of predictor\nFinally, let‚Äôs facet the variables by type.\nWe use the case_when command from dplyr to call any variable containing mm a numeric variable, and the others categorical. We colour our bars and points by variable type, and facet_wrap to make a small multiple chart. This makes it easy to differentiate between the different types of variables.\n\ntidy_reg_conf_int %>%\n  filter(term != \"(Intercept)\") %>%\n  # create a type variable with case_when\n  mutate(type = case_when(\n    str_detect(term, \"mm\") ~ \"Numeric variable\",\n    TRUE ~ \"Categorical variable\"\n  )) %>%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, colour = type)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  # remove the legend as the facet show that information\n  theme(legend.position = \"none\") +\n  # facet by type\n  facet_wrap(~type, scales = \"free_y\") +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\",\n    subtitle = \"By variable type\"\n  )\n\n\n\n\nIn this way we can see that the scaling of the variables can have an impact on how we perceive the results. Because the bill and flipper dimensions are measured in mm, and the body mass in grams, we are seeing the effect of an additional mm of bill depth, for example, on body mass in grams, which appears quite small.\n\n\nConclusion\nGreat - I hope that this was useful! It‚Äôs possible to easily customise your coefficient plots in ggplot, adding labels and colours to help your reader understand your regression results in a glance."
  },
  {
    "objectID": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "href": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "title": "ChatGPT fills in for me as a blog post author",
    "section": "",
    "text": "ChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\nToday I asked ChatGPT to to the same.\nIt took 30 seconds.\nHere are the results!\n\n\n\n\n\nPenguins created with DALL¬∑E with the prompt ‚ÄúA diagram of three penguins in the style of Paul C√©zanne, hyper realistic, beautiful art‚Äù\n\n\n‚ÄòPrompt engineering‚Äô describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\nHere, I asked ChatGPT to:\n\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\n\n\n\n\nI recieved text and code that ran almost perfectly:\nIt began\n\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we‚Äôll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\nTo start, let‚Äôs load the necessary packages :\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\n\nNext, we‚Äôll load the Palmer Penguins dataset and take a look at the variables we have to work with:\n\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_l‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema‚Ä¶  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema‚Ä¶  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema‚Ä¶  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\n\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we‚Äôll focus on the relationship between bill_length_mm and body_mass_g.\nTo understand the coefficient plot, we‚Äôll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We‚Äôll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\n\n\n\n\n\nNow we‚Äôll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\n\n\nreg_results <- lm(body_mass_g ~ bill_length_mm, data = penguins)\nreg_tidy <- broom::tidy(reg_results, conf.int = TRUE)\n\n\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\nTo add the error bars, we‚Äôll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\n\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\nAnd that‚Äôs it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\n\n\n\n\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nWow! Amazing. I learned a bunch about ggplot and R from reading the code which accompanied David Robinson‚Äôs screencasts in 2019 and 2020. I am excited for the generation of data analysts beginning their journey now - being able to ask for tailored instructions for a task is a real boon!"
  },
  {
    "objectID": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "href": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "title": "ChatGPT cooks me dinner",
    "section": "",
    "text": "Intro\nI ask ChatGPT to help my cook dinner based on the ingredients I had in the fridge - it was a great success!\nHave a look below at how it went!\nI also asked it to write this blog post, the output for which I include below.\n\n\nProcess\n\n\n  \nVideo\n\n\n\n\n\n\n\n\nVideo\n\n\nSuggested blog post text\nCooking is always a fun and creative experience, but sometimes it can be challenging to come up with new and exciting recipes. That‚Äôs why having a helpful tool like ChatGPT can make a big difference in the kitchen.\nRecently, I had the opportunity to put ChatGPT to the test as a cooking assistant. I provided a list of ingredients that I had on hand and asked for a recipe suggestion. ChatGPT quickly came up with a recipe for a Gammon and Vegetable Stew with Mint, along with detailed instructions for cooking the dish.\nI was impressed with the speed and accuracy of ChatGPT‚Äôs suggestion, and even more impressed with the quality of the recipe. The stew turned out to be delicious, hearty, and comforting - exactly what I was looking for on a cold winter day.\nBut ChatGPT‚Äôs assistance didn‚Äôt stop there. I asked if I could add a splash of Chardonnay to the stew and ChatGPT not only gave me a positive response, but also provided instructions on when and how to add the wine, along with a reminder to use a good quality Chardonnay to enhance the overall taste of the dish.\nOverall, my experience with ChatGPT as a cooking assistant was extremely positive. It was able to provide me with a recipe and instructions that resulted in a delicious and satisfying meal. I was impressed with its speed, accuracy, and helpful suggestions, and I would definitely use ChatGPT again for recipe ideas and cooking instructions.\n\n\nConclusion\nWhat an amazing tool!"
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome to Interlude One",
    "section": "",
    "text": "This is a line from Interlude One off of Alt-J‚Äôs first album, An Awesome Wave. I like this lyric a great deal!\nI aspire to balance in my own life. This site is a space to share what I find interesting, and encourage me to keep learning and sharing. It includes exploratory data analyses, R tutorials, and answers to some data specific questions. I hope that you enjoy your look around."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#asia",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#asia",
    "title": "Our World In Data choropleth",
    "section": "Asia:::",
    "text": "Asia:::"
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#finished-product",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#finished-product",
    "title": "Our World In Data choropleth",
    "section": "Finished product",
    "text": "Finished product\nHere is the finished product that we will create in the tutorial"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html",
    "title": "Electricity generation per capita",
    "section": "",
    "text": "üî•In this blog post, we will take a closer look at the evolution of electricity generation per capita across the world and examine how China has rapidly climbed above its neighbors in per capita termsüìà. Through the use of data visualizationüìä, we will explore the decrease of electricity generation per capita in New Zealand over the past few decades and compare it to other countries in the regionüåé. This analysis will provide a deeper understanding of the world‚Äôs energy landscape and the global energy marketüå°Ô∏è. So, let‚Äôs dive in and see how energy consumption has evolved over time!üîç"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#americas",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#americas",
    "title": "Electricity generation per capita",
    "section": "Americas",
    "text": "Americas"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#asia",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#asia",
    "title": "Electricity generation per capita",
    "section": "Asia",
    "text": "Asia"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#europe",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#europe",
    "title": "Electricity generation per capita",
    "section": "Europe",
    "text": "Europe"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#oceania",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#oceania",
    "title": "Electricity generation per capita",
    "section": "Oceania",
    "text": "Oceania"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#inspiration",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#inspiration",
    "title": "Electricity generation per capita",
    "section": "Inspiration",
    "text": "Inspiration\nüöÄ I am inspired by the company Electricity Maps based in Copenhagen, who allow you to understand the electricity grid through data. They collect data from all around the world and compute the data in an hourly resolution, allowing you to gain granular insights accounting for the variance of the grid.üí° While we won‚Äôt use their data in this blog post, we will take a look at some great annual statistics compiled by the team at Our World in Data. This organization is dedicated to providing data-driven insights on global issues and their data on electricity generation per capita is a valuable resource for understanding how different countries are progressing in terms of energy production.üìä"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#visualization",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#visualization",
    "title": "Electricity generation per capita",
    "section": "Visualization",
    "text": "Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we can see that South Africa has climbed the rankings from 18th place in 2000 up to 7th in 2020. Interesting!\n\nWhat about the other continents?\nHave a look at each below, and then give my Shiny app a gander (TODO).\n\n\n\n\n\nAmericasAsiaEuropeOceania"
  }
]