[
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html",
    "href": "posts/2021-07-06-students-speak/students-speak.html",
    "title": "Students Speak",
    "section": "",
    "text": "Hi Johan - here are some visualizations I have put together from your students’ diary entries. I think that they tell quite a nice story. I hope that some are useful. I’ve done them in black and white - I’m not sure where you want to publish them in the end. If you’d like some colour I can add it easily.\nI’ve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\n\n\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\n\n\n\nThe students’ diary entries have been ingested to form a dataset such that each row is one student’s observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\n\n\n\n\n\n\n\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\n\n\n\n\n\n\nWe start with a word cloud which shows the words used by the students in their diary entries.\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\n\n\n\n\n\n\n\n\nWord cloud\n\n\n\n\nWe can see that common positive words include “support”, “privileged”, “healthy”, “productive”, and “excited”. Common negative words are dominated by “virus”, followed by “difficult”, “struggling”, and “infected”.\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students’ diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\nTable @ref(tab:excluded) shows some common words in the students’ diary entries which are excluded from the wordcloud in Figure @ref(fig:wordcloud).\n\n\n\nCommon words excluded from wordcloud\n\n\nWord\nNumber of uses\n\n\n\n\nLockdown\n251\n\n\nPeople\n211\n\n\nDay\n183\n\n\nTime\n172\n\n\nFamily\n111\n\n\nFeel\n92\n\n\nSouth\n90\n\n\nHome\n80\n\n\nWorld\n78\n\n\nDays\n71\n\n\nAfrica\n61\n\n\nLife\n54\n\n\nOnline\n51\n\n\n19\n50\n\n\nFriends\n50\n\n\n\n\n\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure @ref(fig:image-grobs).\n\n\n\n\n\n\nConventional word cloud and comparison cloud\n\n\n\n\n\nFigure @ref(fig:sentiment) below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students’ diary entries in a particular week.\n\n\n\n\n\n\n\n\nEvolution of sentiment\n\n\n\n\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn Årup Nielsen in 2009-2011. An example of the scores assigned to words in the students’ diary entries is shown in Table @ref(tab:afinn) below.\n\n\n\nAFINN sentiment scores\n\n\nWord\nSentiment score\n\n\n\n\nBullshit\n-4\n\n\nCatastrophic\n-4\n\n\nPanic\n-3\n\n\nFake\n-3\n\n\nWorse\n-3\n\n\nFunny\n4\n\n\nFun\n4\n\n\nWonderful\n4\n\n\nThrilled\n5\n\n\n\n\n\n\n\n\nThe wordcloud in Figure @ref(fig:wordcloud) showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\nTable @ref(tab:words) below shows the words most specific to each week.\n\n\n\n\n\n\nWeek specific words calculated with weighted log odds\n\n\n\n\n\n\n\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\n\n\n\n\nAfrican leaders\nLockdown starts\nPrivilege\nEaster Sunday\nZoom\nClothing bank\n\n\nChurch\nVirus\nConspiracy theories\nPayment\nR500 billion\nLockdown restrictions\n\n\nAirports\nTownships\nAn obligation\nHot Cross buns\nSUN Learn\nLevel 5\n\n\nNSFAS\nCases recorded\nContinues to rise\nExtension\nEconomic stimulus\nLivelihoods\n\n\n\n\n\nNice! We can see that we capture some elements of the experience in each week of lockdown.\n\n\n\n\n\n\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students’ sentiment across the weeks.\n\n\n\n\n\nSentiment and week-words figure\n\n\n\n\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\n\n\n\nFigure @ref(fig:context) shows the evolution of the sentiment of the students’ diary entries beside the rising Covid-19 case numbers in South Africa.\n\n\n\n\n\n\n\n\nComparison figure\n\n\n\n\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students’ reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\n\n\n\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\nI think option two conveys the message clearly and without clutter."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "title": "Students Speak",
    "section": "Option 1",
    "text": "Option 1\nHere we have a two panel plot of the Covid-19 statistics and number of diary entries recorded by the students."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "title": "Students Speak",
    "section": "Option 2",
    "text": "Option 2\nNext we have a single panel with the period of diary entries superimposed on the Covid-19 statistics."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "title": "Students Speak",
    "section": "Option 3",
    "text": "Option 3\nAlternatively we can annotate a thick line to show where the diary entries occur."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "title": "Students Speak",
    "section": "Option 4",
    "text": "Option 4\nAlternatively we can have a legend variant of option 2."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "Welcome to the first in a series of data screencasts where I attempt to show you how great the R language is.\nThis post follows along with the data screencast and includes the code methodically.\nIt begins by reading in the data, then tidying it up, analysing it, making some visualizations and then performing some predictive modelling.\n\n\nI’m signed up to a fantastic newsletter called “Data is Plural” curated by journalist Jeremy Singer-Vine. Truly, it is a treasure trove of interesting public datasets from all over the internet. You can sign up to the newsletter here if this sounds up your alley.\nThis week it included a link to a fun selection of chocolate bar reviews, which Jeremy described as:\n\nChocolate bar reviews. The Manhattan Chocolate Society’s Brady Brelinski has reviewed 2,500+ bars of craft chocolate since 2006, and compiles his findings into a copy-paste-able table that lists each bar’s manufacturer, bean origin, percent cocoa, ingredients, review notes, and numerical rating.\n\nA live link to the database is shown below.\n\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")\n\n\n\n\n\n\nI’ve copied and pasted the data into an Excel spreadsheet, accessible on my Github if you want to download it an analyse it yourself.\nThe underlying data is served in a Javascript container on the website rather than vanilla HTML. This makes it a little bit more difficult to scrape with a package like rvest, for example. Hence the Excel spreadsheet.\nTo begin the analysis, we’ll read in the Excel file using the readxl package, and the here package that helps us with file paths.1\nWe don’t need to load the packages via the library(readxl) command because we’re only going to use them once or twice. Instead we can call the name of the package followed by two colons and the command, as shown below.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# read in the data\ndf <- readxl::read_excel(here::here(\"posts\", \n                                    \"2022-01-17-flavours-of-cocoa\", \n                                    \"data\", \n                                    \"chocolate-bar-ratings.xlsx\"))\n\n# display the first six rows of tibble\nhead(df)\n\n# A tibble: 6 × 10\n    REF Company…¹ Compa…² Revie…³ Count…⁴ Speci…⁵ Cocoa…⁶ Ingre…⁷ Most …⁸ Rating\n  <dbl> <chr>     <chr>     <dbl> <chr>   <chr>     <dbl> <chr>   <chr>    <dbl>\n1  2454 5150      U.S.A.     2019 Tanzan… Kokoa …    0.76 3- B,S… rich c…   3.25\n2  2458 5150      U.S.A.     2019 Domini… Zorzal…    0.76 3- B,S… cocoa,…   3.5 \n3  2454 5150      U.S.A.     2019 Madaga… Bejofo…    0.76 3- B,S… cocoa,…   3.75\n4  2542 5150      U.S.A.     2021 Fiji    Matasa…    0.68 3- B,S… chewy,…   3   \n5  2546 5150      U.S.A.     2021 Venezu… Sur de…    0.72 3- B,S… fatty,…   3   \n6  2546 5150      U.S.A.     2021 Uganda  Semuli…    0.8  3- B,S… mildly…   3.25\n# … with abbreviated variable names ¹​`Company (Manufacturer)`,\n#   ²​`Company Location`, ³​`Review Date`, ⁴​`Country of Bean Origin`,\n#   ⁵​`Specific Bean Origin or Bar Name`, ⁶​`Cocoa Percent`, ⁷​Ingredients,\n#   ⁸​`Most Memorable Characteristics`\n\n\nThis gives us a tibble (similar to a dataframe) with 10 columns (4 numeric and 6 character) and 2,530 individual reviews.\nThe column names are a big ugly though:\n\n# show column names\ndf %>% colnames()\n\n [1] \"REF\"                              \"Company (Manufacturer)\"          \n [3] \"Company Location\"                 \"Review Date\"                     \n [5] \"Country of Bean Origin\"           \"Specific Bean Origin or Bar Name\"\n [7] \"Cocoa Percent\"                    \"Ingredients\"                     \n [9] \"Most Memorable Characteristics\"   \"Rating\"                          \n\n\nWe can use the janitor package to make the column names snake case (lower case with words separated by an underscore).\n\n# clean names\ndf <- df %>% \n    janitor::clean_names()\n\n# show names again\ndf %>% colnames()\n\n [1] \"ref\"                              \"company_manufacturer\"            \n [3] \"company_location\"                 \"review_date\"                     \n [5] \"country_of_bean_origin\"           \"specific_bean_origin_or_bar_name\"\n [7] \"cocoa_percent\"                    \"ingredients\"                     \n [9] \"most_memorable_characteristics\"   \"rating\"                          \n\n\nNow that we have a nice tibble with clean names, we can ask what the data itself looks like. There are many ways to get summary statistics of a dataset. I love the skim function from the skimr package.\n\n# skim the dataset\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nref\n0\n1\n1429.80\n757.65\n5.00\n802.0\n1454.00\n2079.00\n2712\n▆▇▇▇▇\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006.00\n2012.0\n2015.00\n2018.00\n2021\n▃▅▇▆▅\n\n\ncocoa_percent\n0\n1\n0.72\n0.06\n0.42\n0.7\n0.70\n0.74\n1\n▁▁▇▁▁\n\n\nrating\n0\n1\n3.20\n0.45\n1.00\n3.0\n3.25\n3.50\n4\n▁▁▅▇▇\n\n\n\n\n\nGreat! Our reviews are almost all complete.\n\nOnly 3 percent are missing information on the ingredients.\nThe reviews begin in 2006, the mean review is from 2014, and the latest is from 2021.\nThe percent of the bar comprising of cocoa ranges from 42 to 100, with a mean of 72.\nWe have 62 unique countries of origin for the beans, and 67 countries of manufacture.\nThere are 21 unique combinations of ingredients, comprising of seven elements in total.\n\n\n\n\nLet’s have a look at that ingredients column.\n\n# count elements of ingredients column\ndf %>% \n    count(ingredients, sort = T)\n\n# A tibble: 22 × 2\n   ingredients      n\n   <chr>        <int>\n 1 3- B,S,C       999\n 2 2- B,S         718\n 3 4- B,S,C,L     286\n 4 5- B,S,C,V,L   184\n 5 4- B,S,C,V     141\n 6 <NA>            87\n 7 2- B,S*         31\n 8 4- B,S*,C,Sa    20\n 9 3- B,S*,C       12\n10 3- B,S,L         8\n# … with 12 more rows\n\n\nSo we have a number of ingredients, a dash, and then a key for what the ingredients are. Consulting the website reveals that there are seven possible ingredients:\n\n\n\n\n\nkey\nvalue\n\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\nThese key and value combinations are very sensible - if we have a lot of data we can save space by using the keys instead of the whole string. However, I would prefer to have them written out, because we’re going to split them into their own columns a little bit later.\nWe can use the str_replace_all function from the stringr package to replace items in the list of ingredients with names.\n\ndf <- df %>% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nLet’s look again at our ingredients column:\n\ndf %>% \n    count(ingredients, sort = T)\n\n# A tibble: 22 × 2\n   ingredients                                        n\n   <chr>                                          <int>\n 1 3- beans,sugar,cocoa_butter                      999\n 2 2- beans,sugar                                   718\n 3 4- beans,sugar,cocoa_butter,lecithin             286\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\n 5 4- beans,sugar,cocoa_butter,vanilla              141\n 6 <NA>                                              87\n 7 2- beans,non_sugar_sweetener                      31\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\n10 3- beans,sugar,lecithin                            8\n# … with 12 more rows\n\n\nFantastic! Now we have the number of ingredients, a dash, and then each ingredient by name in one column. Let’s separate this information into two columns so that we can use the number of ingredients as a feature.\nThe separate function from the tidyr package is made just for this purpose. It takes three arguments:\n\nthe name of the column to separate.\nnew column names corresponding to the number of elements.\nthe separator between elements.\n\n\ndf <- df %>% \n  separate(col = ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"),\n           sep = \"-\") %>% \n    # parse_number looks for a number inside a character column and discards the rest\n  mutate(n_ingredients = parse_number(n_ingredients),\n         # str_squish removes whitespace around the elements in the ingredients column\n         ingredients = str_squish(ingredients))\n\ndf %>% \n  select(n_ingredients, ingredients)\n\n# A tibble: 2,530 × 2\n   n_ingredients ingredients                      \n           <dbl> <chr>                            \n 1             3 beans,sugar,cocoa_butter         \n 2             3 beans,sugar,cocoa_butter         \n 3             3 beans,sugar,cocoa_butter         \n 4             3 beans,sugar,cocoa_butter         \n 5             3 beans,sugar,cocoa_butter         \n 6             3 beans,sugar,cocoa_butter         \n 7             3 beans,sugar,cocoa_butter         \n 8             4 beans,sugar,cocoa_butter,lecithin\n 9             4 beans,sugar,cocoa_butter,lecithin\n10             4 beans,sugar,cocoa_butter,lecithin\n# … with 2,520 more rows\n\n\nNow we have a numeric column with the number of ingredints and a column called ingredients with each element separated by a comma.\nFinally, let’s break the ingredients from a comma separated list into a binary variable for each ingredient. We can use the recipes package that is part of the tidymodels metapackage - a framework for doing statistical modelling in a tidy manner.\nFirst we break our ingredients into 6 columns. The problem we run into is that for bars that contain different ingredients, the order of the ingredients split into the columns is not constant.\n\n# df <- df %>% \n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\n#            sep = \",\") \n\n# df %>% \n#     select(company_manufacturer , starts_with(\"ingredient_\")) %>%\n#     slice(c(1L, 51L, 54L))\n\nPerhaps there is a better way to do this? Separate rows and pivot wider?\n\ndf %>% \n    separate_rows(ingredients, sep = \",\") %>%\n    count(ingredients)\n\n# A tibble: 8 × 2\n  ingredients             n\n  <chr>               <int>\n1 beans                2443\n2 cocoa_butter         1668\n3 lecithin              493\n4 non_sugar_sweetener    76\n5 salt                   37\n6 sugar                2360\n7 vanilla               353\n8 <NA>                   87\n\ndf <- df %>% \n    separate_rows(ingredients, sep = \",\") %>%\n    filter(!is.na(ingredients)) %>% \n    pivot_wider(names_from = ingredients, values_from = ingredients) %>% \n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nTidymodels\n\n# library(tidymodels)\n# dummy_multi_choice_rec <- recipe(~ ., data = df) %>%\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %>%\n#   prep()\n# \n# df <- bake(dummy_multi_choice_rec, new_data = NULL)\n\n\n\n\n\n\n\ndf %>% \n  count(rating, sort = T)\n\n# A tibble: 12 × 2\n   rating     n\n    <dbl> <int>\n 1   3.5    552\n 2   3      506\n 3   3.25   453\n 4   2.75   320\n 5   3.75   295\n 6   2.5    156\n 7   4      111\n 8   2       29\n 9   2.25    14\n10   1.5      5\n11   1        1\n12   1.75     1\n\n\nScore range between 1 and 4, and the modal value is 3.5.\n\n# histogram\ndf %>% \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\n    labs(x = \"Chocolate bar rating\",\n         y = \"Number of bars\")\n\n\n\n\n\n\n\n\ndf %>% \n    group_by(review_date) %>% \n    summarise(mean_rating = mean(rating)) %>% \n    ungroup() %>% \n    knitr::kable(digits = 2)\n\n\n\n\nreview_date\nmean_rating\n\n\n\n\n2006\n3.06\n\n\n2007\n3.17\n\n\n2008\n3.04\n\n\n2009\n3.09\n\n\n2010\n3.19\n\n\n2011\n3.27\n\n\n2012\n3.21\n\n\n2013\n3.21\n\n\n2014\n3.20\n\n\n2015\n3.24\n\n\n2016\n3.24\n\n\n2017\n3.36\n\n\n2018\n3.19\n\n\n2019\n3.13\n\n\n2020\n3.26\n\n\n2021\n3.32\n\n\n\n\n\nIt certainly seems like the mean rating is increasing over time. What is driving this?\nWe can make a plot of the figures above to see the increasing trend.\n\ndf %>% \n    group_by(review_date) %>% \n    summarise(mean_rating = mean(rating)) %>% \n    ungroup() %>% \n    ggplot(aes(review_date, mean_rating)) +\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\n    labs(x = \"Date of review\",\n         y = \"Mean rating\")\n\n\n\n\nLet’s make a boxplot to see how the spread of scores has changed over time.\n\ndf %>%\n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_jitter(alpha = .2) +\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\n\n\n\n\nIt seems as if the share of bars with very low scores has decreased over time, while the median value has remained relatively stable over time, shown by the bar in the centre of the boxplots.\nWhat about making a joy plot or ridgeline plot with the ggridges package? This allows us to see how the spread of values has changed over time.\n\nlibrary(ggridges)\n\ndf %>%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\nThis confirms what we saw in the boxplots above: fewer low scores in more recent years mean that the mean has increased, while the top of the distributions remain largely the same.\n\n\n\n\ndf %>% \n  mutate(cocoa_percent = round(cocoa_percent, 1)) %>% \n  count(cocoa_percent, n_ingredients) %>% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\n\n\n\n\ndf %>% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 × 2\n   country_of_bean_origin     n\n   <chr>                  <int>\n 1 Venezuela                246\n 2 Peru                     231\n 3 Dominican Republic       220\n 4 Ecuador                  201\n 5 Madagascar               171\n 6 Blend                    144\n 7 Nicaragua                100\n 8 Bolivia                   79\n 9 Colombia                  78\n10 Tanzania                  78\n# … with 52 more rows\n\ndf %>% \n  add_count(country_of_bean_origin) %>%\n  # only include countries with more than 60 bars\n  filter(n > 60) %>% \n  group_by(country_of_bean_origin) %>% \n  summarise(mean_rating = mean(rating)) %>% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %>% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\n\n\n\n\nlibrary(tmap)\ndata(\"World\")\n\nworld <- World %>% as_tibble()\n\nTo join our data on chocolate to this map, we need to get coutnry codes, using the countrycode package.\n\nlibrary(countrycode)\n\ndf <- df %>% \n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\n\n\nlibrary(sf)\n\ndf_map <- df %>% \n    group_by(iso_a3) %>%\n    add_count() %>% \n    summarise(mean_rating = mean(rating),\n              n = n) %>% \n    ungroup() %>% \n    distinct() %>% \n    left_join(world, by = \"iso_a3\")\n\n\ndf_map %>% \n    filter(n > 3) %>% \n    st_as_sf() %>% ggplot() +\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n    geom_sf(aes(fill = mean_rating)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    labs(fill = \"Mean country rating\")\n\n\n\n\n\n\n\n\n\ndf_characteristics <- df %>% \n  select(c(most_memorable_characteristics, rating)) %>% \n  separate_rows(most_memorable_characteristics, sep = \",\") %>% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\n\ndf_characteristics %>% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 × 2\n   most_memorable_characteristics     n\n   <chr>                          <int>\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# … with 938 more rows\n\n\nWe can start with a naive analysis that looks only at average score per word. These are the highest scoring words.\n\n# df_characteristics %>% \n#   group_by(most_memorable_characteristics) %>% \n#   add_count() %>% \n#   mutate(avg_rating = mean(rating)) %>% \n#   ungroup() %>% \n#   slice_max(avg_rating, n = 12, with_ties = F)\n\ndf_characteristics %>% \n  group_by(most_memorable_characteristics) %>% \n  add_count() %>% \n  filter(n > 3) %>% \n  mutate(avg_rating = mean(rating)) %>% \n  ungroup() %>% \n  distinct(most_memorable_characteristics, avg_rating) %>% \n  slice_max(avg_rating, n = 12, with_ties = F) %>% \n    mutate(avg_rating = round(avg_rating, 2)) %>% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds <- vfold_cv(df_characteristics)\n\nglmnet_recipe <- \n  recipe(formula = rating ~ ., data = df_characteristics) %>% \n  step_tokenize(most_memorable_characteristics) %>% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %>% \n  step_tf(most_memorable_characteristics) %>% \n  step_normalize(all_predictors(), -all_nominal())\n\n\nglmnet_recipe %>% prep() %>% juice()\n\n# A tibble: 6,839 × 101\n   rating tf_most_memo…¹ tf_mo…² tf_mo…³ tf_mo…⁴ tf_mo…⁵ tf_mo…⁶ tf_mo…⁷ tf_mo…⁸\n    <dbl>          <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>   <dbl>\n 1   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 2   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 3   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 4   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 5   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 6   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 7   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 8   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 9   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n10   3           -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n# … with 6,829 more rows, 92 more variables:\n#   tf_most_memorable_characteristics_bitter <dbl>,\n#   tf_most_memorable_characteristics_black <dbl>,\n#   tf_most_memorable_characteristics_bland <dbl>,\n#   tf_most_memorable_characteristics_bold <dbl>,\n#   tf_most_memorable_characteristics_bright <dbl>,\n#   tf_most_memorable_characteristics_brownie <dbl>, …\n\n\n\nglmnet_spec <- \n  linear_reg(penalty = tune(), mixture = 1) %>% \n  set_mode(\"regression\") %>% \n  set_engine(\"glmnet\") \n\nglmnet_workflow <- \n  workflow() %>% \n  add_recipe(glmnet_recipe) %>% \n  add_model(glmnet_spec) \n\nglmnet_grid <- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune <- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\n\nglmnet_tune %>% \n  autoplot()\n\n\n\n\n\nglmnet_model_final <- finalize_workflow(glmnet_workflow, glmnet_tune %>% \n  select_best())\n\nfinal_fit <- glmnet_model_final %>% \n  fit(df_characteristics)\n\nWhat does the fit look like? These are the terms that have the greatest effect on bar rating.\n\nfinal_fit %>%\n  extract_fit_parsnip() %>%\n  tidy() %>%\n  filter(term != \"(Intercept)\") %>%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %>%\n  mutate(sign = estimate > 0) %>%\n  group_by(sign) %>%\n  mutate(estimate = abs(estimate)) %>% \n  slice_max(estimate, n = 12) %>%\n  ungroup() %>%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %>% \n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\nWow! have a look at the terms up top - “creamy”, “complex” and “rich” are good chocolate words. On the other side, “bitter”, “off” and “chemical” are terms that lower the score of the bar."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interlude One",
    "section": "",
    "text": "ChatGPT cooks me dinner\n\n\n\n\n\n🍲 Master the kitchen like a pro with the help of ChatGPT! 🍴 I tried it out and the results will blow your mind! 🤯 Find out what happened when I asked a language model for recipe ideas and cooking instructions. 😲 You won’t believe how delicious my Gammon and Vegetable Stew with Mint turned out! 🍽️ Don’t miss out on this must-read blog post! 👀\n\n\n\n\n\n\nJan 20, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT fills in for me as a blog post author\n\n\n\n\n\nI ask ChatGPT to write a blog post in the same manner as one I wrote last week. The results will amaze you!\n\n\n\n\n\n\nDec 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nCoefficient plots in ggplot\n\n\n\n\n\nDrawing coefficient plots in R and ggplot.\n\n\n\n\n\n\nDec 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nOur World In Data choropleth\n\n\n\n\n\nA post on how recreate the Our World In Data characteristic choropleth in R.\n\n\n\n\n\n\nJun 1, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nLinkedIn style tables for personal websites\n\n\n\n\n\nA post on how to us the {gt} table package to make tables for the about page of a pesonal website or blog.\n\n\n\n\n\n\nMay 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nProgrammatically generate tabs in Rmarkdown\n\n\n\n\n\nA walk-though for writing code that writes it’s own code.\n\n\n\n\n\n\nMar 4, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nFlavours of cocoa\n\n\n\n\n\nAn exploration of chocolate bar reviews\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nR vs Python? R & Python!\n\n\n\n\n\nMining the twitter archive of Fathom Data\n\n\n\n\n\n\nJan 17, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nStudents Speak\n\n\n\n\n\nText analysis of students’ diary entries during the Covid-19 lockdown in South Africa\n\n\n\n\n\n\nJul 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nResources for Learning R and Mastering ggplot\n\n\n\n\n\nA collection of videos and resources to get you started using R and the tidyverse for exploratory data analysis and visualization.\n\n\n\n\n\n\nMay 21, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nMapping Sweden’s Regional GDP\n\n\n\n\n\nVisualizing the relationship between population and regional GDP share in 1571, based on data from Enflo and Missiaia (2018).\n\n\n\n\n\n\nMay 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nShipping and Scraping - Part 1 in a Series on Shipping\n\n\n\n\n\nIn this post I walk through scraping data on cargo ships from Wikipedia as part of a series on shipping. I make use of R, the rvest package for webscraping and the SelectorGadget tool for CSS selection.\n\n\n\n\n\n\nApr 8, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nSwedish trade in goods and exchange students\n\n\n\n\n\nI wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund Univesity from there, this semester.\n\n\n\n\n\n\nFeb 20, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nSouth African Car Colour Exploration\n\n\n\n\n\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\n\n\n\n\n\n\nNov 24, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nCar Price Comparison Tool\n\n\n\n\n\nThis web app helps South Africans gauge the median price for a second hand car. You can compare price by odometer reading, make and model, and province. Ad links are included in a table beside the output.\n\n\n\n\n\n\nOct 10, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nRecreating Our World In Data’s Beautiful Transistor Count Graphic\n\n\n\n\n\nScraping data from Wikipedia and recreating OWID’s figure with ggplot2.\n\n\n\n\n\n\nApr 8, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to Interlude One\n\n\n\n\n\n“Like all good fruit, the balance of life is in the ripe and ruin”\n\n\n\n\n\n\nMar 13, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Fathom data are an analytics and data science training firm which offers training in two of the most popular open source languages for statistical and general purpose programming: #Rstats and #Python. In their words:\n\n\nWe use 100% #opensource tools and we contribute to a number of Open Source projects. There's no doubt that Open Source adds value to @fathomdata, but it's hard to know what value we're creating. Some interesting perspectives on quantifying value.https://t.co/hcw5hylTwn\n\n— Fathom Data (@fathomdata) January 24, 2022\n\n\nSome of the staff state a preference for R, and I wanted to see if this preference was borne out in the tweets on their corporate twitter account.\n\n\nTo quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n# get the tweets!\ndf <- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\n\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew’s account.\n\ndf %>% \n    count(screen_name) %>% \n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\n\n\n\n\nAccount\nNumber of tweets\n\n\n\n\ndatawookie\n3198\n\n\nfathomdata\n448\n\n\n\n\n\n\n\n\nLet’s start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\n\nhashtags <- df %>%\n  filter(screen_name == \"fathomdata\") %>%\n  unnest(cols = c(hashtags)) %>%\n  filter(!is.na(hashtags)) %>%\n  mutate(hashtags = str_to_lower(hashtags)) %>%\n  count(hashtags, is_retweet) %>%\n  mutate(is_retweet = if_else(is_retweet == TRUE,\n    \"Retweet\",\n    \"@fathomdata tweet\"\n  ))\n\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\nI’ve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\n\nlibrary(tidytext)\n# ggsci package for nice blue fill scale\nlibrary(ggsci)\n\nhashtags %>% \n  group_by(is_retweet) %>% \n  slice_max(n, n = 12, with_ties = F) %>% \n  ungroup() %>% \n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %>% \n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %>% \n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\n  geom_col(show.legend = F) +\n  scale_fill_material(\"blue\") +\n  scale_color_manual(values = c(\"white\", \"red\")) +\n  scale_y_reordered() +\n  facet_wrap(~ is_retweet, scales = \"free\") +\n  labs(x = \"Number of tweets tagging\",\n       y = \"Hashtag\",\n       caption = \"Note: red outline on bar indicates language\")\n\n\n\n\nColumn chart of hashtag frequency by tweet type\n\n\n\n\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\n\nhashtags %>%\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %>%\n  mutate(hashtags = fct_reorder(hashtags, n),\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %>%\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\n  geom_col(position = \"fill\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Share of tweets tagging langauge\",\n    fill = \"Language\",\n    y = NULL\n  ) +\n  scale_fill_npg(\n    guide = guide_legend(reverse = TRUE)\n  )\n\n\n\n\nStacked column chart comparing language hashtag frequency by tweet type\n\n\n\n\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\n\ndf %>%\n  select(text) %>%\n  mutate(\n    rstats = str_detect(text, \"rstats\"),\n    python = str_detect(text, \"python\")\n  ) %>%\n  summarise(across(c(rstats, python), sum)) %>%\n  pivot_longer(everything(),\n    names_to = \"language\",\n    values_to = \"n\"\n  ) %>% \n    mutate(share = scales::percent(n / (496 + 134)))\n\n# A tibble: 2 × 3\n  language     n share\n  <chr>    <int> <chr>\n1 rstats     496 79%  \n2 python     134 21%  \n\n\n\n\n\nIn the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\n\ndf %>%\n  filter(\n    str_detect(text, \"rstats\"),\n    screen_name == \"fathomdata\",\n    !is.na(retweet_screen_name)\n  ) %>%\n  count(retweet_name, sort = T) %>%\n  slice_head(n = 12) %>%\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\n\n\n\n\nAccout name\nNumber of retweets\n\n\n\n\nAndrew B. Collier\n43\n\n\nR-bloggers\n19\n\n\nMegan Beckett\n16\n\n\nR posts you might have missed!\n10\n\n\nsatRday South Africa\n6\n\n\nMara Averick\n5\n\n\nAstrid Radermacher\n3\n\n\nRStudio\n3\n\n\nTom Mock\n3\n\n\nFathom Data\n2\n\n\nR-Ladies Cape Town\n2\n\n\nR Weekly Live\n2\n\n\n\n\n\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap.\n\n\n\nFollowing the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\n\n# taking out URLs\nremove_bits <- \"&amp;|&lt;|&gt;\"\n\ntidy_tweets <- df %>%\n  filter(!str_detect(text, \"^RT\")) %>%\n  mutate(text = str_remove_all(text, remove_bits)) %>%\n  unnest_tokens(word, text, token = \"tweets\") %>%\n    # remove stopwords\n  filter(\n    !word %in% stop_words$word,\n    !word %in% str_remove_all(stop_words$word, \"'\"),\n    str_detect(word, \"[a-z]\")\n  ) \n\nfrequency <- tidy_tweets %>% \n  group_by(screen_name) %>% \n  count(word, sort = TRUE) %>% \n  left_join(tidy_tweets %>% \n              group_by(screen_name) %>% \n              summarise(total = n())) %>%\n  mutate(freq = n/total)\n\nfrequency <- frequency %>% \n  select(screen_name, word, freq) %>% \n  pivot_wider(names_from = screen_name, values_from = freq) %>%\n  arrange(desc(datawookie), fathomdata)\n\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew’s tweets.\n\nfrequency %>% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  theme(legend.position = \"none\") +\n    labs(x = \"Andrew Collier\",\n         y = \"Fathom Data\")\n\n\n\n\nComparing the frequency of words used by the Fathom Data account and Andrew Collier\n\n\n\n\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\n\nfrequency %>% \n  mutate(highlight = word == \"@standardbankza\") %>% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\nHighlighting Standard Bank account mentions\n\n\n\n\nThis is clearly the case! Have a look at the top terms, haha!\n\nfrequency %>%\n  filter(is.na(fathomdata)) %>%\n  select(-fathomdata) %>%\n  mutate(datawookie = scales::percent(datawookie)) %>%\n  slice_head(n = 10) %>%\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\n\n\n\n\nWord\nRelative specificity to Andrew Collier’s tweets\n\n\n\n\nkak\n0.6723%\n\n\n#customerservice\n0.6666%\n\n\n@rainsouthafrica\n0.5759%\n\n\nmind\n0.5220%\n\n\nmbps\n0.4255%\n\n\nprepared\n0.3773%\n\n\nlinux\n0.3489%\n\n\n#itcanbe\n0.3262%\n\n\n#fixit\n0.2354%\n\n\nworst\n0.2241%\n\n\n\n\n\n\n\n\nIt appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "title": "Our World In Data choropleth",
    "section": "",
    "text": "I really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, “make progress against the world’s largest problems”.\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser this year, included below.\n\n\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "title": "Our World In Data choropleth",
    "section": "Context",
    "text": "Context\nThere is a lot of information about the OWID grapher tool. You can have a look at their github repo and an older reddit AMA if you are interested. It’s a custom system that allows for systematic changes across their website, drawing on data from a central database."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "title": "Our World In Data choropleth",
    "section": "Components",
    "text": "Components\nWhat are the parts I want to recreate? The map has:\n\na base map, where the colour fill of each country indicates it’s position in a specific measure in a particular year.\na simple line chart that appears when you hover over a country, showing how the measure has changed within a country over time.\na clear legend\na note specifying the source of the data\n\nI walk through creating each of these below.\n\nThe world map\nThe base map is sourced from the maps package. I add a three letter country code from the english name of the country using the countrycode package and filter out Antarctica, Greenland and the French Southern and Antarctic Lands.\n\n\n\nThe base map is projected with the Web Mercator or WGS 84 projection, the same one Google Maps uses.\n\n\n\n\n\n\n\nData\nWe read in the data as a CSV file, and tidy up the column names so that they are in snake case with the clean_names() command from the very helpful janitor package.\n\n\n\nNext we remove the summary groups which we cannot show on the map, including the World Bank country income groupings.\n\n\n# A tibble: 16 × 1\n   entity                      \n   <chr>                       \n 1 East Asia and Pacific       \n 2 Europe and Central Asia     \n 3 European Union              \n 4 High income                 \n 5 Latin America and Caribbean \n 6 Low and middle income       \n 7 Low income                  \n 8 Lower middle income         \n 9 Middle East and North Africa\n10 Middle income               \n11 North America               \n12 South Asia                  \n13 Sub-Saharan Africa          \n14 Tuvalu                      \n15 Upper middle income         \n16 World                       \n\n\n\n\nCreate a colour palette\nSo what we want to do is use the scale_color_viridis_c() palette. We have to map it to the min and max of the values in our dataset.\n\n\n# A tibble: 1 × 2\n    min   max\n  <dbl> <dbl>\n1   3.5  68.5\n\n\n\n\nHow to plot the line graph?\nThe line graph that appears when you hover over OWID map is very simple. It has just the starting value on the y-axis, and the first and last years on the x-axis, and a line that changes colour in accordance with the scale of the choropleth. The hover window which contains the graph also shows the country name, and the value of the measure in the most recent year.\nTo recreate it, we need store these four values, and draw the coloured line.\n\n\n\nA function for plotting the graph\n\n\n\n\n\n\n\nNow making the table\n\n\n\n\n  \n  \n  \n    \n      South Africa\n    \n    \n  \n  \n  \n    \n\n20.3%\nin 2020\n  \n  \n  \n\n\n\n\n\n\nCreating the plots for each country\nHere we use the purrr::map command to make the table in raw HTML for each country and save it inside a tibble. The output shows an HTML list in the column called gt.\n\n\n# A tibble: 162 × 2\n   code  gt        \n   <chr> <list>    \n 1 AFG   <html [1]>\n 2 ALB   <html [1]>\n 3 DZA   <html [1]>\n 4 AND   <html [1]>\n 5 ARG   <html [1]>\n 6 ARM   <html [1]>\n 7 AUS   <html [1]>\n 8 AUT   <html [1]>\n 9 AZE   <html [1]>\n10 BHS   <html [1]>\n# … with 152 more rows\n\n\nWe thencreate a tibble called df_map that selects the most recent year for each country from the dataset and joins it to the map by the country code variable we created above. Finally we join this to the tibble of tables called gt_tables.\n\n\n\n\n\nCreating the interactive figure\nNow we are ready to create the interactive figure!\nWe begin by drawing a static map in grey, with data from the original map. Next we overlay the interactive choropleth. The grey static map will show through all the countries we don’t have data on in the dataset.\n\n\n\n\n\n\n\n\n\n\nShow off the interactive figure"
  },
  {
    "objectID": "posts/2022-12-15-coef-plots/coef-plots.html",
    "href": "posts/2022-12-15-coef-plots/coef-plots.html",
    "title": "Coefficient plots in ggplot",
    "section": "",
    "text": "Recently a colleague asked how they could quickly draw a coefficient plot in R. In Stata, this is relatively simple - just run your regression and use the command coefplot afterwards. This produces a graphic that is perfectly acceptable for exploratory data analysis, but leaves something to be desired if you want to use it in a publication.\nThis post shows you how to draw coefficient plots in R and ggplot, and is extensible for use with regressions beyond the basic lm command.\n\nData\nFor this example we will use data from the lovely Palmer Penguins package from Allison Horst. The penguins dataset is a great toy dataset for exploration and visualization, based on genuine data collected by Dr. Kristen Gorman at the Palmer Station in Antarctica.\n\n\n\n\n\n\n\nBasic regression\nWe begin with a basic regression where our dependent variable is penguin body weight in grams, and independent variables are the dimensions of the penguins’ bills, flipper length, as well as species and sex.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n\n# read in data\ndf <- palmerpenguins::penguins\n\n# basic regression\nbasic_reg <- df %>% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .)\n\nbasic_reg\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + sex, data = .)\n\nCoefficients:\n      (Intercept)     bill_length_mm      bill_depth_mm  flipper_length_mm  \n         -1460.99              18.20              67.22              15.95  \n speciesChinstrap      speciesGentoo            sexmale  \n          -251.48            1014.63             389.89  \n\n\nThis produces a rather messy output. It includes both the regression specification and the coefficients.\n\n\nTidy with broom package\nWe can use the broom package to return a tibble, a neat data object that is easy to work with.\n\nlibrary(broom)\n\ntidy_reg <- df %>% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %>% \n    tidy()\n\ntidy_reg\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic  p.value\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2\n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4\n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3\n6 speciesGentoo       1015.     130.        7.83 6.85e-14\n7 sexmale              390.      47.8       8.15 7.97e-15\n\n\nGreat! This output is much easier to deal with.\n\n\nCoefficient plot\nLet’s try and make a coefficient plot.\n\ntidy_reg %>%\n  filter(term != \"(Intercept)\") %>%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot\"\n  )\n\n\n\n\nWe can see that relative to Adelie penguins (the base category), Gentoo penguins weigh more and Chinstrap penguins weigh less.\nFurther, male penguins weigh more than females.\n\n\nError bars\nTo get the error bars, we specify that we want a confidence interval when we use the tidy command from the broom package, like so: tidy(conf.int = TRUE)\n\ntidy_reg_conf_int <- df %>% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %>% \n    tidy(conf.int = TRUE)\n\ntidy_reg_conf_int\n\n# A tibble: 7 × 7\n  term              estimate std.error statistic  p.value conf.low conf.high\n  <chr>                <dbl>     <dbl>     <dbl>    <dbl>    <dbl>     <dbl>\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2 -2585.      -337. \n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2     4.22      32.2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4    28.4      106. \n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8    10.2       21.7\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3  -411.       -92.0\n6 speciesGentoo       1015.     130.        7.83 6.85e-14   760.      1270. \n7 sexmale              390.      47.8       8.15 7.97e-15   296.       484. \n\n\nNow in our tibble we get columns called conf.low and conf.high.\nTo plot these, we use an additional geometry in our ggplot, called geom_errorbarh. Here the h at the end specifies we want it in the horizontal direction. We map the conf.low and conf.high variables to xmin and xmax respectively.\n\ntidy_reg_conf_int %>%\n  filter(term != \"(Intercept)\") %>%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nBecause none of the error bars cross the dotted line at zero, we conclude that the point estimates of the effects are significantly different from zero at the five percent level.\n\n\nCategories of predictor\nFinally, let’s facet the variables by type.\nWe use the case_when command from dplyr to call any variable containing mm a numeric variable, and the others categorical. We colour our bars and points by variable type, and facet_wrap to make a small multiple chart. This makes it easy to differentiate between the different types of variables.\n\ntidy_reg_conf_int %>%\n  filter(term != \"(Intercept)\") %>%\n  # create a type variable with case_when\n  mutate(type = case_when(\n    str_detect(term, \"mm\") ~ \"Numeric variable\",\n    TRUE ~ \"Categorical variable\"\n  )) %>%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %>%\n  ggplot(aes(estimate, term, colour = type)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  # remove the legend as the facet show that information\n  theme(legend.position = \"none\") +\n  # facet by type\n  facet_wrap(~type, scales = \"free_y\") +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\",\n    subtitle = \"By variable type\"\n  )\n\n\n\n\nIn this way we can see that the scaling of the variables can have an impact on how we perceive the results. Because the bill and flipper dimensions are measured in mm, and the body mass in grams, we are seeing the effect of an additional mm of bill depth, for example, on body mass in grams, which appears quite small.\n\n\nConclusion\nGreat - I hope that this was useful! It’s possible to easily customise your coefficient plots in ggplot, adding labels and colours to help your reader understand your regression results in a glance."
  },
  {
    "objectID": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "href": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "title": "ChatGPT fills in for me as a blog post author",
    "section": "",
    "text": "ChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\nToday I asked ChatGPT to to the same.\nIt took 30 seconds.\nHere are the results!\n\n\n\n\n\nPenguins created with DALL·E with the prompt “A diagram of three penguins in the style of Paul Cézanne, hyper realistic, beautiful art”\n\n\n‘Prompt engineering’ describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\nHere, I asked ChatGPT to:\n\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\n\n\n\n\nI recieved text and code that ran almost perfectly:\nIt began\n\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we’ll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\nTo start, let’s load the necessary packages :\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\n\nNext, we’ll load the Palmer Penguins dataset and take a look at the variables we have to work with:\n\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  <fct>   <fct>              <dbl>         <dbl>       <int>   <int> <fct> <int>\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA <NA>   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we’ll focus on the relationship between bill_length_mm and body_mass_g.\nTo understand the coefficient plot, we’ll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We’ll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\n\n\n\n\n\nNow we’ll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\n\n\nreg_results <- lm(body_mass_g ~ bill_length_mm, data = penguins)\nreg_tidy <- broom::tidy(reg_results, conf.int = TRUE)\n\n\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\nTo add the error bars, we’ll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\n\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\nAnd that’s it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\n\n\n\n\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nWow! Amazing. I learned a bunch about ggplot and R from reading the code which accompanied David Robinson’s screencasts in 2019 and 2020. I am excited for the generation of data analysts beginning their journey now - being able to ask for tailored instructions for a task is a real boon!"
  },
  {
    "objectID": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "href": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "title": "ChatGPT cooks me dinner",
    "section": "",
    "text": "Intro\nI ask ChatGPT to help my cook dinner based on the ingredients I had in the fridge - it was a great success!\nHave a look below at how it went!\nI also asked it to write this blog post, the output for which I include below.\n\n\nProcess\n\n\n  \nVideo\n\n\n\n\n\n\n\n\nVideo\n\n\nSuggested blog post text\nCooking is always a fun and creative experience, but sometimes it can be challenging to come up with new and exciting recipes. That’s why having a helpful tool like ChatGPT can make a big difference in the kitchen.\nRecently, I had the opportunity to put ChatGPT to the test as a cooking assistant. I provided a list of ingredients that I had on hand and asked for a recipe suggestion. ChatGPT quickly came up with a recipe for a Gammon and Vegetable Stew with Mint, along with detailed instructions for cooking the dish.\nI was impressed with the speed and accuracy of ChatGPT’s suggestion, and even more impressed with the quality of the recipe. The stew turned out to be delicious, hearty, and comforting - exactly what I was looking for on a cold winter day.\nBut ChatGPT’s assistance didn’t stop there. I asked if I could add a splash of Chardonnay to the stew and ChatGPT not only gave me a positive response, but also provided instructions on when and how to add the wine, along with a reminder to use a good quality Chardonnay to enhance the overall taste of the dish.\nOverall, my experience with ChatGPT as a cooking assistant was extremely positive. It was able to provide me with a recipe and instructions that resulted in a delicious and satisfying meal. I was impressed with its speed, accuracy, and helpful suggestions, and I would definitely use ChatGPT again for recipe ideas and cooking instructions.\n\n\nConclusion\nWhat an amazing tool!"
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome to Interlude One",
    "section": "",
    "text": "This is a line from Interlude One off of Alt-J’s first album, An Awesome Wave. I like this lyric a great deal!\nI aspire to balance in my own life. This site is a space to share what I find interesting, and encourage me to keep learning and sharing. It includes exploratory data analyses, R tutorials, and answers to some data specific questions. I hope that you enjoy your look around."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Jayes",
    "section": "",
    "text": "“I enjoy some of the old and I enjoy the new. And if I can find a balance between it, that’s where I find my satisfaction.”\n– Anderson .Paak - The Dreamer (feat. Talib Kweli & Timan Family Choir)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jonathan Jayes",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n  \n    \n  \n\nLund UniversityMaster's degree, Economics 2021  Exchange semester \n    \n  \n\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \n    \n  \n\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \n    \n  \n\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jonathan Jayes",
    "section": "Experience",
    "text": "Experience\nLaboratory for the Economics of Africa’s Past | Researcher under Prof Johan Fourie| January 2020 - July 2021\nFTI Consulting | Junior Economist in Economic and Financial Consulting Practice | March 2019 - January 2020"
  },
  {
    "objectID": "about.html#website-aesthetics",
    "href": "about.html#website-aesthetics",
    "title": "Jonathan Jayes",
    "section": "Website aesthetics",
    "text": "Website aesthetics\n\n$font-family-sans-serif: Noto Serif JP; $font-family-monospace: IBM Plex Mono; $font-family-base: Lato; $font-size-base: 1.1rem; $body-color: #404040; $primary: #2b8cbe; $secondary: #DEEBF7; $headings-margin-bottom: $spacer / 1; $headings-color: #2b8cbe; $headings-font-weight: 300;"
  }
]