[
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html",
    "href": "posts/2021-07-06-students-speak/students-speak.html",
    "title": "Students Speak",
    "section": "",
    "text": "Hi Johan - here are some visualizations I have put together from your students‚Äô diary entries. I think that they tell quite a nice story. I hope that some are useful. I‚Äôve done them in black and white - I‚Äôm not sure where you want to publish them in the end. If you‚Äôd like some colour I can add it easily.\nI‚Äôve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\n\n\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\n\n\n\nThe students‚Äô diary entries have been ingested to form a dataset such that each row is one student‚Äôs observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\n\n\n\n\n\n\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\n\n\n\nWe start with a word cloud which shows the words used by the students in their diary entries.\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\n\n\n\n\n\nWord cloud\n\n\n\n\nWe can see that common positive words include ‚Äúsupport‚Äù, ‚Äúprivileged‚Äù, ‚Äúhealthy‚Äù, ‚Äúproductive‚Äù, and ‚Äúexcited‚Äù. Common negative words are dominated by ‚Äúvirus‚Äù, followed by ‚Äúdifficult‚Äù, ‚Äústruggling‚Äù, and ‚Äúinfected‚Äù.\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students‚Äô diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\nTable @ref(tab:excluded) shows some common words in the students‚Äô diary entries which are excluded from the wordcloud in Figure @ref(fig:wordcloud).\n\n\n\nCommon words excluded from wordcloud\n\n\nWord\nNumber of uses\n\n\n\n\nLockdown\n251\n\n\nPeople\n211\n\n\nDay\n183\n\n\nTime\n172\n\n\nFamily\n111\n\n\nFeel\n92\n\n\nSouth\n90\n\n\nHome\n80\n\n\nWorld\n78\n\n\nDays\n71\n\n\nAfrica\n61\n\n\nLife\n54\n\n\nOnline\n51\n\n\n19\n50\n\n\nFriends\n50\n\n\n\n\n\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure @ref(fig:image-grobs).\n\n\n\nConventional word cloud and comparison cloud\n\n\n\n\n\nFigure @ref(fig:sentiment) below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students‚Äô diary entries in a particular week.\n\n\n\n\n\nEvolution of sentiment\n\n\n\n\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn √Örup Nielsen in 2009-2011. An example of the scores assigned to words in the students‚Äô diary entries is shown in Table @ref(tab:afinn) below.\n\n\n\nAFINN sentiment scores\n\n\nWord\nSentiment score\n\n\n\n\nBullshit\n-4\n\n\nCatastrophic\n-4\n\n\nPanic\n-3\n\n\nFake\n-3\n\n\nWorse\n-3\n\n\nFunny\n4\n\n\nFun\n4\n\n\nWonderful\n4\n\n\nThrilled\n5\n\n\n\n\n\n\n\n\nThe wordcloud in Figure @ref(fig:wordcloud) showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin‚Äô Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\nTable @ref(tab:words) below shows the words most specific to each week.\n\n\n\nWeek specific words calculated with weighted log odds\n\n\n\n\n\n\n\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\n\n\n\n\nAfrican leaders\nLockdown starts\nPrivilege\nEaster Sunday\nZoom\nClothing bank\n\n\nChurch\nVirus\nConspiracy theories\nPayment\nR500 billion\nLockdown restrictions\n\n\nAirports\nTownships\nAn obligation\nHot Cross buns\nSUN Learn\nLevel 5\n\n\nNSFAS\nCases recorded\nContinues to rise\nExtension\nEconomic stimulus\nLivelihoods\n\n\n\n\n\nNice! We can see that we capture some elements of the experience in each week of lockdown.\n\n\n\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students‚Äô sentiment across the weeks.\n\n\n\n\n\nSentiment and week-words figure\n\n\n\n\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\n\n\n\nFigure @ref(fig:context) shows the evolution of the sentiment of the students‚Äô diary entries beside the rising Covid-19 case numbers in South Africa.\n\n\n\n\n\nComparison figure\n\n\n\n\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students‚Äô reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\n\n\n\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\nI think option two conveys the message clearly and without clutter."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#students-speak",
    "href": "posts/2021-07-06-students-speak/students-speak.html#students-speak",
    "title": "Students Speak",
    "section": "",
    "text": "Hi Johan - here are some visualizations I have put together from your students‚Äô diary entries. I think that they tell quite a nice story. I hope that some are useful. I‚Äôve done them in black and white - I‚Äôm not sure where you want to publish them in the end. If you‚Äôd like some colour I can add it easily.\nI‚Äôve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\n\n\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\n\n\n\nThe students‚Äô diary entries have been ingested to form a dataset such that each row is one student‚Äôs observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\n\n\n\n\n\n\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\n\n\n\nWe start with a word cloud which shows the words used by the students in their diary entries.\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\n\n\n\n\n\nWord cloud\n\n\n\n\nWe can see that common positive words include ‚Äúsupport‚Äù, ‚Äúprivileged‚Äù, ‚Äúhealthy‚Äù, ‚Äúproductive‚Äù, and ‚Äúexcited‚Äù. Common negative words are dominated by ‚Äúvirus‚Äù, followed by ‚Äúdifficult‚Äù, ‚Äústruggling‚Äù, and ‚Äúinfected‚Äù.\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students‚Äô diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\nTable @ref(tab:excluded) shows some common words in the students‚Äô diary entries which are excluded from the wordcloud in Figure @ref(fig:wordcloud).\n\n\n\nCommon words excluded from wordcloud\n\n\nWord\nNumber of uses\n\n\n\n\nLockdown\n251\n\n\nPeople\n211\n\n\nDay\n183\n\n\nTime\n172\n\n\nFamily\n111\n\n\nFeel\n92\n\n\nSouth\n90\n\n\nHome\n80\n\n\nWorld\n78\n\n\nDays\n71\n\n\nAfrica\n61\n\n\nLife\n54\n\n\nOnline\n51\n\n\n19\n50\n\n\nFriends\n50\n\n\n\n\n\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure @ref(fig:image-grobs).\n\n\n\nConventional word cloud and comparison cloud\n\n\n\n\n\nFigure @ref(fig:sentiment) below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students‚Äô diary entries in a particular week.\n\n\n\n\n\nEvolution of sentiment\n\n\n\n\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn √Örup Nielsen in 2009-2011. An example of the scores assigned to words in the students‚Äô diary entries is shown in Table @ref(tab:afinn) below.\n\n\n\nAFINN sentiment scores\n\n\nWord\nSentiment score\n\n\n\n\nBullshit\n-4\n\n\nCatastrophic\n-4\n\n\nPanic\n-3\n\n\nFake\n-3\n\n\nWorse\n-3\n\n\nFunny\n4\n\n\nFun\n4\n\n\nWonderful\n4\n\n\nThrilled\n5\n\n\n\n\n\n\n\n\nThe wordcloud in Figure @ref(fig:wordcloud) showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin‚Äô Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\nTable @ref(tab:words) below shows the words most specific to each week.\n\n\n\nWeek specific words calculated with weighted log odds\n\n\n\n\n\n\n\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\n\n\n\n\nAfrican leaders\nLockdown starts\nPrivilege\nEaster Sunday\nZoom\nClothing bank\n\n\nChurch\nVirus\nConspiracy theories\nPayment\nR500 billion\nLockdown restrictions\n\n\nAirports\nTownships\nAn obligation\nHot Cross buns\nSUN Learn\nLevel 5\n\n\nNSFAS\nCases recorded\nContinues to rise\nExtension\nEconomic stimulus\nLivelihoods\n\n\n\n\n\nNice! We can see that we capture some elements of the experience in each week of lockdown.\n\n\n\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students‚Äô sentiment across the weeks.\n\n\n\n\n\nSentiment and week-words figure\n\n\n\n\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\n\n\n\nFigure @ref(fig:context) shows the evolution of the sentiment of the students‚Äô diary entries beside the rising Covid-19 case numbers in South Africa.\n\n\n\n\n\nComparison figure\n\n\n\n\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students‚Äô reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\n\n\n\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\nI think option two conveys the message clearly and without clutter."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "title": "Students Speak",
    "section": "Option 1",
    "text": "Option 1\nHere we have a two panel plot of the Covid-19 statistics and number of diary entries recorded by the students."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "title": "Students Speak",
    "section": "Option 2",
    "text": "Option 2\nNext we have a single panel with the period of diary entries superimposed on the Covid-19 statistics."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "title": "Students Speak",
    "section": "Option 3",
    "text": "Option 3\nAlternatively we can annotate a thick line to show where the diary entries occur."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "title": "Students Speak",
    "section": "Option 4",
    "text": "Option 4\nAlternatively we can have a legend variant of option 2."
  },
  {
    "objectID": "posts/2023-01-31-map-projections/Map-projections.html",
    "href": "posts/2023-01-31-map-projections/Map-projections.html",
    "title": "Map projections in R",
    "section": "",
    "text": "Transform two shapefiles to the same coordinate system and plot them together.\n\n\nWe load the tidyverse for data manipulation, the sf package fo working with maps and set the ggplot theme to a light one for more attractive plots.\n\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n\n\n\nWe read in our shapefiles with the st_read() function, and ask for output of the crs with the st_crs() function so that we can see what it is.\nFor Sweden we can see we are told in he last line Projected CRS: SWEREF99 TM.\nSWEREF99 TM a projection favoured by the statistics body in Sweden.\n\nsweden_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\",\n                                 \"sweden_map.shp\"))\n\nReading layer `sweden_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\sweden_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 278651.2 ymin: 6134621 xmax: 917514.5 ymax: 7668755\nProjected CRS: SWEREF99 TM\n\n\nFor Norway it is WGS 84, a very common projection with full name World Geodetic System 1984.\n\nnorway_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\", \n                                 \"norway_map.shp\"))\n\nReading layer `norway_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\norway_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -9.098877 ymin: 58.02095 xmax: 33.6293 ymax: 80.47783\nGeodetic CRS:  WGS 84\n\n\nLet‚Äôs have a quick look at them side by side.\nggplot()+ \n  geom_sf(data = sweden_map) +\n  labs(title = \"Sweden\",\n       subtitle = \"CRS = SWEREF99\")\nggplot()+ \n  geom_sf(data = norway_map) +\n  labs(title = \"Norway\",\n       subtitle = \"CRS = WGS84\")\n\n\n\n\n\n\nSweden\n\n\n\n\n\n\n\nNorway\n\n\n\n\n\n\n\n\nWe‚Äôll use EPSG 4326 or WGS84. We can use the st_transform() function, specifying the crs that we want for our new map object, which we name sweden_map_4326 to avoid confusion.\nThis is what the CRS now looks like for Sweden - the same as the one for Norway.\n\nsweden_map_4326 &lt;- st_transform(sweden_map, crs = 4326)\n\nsweden_map_4326 %&gt;% \n  st_crs()\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nWe can plot the together and choose some nice colours from the national flags for the fill aesthetic.\n\nggplot() +\n  geom_sf(data = sweden_map_4326, fill = \"#03558A\") +\n  geom_sf(data = norway_map, fill = \"#B40C2E\")\n\n\n\n\nMap of Sweden and Norway\n\n\n\n\n\n\n\nWe can save the shapefile for Sweden with the st_write() command if we want to keep the maps separate.\n\nsweden_map_4326 %&gt;% \n  st_write(here::here(\"posts\", \n                      \"2023-01-31-map-projections\", \n                      \"data\", \n                      \"sweden_map_4326.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting layer `sweden_map_4326' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting 1 features with 63 fields and geometry type Multi Polygon.\n\n\n\n\n\nAlternatively we can join together the maps as one object and then save one shapefile with the maps combined.\nTo do this, we first bind the two objects together using bind_rows() command and write it out with the same st_write() command.\n\ncombined_map &lt;- bind_rows(norway_map, sweden_map_4326)\n\ncombined_map %&gt;% st_write(here::here(\"posts\", \n                                     \"2023-01-31-map-projections\", \n                                     \"data\", \n                                     \"shapefile_combined.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting layer `shapefile_combined' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting 2 features with 63 fields and geometry type Multi Polygon.\n\n\nIf we want to get rid of the borders between our objects, we can use the st_union() command:\n\nunion_map &lt;- st_union(norway_map, sweden_map_4326)\n\nWe can compare the outputs here:\n# Combined map\nggplot() +\n  geom_sf(data = combined_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"#B40C2E\", \"#03558A\")) +\n  labs(fill = \"Country\")\n# Union map\nggplot() +\n  geom_sf(data = union_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"darkgreen\")) +\n  labs(fill = \"Country\")\n\n\n\n\n\n\nOutput from bind_rows\n\n\n\n\n\n\n\nOutput from st_union\n\n\n\n\n\nWhile st_union() is not useful in this particular instance, it may be of use to you in the future!\n\n\n\nHere we learned how to make it possible to plot maps with two different projections together by making use of the st_transform() command in R. We learned how to save the two maps as separate features in a shapefile, as well as how to combine them together. We can import the shapefile into any other software for plotting."
  },
  {
    "objectID": "posts/2023-01-31-map-projections/Map-projections.html#purpose",
    "href": "posts/2023-01-31-map-projections/Map-projections.html#purpose",
    "title": "Map projections in R",
    "section": "",
    "text": "Transform two shapefiles to the same coordinate system and plot them together.\n\n\nWe load the tidyverse for data manipulation, the sf package fo working with maps and set the ggplot theme to a light one for more attractive plots.\n\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n\n\n\nWe read in our shapefiles with the st_read() function, and ask for output of the crs with the st_crs() function so that we can see what it is.\nFor Sweden we can see we are told in he last line Projected CRS: SWEREF99 TM.\nSWEREF99 TM a projection favoured by the statistics body in Sweden.\n\nsweden_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\",\n                                 \"sweden_map.shp\"))\n\nReading layer `sweden_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\sweden_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 278651.2 ymin: 6134621 xmax: 917514.5 ymax: 7668755\nProjected CRS: SWEREF99 TM\n\n\nFor Norway it is WGS 84, a very common projection with full name World Geodetic System 1984.\n\nnorway_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\", \n                                 \"norway_map.shp\"))\n\nReading layer `norway_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\norway_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -9.098877 ymin: 58.02095 xmax: 33.6293 ymax: 80.47783\nGeodetic CRS:  WGS 84\n\n\nLet‚Äôs have a quick look at them side by side.\nggplot()+ \n  geom_sf(data = sweden_map) +\n  labs(title = \"Sweden\",\n       subtitle = \"CRS = SWEREF99\")\nggplot()+ \n  geom_sf(data = norway_map) +\n  labs(title = \"Norway\",\n       subtitle = \"CRS = WGS84\")\n\n\n\n\n\n\nSweden\n\n\n\n\n\n\n\nNorway\n\n\n\n\n\n\n\n\nWe‚Äôll use EPSG 4326 or WGS84. We can use the st_transform() function, specifying the crs that we want for our new map object, which we name sweden_map_4326 to avoid confusion.\nThis is what the CRS now looks like for Sweden - the same as the one for Norway.\n\nsweden_map_4326 &lt;- st_transform(sweden_map, crs = 4326)\n\nsweden_map_4326 %&gt;% \n  st_crs()\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nWe can plot the together and choose some nice colours from the national flags for the fill aesthetic.\n\nggplot() +\n  geom_sf(data = sweden_map_4326, fill = \"#03558A\") +\n  geom_sf(data = norway_map, fill = \"#B40C2E\")\n\n\n\n\nMap of Sweden and Norway\n\n\n\n\n\n\n\nWe can save the shapefile for Sweden with the st_write() command if we want to keep the maps separate.\n\nsweden_map_4326 %&gt;% \n  st_write(here::here(\"posts\", \n                      \"2023-01-31-map-projections\", \n                      \"data\", \n                      \"sweden_map_4326.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting layer `sweden_map_4326' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting 1 features with 63 fields and geometry type Multi Polygon.\n\n\n\n\n\nAlternatively we can join together the maps as one object and then save one shapefile with the maps combined.\nTo do this, we first bind the two objects together using bind_rows() command and write it out with the same st_write() command.\n\ncombined_map &lt;- bind_rows(norway_map, sweden_map_4326)\n\ncombined_map %&gt;% st_write(here::here(\"posts\", \n                                     \"2023-01-31-map-projections\", \n                                     \"data\", \n                                     \"shapefile_combined.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting layer `shapefile_combined' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting 2 features with 63 fields and geometry type Multi Polygon.\n\n\nIf we want to get rid of the borders between our objects, we can use the st_union() command:\n\nunion_map &lt;- st_union(norway_map, sweden_map_4326)\n\nWe can compare the outputs here:\n# Combined map\nggplot() +\n  geom_sf(data = combined_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"#B40C2E\", \"#03558A\")) +\n  labs(fill = \"Country\")\n# Union map\nggplot() +\n  geom_sf(data = union_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"darkgreen\")) +\n  labs(fill = \"Country\")\n\n\n\n\n\n\nOutput from bind_rows\n\n\n\n\n\n\n\nOutput from st_union\n\n\n\n\n\nWhile st_union() is not useful in this particular instance, it may be of use to you in the future!\n\n\n\nHere we learned how to make it possible to plot maps with two different projections together by making use of the st_transform() command in R. We learned how to save the two maps as separate features in a shapefile, as well as how to combine them together. We can import the shapefile into any other software for plotting."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html",
    "href": "posts/2023-10-02-geocoding/geocoding.html",
    "title": "Geocoding: two ways",
    "section": "",
    "text": "Locating places of interest on a map is an important task in many economic history projects. This tutorial will show you how to geocode a list of addresses.\nI will use both R and python to show you how to do this, depending on your preference and what you are most comfortable with.\n\n\n\n\n\n\nA beautiful oil painting of a mosaic of a map or a city plan in the unique texture and color palette of Gustav Caillebotte"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#introduction",
    "href": "posts/2023-10-02-geocoding/geocoding.html#introduction",
    "title": "Geocoding: two ways",
    "section": "",
    "text": "Locating places of interest on a map is an important task in many economic history projects. This tutorial will show you how to geocode a list of addresses.\nI will use both R and python to show you how to do this, depending on your preference and what you are most comfortable with.\n\n\n\n\n\n\nA beautiful oil painting of a mosaic of a map or a city plan in the unique texture and color palette of Gustav Caillebotte"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#getting-started",
    "href": "posts/2023-10-02-geocoding/geocoding.html#getting-started",
    "title": "Geocoding: two ways",
    "section": "Getting started",
    "text": "Getting started\nThere are many engines that you can use to geocode place names or addresses to coordinates, some popular engines include the Open Street Map Nominatim API and the Google Maps API.\nConceptually, there is a distinction between the code that you use to call the engine which gets the coordinates (R or python), and then there is the engine itself (OSM or Google Maps).\n\n\n\n\n\n\n\n\n\nFramework\nR (tidygeocoder) & Open Street Map Nominatim API\nPython (requests) & Google Maps Geocoding API\n\n\n\n\nGeocoding Engine\nOpen Street Map‚Äôs Nominatim\nGoogle Maps Geocoding\n\n\nRequest Method\nUses the tidygeocoder R package to send requests.\nUses the requests Python library to send HTTP requests.\n\n\nCost\nFree and open-source, but has usage policies to prevent heavy traffic.\nOffers limited free requests, billing is required for extensive usage.\n\n\n\n\nI use the tidygeocoder package in R, which is free, and the Google Maps Geocoding API in python, which is also free (up to some threshold), but requires an API key from Google and a credit card to sign up.1\nYou can read about getting an API key here."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#data",
    "href": "posts/2023-10-02-geocoding/geocoding.html#data",
    "title": "Geocoding: two ways",
    "section": "Data",
    "text": "Data\nFor the purpose of this tutorial, I have generated 20 place names to geocode and a column of arbitrary values for visualization.\n\n\n\nPlace Name\nArbitrary Value\n\n\n\n\nStockholm\n35\n\n\nGothenburg\n12\n\n\nMalm√∂\n27\n\n\nUppsala\n19\n\n\nV√§ster√•s\n22\n\n\n√ñrebro\n15\n\n\nLink√∂ping\n20\n\n\nHelsingborg\n24\n\n\nNorrk√∂ping\n28\n\n\nJ√∂nk√∂ping\n17\n\n\nLund\n23\n\n\nUme√•\n10\n\n\nG√§vle\n18\n\n\nBor√•s\n14\n\n\nEskilstuna\n16\n\n\nS√∂dert√§lje\n13\n\n\nKarlstad\n21\n\n\nT√§by\n29\n\n\nV√§xj√∂\n11\n\n\nHalmstad\n26"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#r-and-tidygeocoder",
    "href": "posts/2023-10-02-geocoding/geocoding.html#r-and-tidygeocoder",
    "title": "Geocoding: two ways",
    "section": "R and tidygeocoder",
    "text": "R and tidygeocoder\nJesse Cambon has kindly made a package called tidygeocoder available on github.\nYou can install it on your computer using the following code:\n\ninstall.packages('tidygeocoder')\n\nHere is how you geocode a list of addresses using the tidygeocoder package.\nNote that lat and long here will be the column names for the coordinates that are returned.\nThe line method = \"osm\" specifies that we are using the Open Street Map Nominatim API. Here you can switch to another engine if you prefer.\n\nlibrary(tidyverse) # for data manipulation\nlibrary(tidygeocoder) # for geocoding\n\n# Read in data\nlibrary(readxl) # for reading excel files\nplaces &lt;- read_excel(\"data/data_to_geocode.xlsx\")\n\n# Geocode places\nplaces_geocoded &lt;- places %&gt;%\n    geocode(place_name,\n        method = \"osm\",\n        lat = latitude,\n        long = longitude\n    )\n\nThis what the output looks like when the geocoding is complete.\n\nPassing 20 addresses to the Nominatim single address geocoder [===================================] 20/20 (100%) Elapsed: 20s Remaining: 0s\n\nWe can visualize the results using the ggplot2 package.\n\n# Find bounds to trim map, adding some padding\nbounds &lt;- places_geocoded %&gt;%\n    summarise(xmin = min(longitude) - 0.1,\n              xmax = max(longitude) + 0.1,\n              ymin = min(latitude) - 0.1,\n              ymax = max(latitude) + 0.1)\n\n# Plot places\nggplot(places_geocoded, aes(longitude, latitude), color = \"grey99\") +\n    borders() +\n    geom_point(aes(size = arbitrary_value), colour = \"midnightblue\") +\n    ggrepel::geom_label_repel(aes(label = place_name)) +\n    theme_void() +\n    coord_cartesian(xlim = c(bounds$xmin, bounds$xmax), \n                    ylim = c(bounds$ymin, bounds$ymax)) +\n    labs(size = \"Arbitrary value\")\n\n\n\n\n\n\n\n\nGreat! We have successfully geocoded our list of addresses."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#geocoding-in-python",
    "href": "posts/2023-10-02-geocoding/geocoding.html#geocoding-in-python",
    "title": "Geocoding: two ways",
    "section": "Geocoding in python",
    "text": "Geocoding in python\nI have created a Colab notebook that shows you how to geocode addresses using the Google Maps Geocoding API in python. You can simply run the code in the notebook to get the coordinates of your own data by providing your own API key and data.\nYou can have a look at the Colab notebook here"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#real-world-use-case",
    "href": "posts/2023-10-02-geocoding/geocoding.html#real-world-use-case",
    "title": "Geocoding: two ways",
    "section": "Real world use case",
    "text": "Real world use case\nIn reality, it will likely not be as simple as running the above code once and getting all the coordinates you need.\nYou will likely need to do multiple passes, modify addresses, and evaluate the results.\nMultiple passes are necessary to maximize the accuracy and success rate of geocoding. The initial pass may not successfully geocode all addresses due to inaccuracies, ambiguities, or limitations of the geocoding tool. By modifying the problematic addresses and doing subsequent passes, you can resolve these issues and geocode more addresses correctly.\nFor example, an address might fail to geocode because it contains a typo, is too specific, or too ambiguous. By modifying the address to correct the typo or by making it more general or more specific, it might geocode successfully in a subsequent pass.\nThe term ‚Äúout of bounds‚Äù in the context of geocoding refers to the geocoded results that do not fit within a predefined geographical area or are not reasonable based on prior knowledge. For instance, if you are geocoding addresses in Sweden, any result that lies outside the borders of Sweden would be considered ‚Äúout of bounds.‚Äù\nIn practical terms, if you know the general area where the addresses should be located (like a specific country or city), you can compare the geocoded results against this area. If the geocoded location falls outside this area, it is considered out of bounds, and you might need to modify the address and try geocoding it again, or use a different geocoding engine that might provide more accurate results.\nHere is a diagram that shows the real world process of geocoding in more detail.\n\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; FirstPass\n    FirstPass --&gt; StoreResults: In bounds\n    FirstPass --&gt; ModifyAddresses: Out of bounds\n    \n    ModifyAddresses --&gt; SecondPass\n    \n    SecondPass --&gt; StoreSecondPassResults: In bounds\n    SecondPass --&gt; Evaluate: Out of bounds\n    \n    StoreResults --&gt; Evaluate\n    StoreSecondPassResults --&gt; Evaluate\n    \n    Evaluate --&gt; ManualGeocode: Handful left\n    Evaluate --&gt; TryDifferentEngine: Many left\n    \n    ManualGeocode --&gt; FinalResults\n    TryDifferentEngine --&gt; FinalResults\n    \n    FinalResults --&gt; [*]\n\n\n\n\n\n\n\n\n1. First Pass\n\nObjective: To attempt an initial conversion of place names or addresses into geographical coordinates.\nExample: If you have a list of street addresses in Sweden, you use a geocoding tool to convert these addresses into geographical coordinates.\nOutcome: Some addresses will be successfully geocoded within the reasonable bounds (i.e., within Sweden), and some may not be (i.e., places outside Sweden).\n\n\n\n2. Modify Addresses\n\nObjective: To modify the addresses that were not successfully geocoded in the first pass.\nExample: If an address, ‚Äú123 Main St, Stockholm,‚Äù fails in the first pass, you might modify it to just ‚ÄúStockholm‚Äù or append the country, ‚Äú123 Main St, Stockholm, Sweden.‚Äù\nOutcome: The modified addresses are ready for the second pass.\n\n\n\n3. Second Pass\n\nObjective: To attempt the conversion again with the modified addresses.\nOutcome: More addresses will be successfully geocoded within the reasonable bounds.\n\n\n\n4. Evaluate Results\n\nObjective: To assess the number of addresses that are left un-geocoded after the second pass.\nOutcome: Determine whether manual geocoding or trying a different geocoding engine is needed.\n\n\n\n5. Manual Geocode or Try Different Engine\n\nObjective: To geocode the remaining un-geocoded addresses.\nExample: If there are only a few addresses left, you might geocode them by hand using online maps. If there are many addresses left, you might switch to a different geocoding engine, e.g., from Open Street Map to Google Maps.\nOutcome: All addresses are successfully geocoded.\n\n\n\n6. Final Results\n\nObjective: To obtain a complete list of geographical coordinates corresponding to the initial list of place names or addresses.\nOutcome: A successfully geocoded list is ready for further use or analysis."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#footnotes",
    "href": "posts/2023-10-02-geocoding/geocoding.html#footnotes",
    "title": "Geocoding: two ways",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use the Google Maps Geocoding API in R through the tidygeocoder package, but you will still need to sign up for an API key from Google.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html",
    "title": "Electricity generation per capita",
    "section": "",
    "text": "üî•In this blog post, we will take a closer look at the evolution of electricity generation per capita across the world and examine how China has rapidly climbed above its neighbors in per capita termsüìà. Through the use of data visualizationüìä, we will explore the decrease of electricity generation per capita in New Zealand over the past few decades and compare it to other countries in the regionüåé. This analysis will provide a deeper understanding of the world‚Äôs energy landscape and the global energy marketüå°Ô∏è. So, let‚Äôs dive in and see how energy consumption has evolved over time!üîç"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#introduction",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#introduction",
    "title": "Electricity generation per capita",
    "section": "",
    "text": "üî•In this blog post, we will take a closer look at the evolution of electricity generation per capita across the world and examine how China has rapidly climbed above its neighbors in per capita termsüìà. Through the use of data visualizationüìä, we will explore the decrease of electricity generation per capita in New Zealand over the past few decades and compare it to other countries in the regionüåé. This analysis will provide a deeper understanding of the world‚Äôs energy landscape and the global energy marketüå°Ô∏è. So, let‚Äôs dive in and see how energy consumption has evolved over time!üîç"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#inspiration",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#inspiration",
    "title": "Electricity generation per capita",
    "section": "Inspiration",
    "text": "Inspiration\nüöÄ I am inspired by the company Electricity Maps based in Copenhagen, who allow you to understand the electricity grid through data. They collect data from all around the world and compute the data in an hourly resolution, allowing you to gain granular insights accounting for the variance of the grid.üí° While we won‚Äôt use their data in this blog post, we will take a look at some great annual statistics compiled by the team at Our World in Data. This organization is dedicated to providing data-driven insights on global issues and their data on electricity generation per capita is a valuable resource for understanding how different countries are progressing in terms of energy production.üìä"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#visualization",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#visualization",
    "title": "Electricity generation per capita",
    "section": "Visualization",
    "text": "Visualization\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow we can see that South Africa has climbed the rankings from 18th place in 2000 up to 7th in 2020. Interesting!\n\nWhat about the other continents?\nHave a look at each below, and then give my Shiny app a gander (TODO).\n\n\nAmericasAsiaEuropeOceania"
  },
  {
    "objectID": "posts/2024-11-19-grey-couches/grey-couches.html",
    "href": "posts/2024-11-19-grey-couches/grey-couches.html",
    "title": "NeverTooSmall Couch Colours",
    "section": "",
    "text": "I really enjoy watching the YouTube channel NeverTooSmall, which showcases cleverly designed small spaces from around the world.\nI was watching an episode with my girlfriend recently when she remarked, ‚ÄúDoesn‚Äôt it feel like every second couch is a boring grey one on this channel? Why can‚Äôt they choose some more fun colors?!‚Äù\n\n\nThere are probably a couple of practical reasons people might go for a grey couch. For one, in a small space, a bold color can quickly overpower the room. And secondly, grey doesn‚Äôt show wear and discoloration as easily as white, which is often a popular choice for its stylish look.\nBut Kathy‚Äôs comment sent me down a rabbit hole. I wanted to find out how many of the couches on NeverTooSmall were actually grey.\nMy close friend and flatmate duing my Master‚Äôs degree, Nick, once confidently said regarding his interest in audio processing that, ‚ÄúComputer vision is a solved field‚Äù‚Äîand that was five years ago! So I thought, how hard could it be to use a small, open-source, CPU-based computer vision model to analyze the videos from NeverTooSmall, identify the frames with a couch, extract its color, and visualize the results in a nice waffle chart?\nIt turns out, it‚Äôs not that hard. In this post, I‚Äôll share the answer to my question and, if you‚Äôre interested, explain the technical steps I took to get there.\n\n\nCode\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(gt)\n\nsetwd(here::here(\"posts\", \"2024-11-19-grey-couches\"))\n\ndf &lt;- fromJSON(\"data/couch_info_with_colour_classifications.json\")\n\ndf &lt;- as_tibble(df)\n\nn_videos &lt;- nrow(df)\n\nn_couches &lt;- nrow(df %&gt;% filter(couch_detected))\n\nn_grey_couches &lt;- nrow(df %&gt;% filter(couch_detected, couch_colour == \"grey\"))\n\n\ncolor_mapping &lt;- c(\n    \"beige\" = \"#F5F5DC\", # Soft beige\n    \"light blue\" = \"#A9D8E6\", # Lightened sky blue\n    \"green\" = \"#A3DCA3\", # Soft pastel green\n    \"grey\" = \"#C0C0C0\", # Light grey\n    \"blue\" = \"#87CEEB\", # Light blue\n    \"black\" = \"#4E4E4E\", # Dark grey to represent pastel black\n    \"brown\" = \"#C8A165\", # Light brown\n    \"white\" = \"#FFFFFF\", # White\n    \"red\" = \"#FF9999\", # Lightened true red\n    \"yellow\" = \"#FFFFB3\", # Soft pastel yellow\n    \"teal\" = \"#99CED3\", # Soft teal\n    \"orange\" = \"#FFCC99\", # Lightened orange\n    \"pink\" = \"#FFB6C1\", # Soft pink\n    \"purple\" = \"#CBA1E6\" # Light purple\n)\n\n\n# Expand the data to repeat each color based on the count\nexpanded_df &lt;- df %&gt;%\n    count(couch_colour) %&gt;%\n    filter(!is.na(couch_colour)) %&gt;%\n    arrange(desc(n)) %&gt;%\n    uncount(n) %&gt;%\n    mutate(tile_id = row_number())\n\n# Calculate the number of columns and rows for the grid\nn_cols &lt;- ceiling(sqrt(nrow(expanded_df)))\n\n# Create a grid with the colors\ngrid &lt;- expand.grid(x = 1:n_cols, y = 1:n_cols) %&gt;%\n    slice(1:nrow(expanded_df)) %&gt;%\n    bind_cols(expanded_df)\n\n# Create legend order using number of couches\nlegend_order &lt;- expanded_df %&gt;%\n    group_by(couch_colour) %&gt;%\n    summarise(n = n()) %&gt;%\n    arrange(desc(n)) %&gt;%\n    pull(couch_colour)\n\n# Plot the grid with colors\ngrid %&gt;%\n    ggplot(aes(x = x, y = y, fill = factor(couch_colour, levels = legend_order))) +\n    geom_tile(color = \"black\") +\n    scale_fill_manual(values = color_mapping) +\n    theme_void() +\n    coord_fixed() +\n    labs(\n        # title = \"The colours of the couches in NeverTooSmall videos\",\n        fill = \"Couch Colour\"\n    ) +\n    guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\nSpoiler Alert: Out of the 189 videos on the channel, I identified 148 couches, and 61 of them were grey! That‚Äôs nearly three times the next most common color, beige, and four times the third most common color, white.\n\n\n\n\n\n\n\n\n\n\nNeverTooSmall Couches\n\n\n\n\nGrey\n61\n\n\nNo couch\n41\n\n\nBeige\n23\n\n\nWhite\n15\n\n\nGreen\n12\n\n\nBlue\n11\n\n\nBlack\n8\n\n\nBrown\n7\n\n\nRed\n4\n\n\nOrange\n2\n\n\nLight Blue\n1\n\n\nPink\n1\n\n\nPurple\n1\n\n\nTeal\n1\n\n\nYellow\n1"
  },
  {
    "objectID": "posts/2024-11-19-grey-couches/grey-couches.html#intro",
    "href": "posts/2024-11-19-grey-couches/grey-couches.html#intro",
    "title": "NeverTooSmall Couch Colours",
    "section": "",
    "text": "I really enjoy watching the YouTube channel NeverTooSmall, which showcases cleverly designed small spaces from around the world.\nI was watching an episode with my girlfriend recently when she remarked, ‚ÄúDoesn‚Äôt it feel like every second couch is a boring grey one on this channel? Why can‚Äôt they choose some more fun colors?!‚Äù\n\n\nThere are probably a couple of practical reasons people might go for a grey couch. For one, in a small space, a bold color can quickly overpower the room. And secondly, grey doesn‚Äôt show wear and discoloration as easily as white, which is often a popular choice for its stylish look.\nBut Kathy‚Äôs comment sent me down a rabbit hole. I wanted to find out how many of the couches on NeverTooSmall were actually grey.\nMy close friend and flatmate duing my Master‚Äôs degree, Nick, once confidently said regarding his interest in audio processing that, ‚ÄúComputer vision is a solved field‚Äù‚Äîand that was five years ago! So I thought, how hard could it be to use a small, open-source, CPU-based computer vision model to analyze the videos from NeverTooSmall, identify the frames with a couch, extract its color, and visualize the results in a nice waffle chart?\nIt turns out, it‚Äôs not that hard. In this post, I‚Äôll share the answer to my question and, if you‚Äôre interested, explain the technical steps I took to get there.\n\n\nCode\nlibrary(tidyverse)\nlibrary(jsonlite)\nlibrary(gt)\n\nsetwd(here::here(\"posts\", \"2024-11-19-grey-couches\"))\n\ndf &lt;- fromJSON(\"data/couch_info_with_colour_classifications.json\")\n\ndf &lt;- as_tibble(df)\n\nn_videos &lt;- nrow(df)\n\nn_couches &lt;- nrow(df %&gt;% filter(couch_detected))\n\nn_grey_couches &lt;- nrow(df %&gt;% filter(couch_detected, couch_colour == \"grey\"))\n\n\ncolor_mapping &lt;- c(\n    \"beige\" = \"#F5F5DC\", # Soft beige\n    \"light blue\" = \"#A9D8E6\", # Lightened sky blue\n    \"green\" = \"#A3DCA3\", # Soft pastel green\n    \"grey\" = \"#C0C0C0\", # Light grey\n    \"blue\" = \"#87CEEB\", # Light blue\n    \"black\" = \"#4E4E4E\", # Dark grey to represent pastel black\n    \"brown\" = \"#C8A165\", # Light brown\n    \"white\" = \"#FFFFFF\", # White\n    \"red\" = \"#FF9999\", # Lightened true red\n    \"yellow\" = \"#FFFFB3\", # Soft pastel yellow\n    \"teal\" = \"#99CED3\", # Soft teal\n    \"orange\" = \"#FFCC99\", # Lightened orange\n    \"pink\" = \"#FFB6C1\", # Soft pink\n    \"purple\" = \"#CBA1E6\" # Light purple\n)\n\n\n# Expand the data to repeat each color based on the count\nexpanded_df &lt;- df %&gt;%\n    count(couch_colour) %&gt;%\n    filter(!is.na(couch_colour)) %&gt;%\n    arrange(desc(n)) %&gt;%\n    uncount(n) %&gt;%\n    mutate(tile_id = row_number())\n\n# Calculate the number of columns and rows for the grid\nn_cols &lt;- ceiling(sqrt(nrow(expanded_df)))\n\n# Create a grid with the colors\ngrid &lt;- expand.grid(x = 1:n_cols, y = 1:n_cols) %&gt;%\n    slice(1:nrow(expanded_df)) %&gt;%\n    bind_cols(expanded_df)\n\n# Create legend order using number of couches\nlegend_order &lt;- expanded_df %&gt;%\n    group_by(couch_colour) %&gt;%\n    summarise(n = n()) %&gt;%\n    arrange(desc(n)) %&gt;%\n    pull(couch_colour)\n\n# Plot the grid with colors\ngrid %&gt;%\n    ggplot(aes(x = x, y = y, fill = factor(couch_colour, levels = legend_order))) +\n    geom_tile(color = \"black\") +\n    scale_fill_manual(values = color_mapping) +\n    theme_void() +\n    coord_fixed() +\n    labs(\n        # title = \"The colours of the couches in NeverTooSmall videos\",\n        fill = \"Couch Colour\"\n    ) +\n    guides(fill = guide_legend(reverse = TRUE))\n\n\n\n\n\n\n\n\n\nSpoiler Alert: Out of the 189 videos on the channel, I identified 148 couches, and 61 of them were grey! That‚Äôs nearly three times the next most common color, beige, and four times the third most common color, white.\n\n\n\n\n\n\n\n\n\n\nNeverTooSmall Couches\n\n\n\n\nGrey\n61\n\n\nNo couch\n41\n\n\nBeige\n23\n\n\nWhite\n15\n\n\nGreen\n12\n\n\nBlue\n11\n\n\nBlack\n8\n\n\nBrown\n7\n\n\nRed\n4\n\n\nOrange\n2\n\n\nLight Blue\n1\n\n\nPink\n1\n\n\nPurple\n1\n\n\nTeal\n1\n\n\nYellow\n1"
  },
  {
    "objectID": "posts/2024-11-19-grey-couches/grey-couches.html#how-did-we-do-it",
    "href": "posts/2024-11-19-grey-couches/grey-couches.html#how-did-we-do-it",
    "title": "NeverTooSmall Couch Colours",
    "section": "How Did We Do It?",
    "text": "How Did We Do It?\nI made all my code available for this project in the GitHub repo linked here.\nHere‚Äôs how I broke down the project:\n\nIngest data from the NeverTooSmall YouTube playlist.\nIdentify frames in the videos that contain a couch, selecting the frame with the largest ratio of couch-to-frame area.\nIsolate the image of the couch from this frame.\nClassify the couch‚Äôs color.\nVisualize the results.\n\nIn the process, I ran into some challenges‚Äîespecially with color classification‚Äîso I tried two approaches to get accurate results. I‚Äôll walk you through each step in detail below.\nI think it is worth noting here that while the task of telling what colour a couch is may seem trivial, the fact that it seems so is truly a testament to the fantastic computer vision libraries available today that make it so easy to do.\n\n\n1. Ingesting the Data\nThis part was relatively straightforward. Using the yt_dlp library, I pulled video information from the official NeverTooSmall playlist and stored the thumbnails in a folder in my repo.\n\n\n2. Identifying Frames with a Couch\nLuckily, there are a plethora of open-source computer vision models available on platforms like Hugging Face. For this project, I chose a model that performs well on just a CPU and is trained to identify about 80 objects, including couches: the You Only Look Once (YOLO) v8 model. YOLO was originally created in 2016 by Joseph Redmon and coauthors, who aimed to make object detection fast and efficient for real-time applications.1\nI wrote a loop to analyze every 100th frame of each NeverTooSmall video, checking if the frame contained a couch and, if so, calculating the ratio of the couch‚Äôs bounding box to the frame. At the end of each loop, I selected the frame with the largest ratio, giving me the clearest image of the couch. While there are probably more advanced ways to do this, this method was simple and effective.\n\n\n\nHere is an example of frame selection from the Sydney Loft House Small Homes by Brad Swartz above\n\n\n\n\n3. Isolating the Couch Image from the Frame\nNext, I used a different version of the YOLO model to segment the frame and extract only the part containing the couch, removing everything else in the image. This step allowed me to focus solely on the couch for color classification.\n\n\n\nHere is an example of couch segmentation from the Sydney Loft House Small Homes by Brad Swartz above\n\n\n\n\n4. Classifying the Couch‚Äôs Color\nFor color classification, I initially used a standard clustering algorithm, k-means, to determine the most common 5 colours in the segmented couch image. However, this approach had its limitations. For example, in one instance, the couch was wooden with a white bench cushion and large green scatter cushions. Since the scatter cushions took up more space, my k-means algorithm gave most weight to the color green.2 If asked, though, I‚Äôd classify this couch as white based on the cushion.\nAnother challenge came from the variations in lighting conditions across different videos, as the shots are filmed at various times of day and in different locations. This lighting variance made it hard to extract a general color, as each image offered a very specific shade instead. I created a colour strip for each couch including five colours, weighted by their frequency in the image. As you can see in a gif of these strips in the margin, the colours vary significantly, and are quite dull in some cases.\n\n\n\n\n\nThe 5 colour strips extracted from the segmented image of each couch\n\n\nTo address these issues, I tried grouping colors by their HSV values to create a more generalized classification. While this produced a visually appealing waffle chart, it didn‚Äôt fully answer Kathy‚Äôs question about the dominant couch color.\n\n\n\nThe visually appealing waffle chart of couch colors\n\n\n\nSidebar: A bit about color perception\nIt‚Äôs easy for a human to answer, ‚ÄúWhat color is this?‚Äù because our brains are adept at contextualizing color based on lighting, surrounding colors, and other factors. For a computer, though, the task is more challenging. Computers see a ‚Äúpure‚Äù color without context, so they struggle with nuances that are obvious to us. This challenge made me appreciate the difficulty of color perception in computer vision.\n\nTo improve accuracy, I took a different approach, using the OpenAI API, which now accepts images. I simply asked the GPT-4o model to classify the couch color based on the segmented image. OpenAI‚Äôs API now allows you to specify a data model in your request to ensure you get a specific type of response. Using Pydantic data models in the API request helped me standardize outputs and reduced errors significantly. I can really recommend having a look at the structured outputs option if you are working with the OpenAI API.\nHere is the simple data model and prompt I used to get the color classification from the API:\n\n\nCode\nclass CouchColourClassification(BaseModel):\n    couch_colour: str = Field(description=\"The classified color of the couch\")\n\nPROMPT = \"\"\"\nYou will see a still from a video. What color is the couch in the image? Use a single word to describe the color, e.g. 'white', 'black', 'grey', 'beige', 'blue', 'green', 'red', 'brown', 'purple', 'yellow', 'orange', 'pink'. If the couch has multiple colors, choose the most prominent one. \n\"\"\"\n\n\n\n\n5. Visualizing the Results\nI‚Äôm a fan of the waffle chart for representing shares‚Äîit‚Äôs easier to interpret than a pie chart and, in my opinion, looks great! The visual result of our color classification highlights the distribution of couch colors across the channel‚Äôs videos.\n\n\n\nThe waffle chart of couch colors\n\n\nAdditionally, I used Quarto‚Äôs tabsets to group videos by couch color, so you can explore them below.\n\nRed CouchesBrown CouchesBlack CouchesBlue CouchesGreen CouchesWhite CouchesBeige CouchesGrey Couches\n\n\n\n\n\n\n\n\nNEVER TOO SMALL Sydney Small Apartment for Downsizers - 38sqm/410sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs 90‚Äôs Inspired Apartment, Paris 57sqm/614sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Slide and Fold Micro Apartment, Brazil 29sqm/312sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Spanish Couple‚Äôs Multifunctional Apartment, Seville 55sqm/592sqft\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL London Heritage Loft Apartment Conversion 54sqm/581sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Hong Kong Small Treehouse Loft Apartment - 33sqm/355sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Japanese 5S Micro Apartment - 24sqm/260sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL 40sqm/430sqft Small Apartment - Karoot Apartment\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Paris Architect/Book Lover‚Äôs Cozy Tiny Apartment - 25sqm/270sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Modern Compact Japanese Family Home, Osaka - 57sqm/613sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Family of 5‚Äôs Simple Home, Japan 45sqm/483sqft\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs Treehouse Inspired Loft Apartment, Madrid 45sqm/484sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Tokyo Light-Filled Industrial Oasis, 59sqm/635sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Micro apartment - 22sqm/236sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL 1800‚Äôs Milanese Micro Loft Apartment - 14sqm/150sqft\n\n\n  \n\n\n\n\nDesigning Happier Living - SMALL FOOTPRINT - Ep 1\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Bold, Two Toned Small Apartment, Singapore - 47sqm/505sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: From Dark to Light Heritage Attic Transformation, Lisbon 60sqm/645sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Contemporary Attic Conversion for Family of Three, Amsterdam 60sqm/645sqft\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL Australian Apartment Style Tiny Home -27sqm/291sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Metal Clad Sydney Micro Studio Apartment - 30sqm/322sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL New Zealand Backyard Flexible Micro Loft - 36sqm/387sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Experimental Small Sydney Apartment - 24sqm/258sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Italian Micro Loft Apartment - 37sqm/398sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL 40sqm/430sqft Tiny Cabin - The Pod\n\n\n  \n\n\n\n\nNEVER TOO SMALL Kyiv Compact Designer Apartment Ukraine - 32sqm/344sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs Stylish Small Apartment Renovation Italy - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Cinema Inspired Small Apartment, Madrid - 33sqm/355sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs Modernist Apartment Restoration, Melbourne 58sqm/624sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs 19th Century Apartment Restoration, Barcelona - 60sqm/645sqft\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL: Paris Architect‚Äôs 70s Luxe Apartment, 53sqm/570sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Paris Architect‚Äôs Micro Apartment - 31sqm/344sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL - 29m2/318sqf - Boho Japanese Micro London Apartment\n\n\n  \n\n\n\n\nNEVER TOO SMALL 1970‚Äôs Melbourne Tiny Apartment - 35sqm/370sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Tasmanian bed-Sit Micro Apartment - 26.5sqm/285sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Central Paris Revolution Era Small Apartment - 42sqm/450sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Mid-Century Retro Studio Apartment Sydney 26sqm/280sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Seoul Small City Sanctuary Apartment 45sqm/484sqf\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Furniture Dealer‚Äôs Mid-Century Attic Apartment, Barcelona - 34sqm/366sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Interior Architect‚Äôs DIY Studio Apartment, Antwerp - 51sqm/548sqft\n\n\n  \n\n\n\n\nNTS Renters: Washington DC DIYers Eclectic Studio Apartment: 45sqm/490sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Japanese Architect‚Äôs Industrial Style Studio, Tokyo 48sqm/516sqf\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL Modular Milanese Micro Apartment - 34sqm/365sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Sustainable Small Apartment - 51sqm/548sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Sydney Minimalist Small Apartment - 46sqm/495sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Scandi Style Paris Small Apartment - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs Live/Work Home Design Singapore 60sqm/645sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Berlin Designer Open Plan Micro Apartment - 36sqm/388sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Flexible Minimalist Micro Apartment, Sydney - 27sqm/291sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: DIY Plant-filled Waterfront Loft, Amsterdam - 45sqm/484sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: 1970‚Äôs Timber Accented Small Apartment Renovation, Melbourne - 42sqm/452sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Self Taught Interior Designer‚Äôs Apartment, Hong Kong - 48sqm/516sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Bespoke Airy Paris Small Apartment, 47sqm/505sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Bright, Minimalist Mezzanine Apartment, Taiwan 70sqm/753sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Garage to Beach Studio Transformation, Australia 40sqm/431sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL - Japanese Artist‚Äôs Unique Open Air Family Home, Tokyo 57sqm/613sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Flexible Family Apartment, Singapore 58sqm/624sqft\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL Sydney Art Studio + Small Apartment - 49sqm/419sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Waterside Italian Two Bedroom Tiny Apartment - 35sqm/376sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Hong Kong City Escape Small Apartment -52sqm/559sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Darlinghurst Heritage Small Apartment - 27sqm/ 290sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL London Townhouse Conversion Small Apartment -40sqm/430sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Modular Antwerp Micro Apartment - 25sqm/269sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL, re-imagining small spaces - channel trailer\n\n\n  \n\n\n\n\nNEVER TOO SMALL Tasmanian Heritage Small Home - 44sqm/473sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Tiny Mediterranean Villa With Pool - 40sqm/430sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Vibrant Small Athens Apartment 50sqm/538sqf\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Singapore Graphic Designer‚Äôs Maximalist Apartment - 59sqm/635sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Scandi Style Small Apartment - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architects‚Äô Tiny Loft Amsterdam 49sqm/527sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Diamond Shaped Apartment With Hidden Storage - Hong Kong 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Simple and Stylish Singapore Apartment, 55sqm/592sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Flexible Japanese Inspired Apartment, Thailand 33sqm/355sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Japanese Inspired Tranquil Apartment, Taiwan 33sqm/355sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Ceramicist‚Äôs Vintage Furniture Apartment, Warsaw 33sqm/355sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Amsterdam Couple‚Äôs Luxe DIY Apartment, 48sqm/516 sqft Amsterdam\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Japanese Inspired Sydney Terrace House, 47sqm/506sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Art Collector‚Äôs Luxe Melbourne Apartment, 55sqm/592sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Storage Packed Micro Apartment, Hong Kong 24sqm/258sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: 3 in 1 Artist‚Äôs Home, Studio and Gallery, Lisbon 50sqm/538sqft\n\n\n  \n\n\n\n\n\n\n\n\n\n\n\n\n\nNEVER TOO SMALL: Clever Spaces in an Illustrator‚Äôs Vibrant Apartment, Singapore 47sqm/506sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Vibrant Retro Inspired Small Apartment - Madrid 47sqm/506sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Amsterdam Pod Small Loft Apartment - 45sqm/484sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Flexible Small Seaside Apartment - 49sqm/527sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Hotel Small Apartment Conversion - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL 24sqm/258sqft Micro Apartment - Boneca\n\n\n  \n\n\n\n\nNEVER TOO SMALL Small Family Hong Kong Apartment - 40sqm/430sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Toolbox Micro Apartment - 24sqm/258sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Sci-Fi Apartment Hong Kong - 59sqm/635sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Hong Kong Walk-Up Tiny Apartment - 25sqm/269sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL 2 x Sydney Loft House Small Homes - 35sqm/375sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Small Hybrid Loft Pod Apartment - 40sqm/430sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Art Deco Compartmented Sydney Micro apartment - 22sqm/236sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL London Houseboat Home - 40sqm/430sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Community First Small Apartment - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Barbican Brutalist Small Apartment - 41sqm/441sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Southeast Small Apartment - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Heritage Art Deco Sydney Apartment - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Flexible Milanese Micro Apartment - 30sqm/340sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Sydney Small Basement Apartment - 49sqm/527sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Italian Transforming Tiny Studio Apartment - 44sqm/473sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Buenos Aires Multi-purpose Small Apartment - 42sqm/452sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Melbourne Flexible Small Apartment - 55sqm/592sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL 40sqm/431sqft Small Apartment Design - Lycabettus Hill Studio\n\n\n  \n\n\n\n\nNEVER TOO SMALL Argentinian Small Mezzanine Apartment - 44sqm/474sqft\n\n\n  \n\n\n\n\nHow We Live Small - SMALL FOOTPRINT Introduction & Trailer\n\n\n  \n\n\n\n\nReimagining Small Living - SMALL FOOTPRINT - Ep 2\n\n\n  \n\n\n\n\nBuilding Urban Villages - SMALL FOOTPRINT - Ep 3\n\n\n  \n\n\n\n\nRethinking Affordability - SMALL FOOTPRINT - Ep 4\n\n\n  \n\n\n\n\nGreening our Cities - SMALL FOOTPRINT - Ep 5\n\n\n  \n\n\n\n\nBuilding Happier Cities - SMALL FOOTPRINT - Ep 6\n\n\n  \n\n\n\n\nNEVER TOO SMALL - Retro Chic Athens Small Apartment 48sqm/516sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL - Our NTS Space Studio\n\n\n  \n\n\n\n\nNEVER TOO SMALL New York Designer Studio Apartment 33sqm/350sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Architects Paris Small Family Apartment - 54sqm/581sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL Shaker Style Compact Apartment Auckland - 27sqm/291sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs Blue Mountains Cabin, Sydney 28sqm/310sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Artist‚Äôs Tiny Beach Shack Australia - 30sqm/323sqf\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Adaptable Small Apartment for Family of Five Paris - 50sqm/538sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs Colourful 50‚Äôs Small Apartment Italy 39sqm/429sqft\n\n\n  \n\n\n\n\nNTS Renters: Japanese YouTuber‚Äôs DIY Rental, Tokyo 58sqm/624sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Easy, Breezy, Adaptable Small Apartment - Poland 32sqm/344sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect‚Äôs DIY Beachside Apartment: Sydney 51sqm/549sqf\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Film Director‚Äôs Tiny Urban Oasis, Manila - 35sqm/376sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Musicians‚Äô Pet Friendly Apartment, Buenos Aires - 48sqm/516sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Designer Couple‚Äôs Simple Luxe Apartment, Amsterdam 33sqm/355sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architect Couple‚Äôs Small Heritage Apartment, France 57sqm/613sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Transforming Multifunctional Brazilian Apartment , 56sqm/603sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Family of 5‚Äôs Multifunctional Micro Apartment, Manila 28 sqm/301sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Tetris Shaped French Modernist Apartment, Paris 43m2/463sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Colourful, Open Plan Apartment, Seville 58qm/624sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Colourful Art Deco Micro Apartment, Sydney 27sqm/290sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Elegant, Heritage Apartment Renovation, Berlin 46sqm/495sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Architects‚Äô Flexible Three Balcony 1860‚Äôs Apartment, Madrid 55sqm/592sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: DIY YouTuber‚Äôs Handcrafted Apartment, Portugal 31sqm/333sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Movie Director‚Äôs Micro Loft Apartment, Philippines 24sqm/258sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Wes Anderson Inspired Attic Apartment, Madrid 42sqm/452sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Couple‚Äôs Japanese Inspired Loft, Singapore 41sqm/441sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Micro Apartment With Multifunctional Mirrored Pod, Melbourne 27sqm/291sqft\n\n\n  \n\n\n\n\nNEVER TOO SMALL: Interior Designer‚Äôs Custom Built Plywood Apartment, Paris 32sqm/344sqft\n\n\n  \n\n\n\n\nNTS: Melbourne Architect‚Äôs Smart Space-Saving Hacks in Small Apartment"
  },
  {
    "objectID": "posts/2024-11-19-grey-couches/grey-couches.html#conclusion",
    "href": "posts/2024-11-19-grey-couches/grey-couches.html#conclusion",
    "title": "NeverTooSmall Couch Colours",
    "section": "Conclusion",
    "text": "Conclusion\nIn conclusion, Kathy was right: grey dominates the couch color spectrum on NeverTooSmall.\nI hope you enjoyed following along on this journey to answer a casual question with computer vision. If you‚Äôd like to try it yourself, feel free to check out the GitHub repo here and let me know what you find!"
  },
  {
    "objectID": "posts/2024-11-19-grey-couches/grey-couches.html#footnotes",
    "href": "posts/2024-11-19-grey-couches/grey-couches.html#footnotes",
    "title": "NeverTooSmall Couch Colours",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nJoseph Redmon‚Äôs CV is worth a peak if you‚Äôre needing some inspiration for your own.‚Ü©Ô∏é\n\n\n\n\n\n\nThe offending image\n\n\n\n‚Ü©Ô∏é"
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "Academic Data",
    "section": "",
    "text": "I have done a fair bit of web scraping to get data into usable formats.\n\n\n\n\n\n  \n    \n      Available Data\n    \n    \n      From my GitHub repos\n    \n  \n  \n    \n      Title and link\n      Description\n    \n  \n  \n    \nCar Scraper ZA\n      \nAn automated scraper that gets adverts for vehicles off of Gumtree in South Africa every day and stores them on this GitHub repo. Have a look at my Shiny App to understand the kind of data collected.\n\n    \nBicycle Advert Scraper\n      \nSimilar to the car scraper, this repo autmotically scrapes adverts for bicycles from Bikehub and Gumtree.\n\n    \nSwedish Job adverts\n      \nThis repo contains a one percent sample of the enriched jobtech data from jobtechdev.se. They provide a one percent sample of all jobs advertised in Sweden from 2016 to 2022Q2 to help you get to grips with the data. I simply put it into excel format from json\n\n    \nSwedish Agriculture and Livestock\n      \nThis repo is for the digitization of the SCB reports on Agriculture and Animal Husbandry. They span from 1865 to 1911. You can have a look at my Shiny app to get a better understanding\n\n    \nSwedish patent data\n      \nI have scraped Google Patents for 10,000 patents that were registered in Sweden, and collected PDF data from The Swedish Patent Registration Authority for further analysis.\n\n    \nHot Jobs in Sweden from LinkedIn\n      \nThis repo has a set of data from LinkedIn's collaboration with the World Bank on talent migration. I've written a short report on the data here"
  },
  {
    "objectID": "content/data.html#available-data",
    "href": "content/data.html#available-data",
    "title": "Academic Data",
    "section": "",
    "text": "I have done a fair bit of web scraping to get data into usable formats.\n\n\n\n\n\n  \n    \n      Available Data\n    \n    \n      From my GitHub repos\n    \n  \n  \n    \n      Title and link\n      Description\n    \n  \n  \n    \nCar Scraper ZA\n      \nAn automated scraper that gets adverts for vehicles off of Gumtree in South Africa every day and stores them on this GitHub repo. Have a look at my Shiny App to understand the kind of data collected.\n\n    \nBicycle Advert Scraper\n      \nSimilar to the car scraper, this repo autmotically scrapes adverts for bicycles from Bikehub and Gumtree.\n\n    \nSwedish Job adverts\n      \nThis repo contains a one percent sample of the enriched jobtech data from jobtechdev.se. They provide a one percent sample of all jobs advertised in Sweden from 2016 to 2022Q2 to help you get to grips with the data. I simply put it into excel format from json\n\n    \nSwedish Agriculture and Livestock\n      \nThis repo is for the digitization of the SCB reports on Agriculture and Animal Husbandry. They span from 1865 to 1911. You can have a look at my Shiny app to get a better understanding\n\n    \nSwedish patent data\n      \nI have scraped Google Patents for 10,000 patents that were registered in Sweden, and collected PDF data from The Swedish Patent Registration Authority for further analysis.\n\n    \nHot Jobs in Sweden from LinkedIn\n      \nThis repo has a set of data from LinkedIn's collaboration with the World Bank on talent migration. I've written a short report on the data here"
  },
  {
    "objectID": "content/data.html#academic-data-processing",
    "href": "content/data.html#academic-data-processing",
    "title": "Academic Data",
    "section": "Academic data processing",
    "text": "Academic data processing\nI have been happy to help a number of my colleagues with scraping data from public websites or processing geographic data. I link these GitHub repositories here, in case you‚Äôre looking for inspiration on how to structure a scraping project, or perhaps want to access data on soil suitability or Europe‚Äôs urban populations.\n\nScraping a genealogy wiki of Sweden‚Äôs noble families\nHere I scraped and structured a database of more than 120,000 individuals belonging to various branches of Sweden‚Äôs nobility.\n\n\n\n\n\n\n\n\n\n\n\nScraping tax registers from the Stockholm City Archive\nHere I scraped an index of records from 1800 to 1880 for residents of Sweden‚Äôs most populous city, Stockholm.\n\n\n\n\n\n\n\n\n\n\n\nSoil suitability calculation\nIn this project, I helped acquire data on soil suitability for wheat cultivation across Europe, and aggregate the raster spatial data to NUTS 2 and NUT 3 regions for use as control variables in a study of persistence.\n\n\n\n\n\n\n\n\n\n\n\nEuropean population aggregations\nIn this project, I helped aggregate the population from cities across Europe to NUTS2 and NUTS3 regions, to show the growth of urban populations across time."
  },
  {
    "objectID": "content/teaching.html",
    "href": "content/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Check out my site for a set of Stata labs I taught in the Spring of 2023"
  },
  {
    "objectID": "content/teaching.html#regional-development-and-growth",
    "href": "content/teaching.html#regional-development-and-growth",
    "title": "Teaching",
    "section": "",
    "text": "Check out my site for a set of Stata labs I taught in the Spring of 2023"
  },
  {
    "objectID": "content/teaching.html#screencasts",
    "href": "content/teaching.html#screencasts",
    "title": "Teaching",
    "section": "Screencasts",
    "text": "Screencasts\nHave a look at a set of screencasts I am working on for learning R."
  },
  {
    "objectID": "content/teaching.html#data-visualization",
    "href": "content/teaching.html#data-visualization",
    "title": "Teaching",
    "section": "Data visualization",
    "text": "Data visualization\nI am working on adapting a course from Claus Wilke for presentation in Lund."
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html",
    "href": "posts/2025-03-28-hisco/hisco.html",
    "title": "HISCO lookup",
    "section": "",
    "text": "This is a post for me to come back to as a refrence. I always need to look up HISCO groups, and I have scraped and stored this information in this github repo but it would be nice to have it in a table to qukickly look up."
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html#purpose",
    "href": "posts/2025-03-28-hisco/hisco.html#purpose",
    "title": "HISCO lookup",
    "section": "",
    "text": "This is a post for me to come back to as a refrence. I always need to look up HISCO groups, and I have scraped and stored this information in this github repo but it would be nice to have it in a table to qukickly look up."
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html#overview-of-hisco",
    "href": "posts/2025-03-28-hisco/hisco.html#overview-of-hisco",
    "title": "HISCO lookup",
    "section": "Overview of HISCO",
    "text": "Overview of HISCO\nThe Historical International Standard Classification of Occupations (HISCO) is a standardized system developed to classify and compare historical occupations across different countries and time periods. It is based on the International Standard Classification of Occupations (ISCO) from 1968 and has been adapted for historical research purposes. HISCO provides a common framework that enables researchers to analyze social structures, economic development, and labor market changes over time."
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html#structure-of-hisco",
    "href": "posts/2025-03-28-hisco/hisco.html#structure-of-hisco",
    "title": "HISCO lookup",
    "section": "Structure of HISCO",
    "text": "Structure of HISCO\nHISCO employs a hierarchical coding system consisting of five digits:ÓàÜ\n\nMajor Groups: The first digit represents broad occupational categories.ÓàÜ\nMinor Groups: The second digit specifies subgroups within the major group.ÓàÜ\nUnit Groups: The third digit further refines the occupation category.ÓàÜ\nHeadings: The fourth and fifth digits provide detailed occupational titles.ÓàÜ\n\nThis structure allows for detailed classification and analysis of occupations.\n\nHISCO Major Groups\n\n\n\n\n\n\n\n\nHISCO Major Groups\n\n\nA table of the HISCO major groups\n\n\nHISCO Code\nHISCO Title\nDescription\n\n\n\n\n0\n0/1 Professional, technical and related workers\nWorkers in this major group conduct research and apply scientific knowledge to the solution of a variety of technological, economic, social and industrial problems and perform other professional, technical, artistic and related functions in such fields as the physical and natural sciences, engineering, law, medicine, religion, education, literature, art, entertainment and sport.\n\n\n2\n2 Administrative and managerial workers\nWorkers in this major group conduct research and apply scientific knowledge to the solution of a variety of technological, economic, social and industrial problems and perform other professional, technical, artistic and related functions in such fields as the physical and natural sciences, engineering, law, medicine, religion, education, literature, art, entertainment and sport.\n\n\n3\n3 Clerical and related workers\nWorkers in this major group put into effect laws, rules and regulations made by central, state, provincial or local governments; supervise clerical and related work, transport and communications service operations; compile and maintain records of financial and other business transactions; handle cash on behalf of an organisation and its customers; record oral or written matter by shorthand writing, typing and other means; operate office machines and telephone and telegraph equipment; conduct passenger transport vehicles; take part in postal work and mail distribution and perform other duties related to the foregoing.\n\n\n4\n4 Sales workers\nWorkers in this major group are engaged in, or directly associated with, buying and selling goods and services of all kinds and in conducting wholesale and retail businesses on their own behalf.\n\n\n5\n5 Service workers\nWorkers in this major group organise or perform catering, housekeeping, personal, protective and related services.\n\n\n6\n6 Agricultural, animal husbandry and forestry workers, fishermen and hunters\nWorkers in this major group conduct farms on their own behalf or in partnership, perform agricultural, animal husbandry and forestry tasks, catch fish, hunt and trap animals, and perform related tasks.\n\n\n7\n7-8-9 Production and related workers, transport equipment operators and labourers\nWorkers in this major group are engaged in or directly associated with the extraction of minerals, petroleum and natural gas from the earth and their treatment; manufacturing processes; the construction, maintenance and repair of various types of roads, structures, machines and other products. Also included are those who handle materials, operate transport and other equipment and perform labouring tasks requiring primarily physical effort.\n\n\n\n\n\n\n\n\n\nHISCO Three Digit Occupations"
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html#related-classification-systems",
    "href": "posts/2025-03-28-hisco/hisco.html#related-classification-systems",
    "title": "HISCO lookup",
    "section": "Related Classification Systems",
    "text": "Related Classification Systems\nHISCO serves as a foundation for other classification systems that analyze historical social stratification:ÓàÜ\n\nHISCLASS: An occupational stratification scale that classifies occupations into social classes based on HISCO codes, facilitating international comparisons across different periods, countries, and languages.\nHISCAM: A social status scale derived from HISCO, developed to analyze social stratification and inequality in historical contexts. These systems provide additional tools for researchers studying historical social structures and mobility.ÓàÜ"
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html#resources-and-tools",
    "href": "posts/2025-03-28-hisco/hisco.html#resources-and-tools",
    "title": "HISCO lookup",
    "section": "Resources and Tools",
    "text": "Resources and Tools\nFor further exploration and practical application of HISCO, consider the following resources:ÓàÜ\n\nHISCO Database: This database offers access to occupational titles and their corresponding HISCO codes, as well as CSV downloads.\nR Package for HISCO Classification: An R package developed for classifying HISCO codes into social class systems such as HISCLASS, HISCLASS 5, SOCPO, and Universal HISCAM. This tool aids researchers in analyzing historical occupational data.\nMachine Learning Based Classifier This GitHub repo hosts ‚Äúa tool designed to transform occupational descriptions into standardized HISCO (Historical International Standard Classification of Occupations) codes automatically. Developed by Christian M√∏ller Dahl, Torben Johansen and Christian Vedel from the University of Southern Denmark, this tool leverages the power of a finetuned language model to process and classify occupational descriptions with high accuracy, precision, and recall.‚Äù"
  },
  {
    "objectID": "posts/2025-03-28-hisco/hisco.html#conclusion",
    "href": "posts/2025-03-28-hisco/hisco.html#conclusion",
    "title": "HISCO lookup",
    "section": "Conclusion",
    "text": "Conclusion\nThis post provides a structured overview of HISCO, its major groups, related classification systems, and available resources. By consolidating this information, it serves as a convenient reference for quickly accessing and understanding historical occupational classifications.ÓàÜ"
  },
  {
    "objectID": "content/research/Wired-for-success.html",
    "href": "content/research/Wired-for-success.html",
    "title": "Wired for Success: Sweden‚Äôs Electrical Innovators in the Age of Industrial Transformation",
    "section": "",
    "text": "I am working on a paper about the individuals who helped usher in an age of great technical progress and economic levelling in Sweden, by making use of new technology associated with electrification.\nIn the first half of the 20th century, electricity went from a luxury to an essential input into the production process. This new technology was used to power machines, light homes, and heat buildings. It was also used to power the new communications technology of the telephone and radio.\nThis new technology raised the marginal product of labour, and put workers in a better bargaining position to argue for higher wages and better working conditions. It resulted in higher incomes, a more equal income distribution, and the creation of new kinds of jobs in areas that electrified first.\nWhat I seek to do in this paper is trace the career paths of innovators and early adopters of electrification technology in Sweden. I want to know who they were, where they came from, and what they did. I want to know how they were connected to each other, and how they were connected to the rest of the world.\nTo accomplish this, I make use of a fun data source, an archive of biographical information from a series of books called Vem √§r vem? (Who is who?). These books were published in two editions in Sweden between 1945-1950 and 1962-1968, and contain biographical information about prominent Swedes. Eight of the ten volumes have been digitized and are available on the Projekt Runeberg website.\nI have scraped the biographical information from these books, and used it to create a database of biographical information about nearly 70,000 individuals. I plan to use this database to create a network of individuals, and to create a network of the places they were born, lived, travelled and worked. I will also use the database to create a network of the companies they worked for, and the industries they worked in.\nIt is still a work in progress, but you can see some slides about the project which I was glad to present at the CBS Strategy and Innovation Graduate Seminar in May 2023."
  },
  {
    "objectID": "content/phd-planning.html",
    "href": "content/phd-planning.html",
    "title": "PhD planning",
    "section": "",
    "text": "I need to commit to more structure and deadlines in order to complete my PhD on time.\nMy PhD consists of four papers and coursework."
  },
  {
    "objectID": "content/phd-planning.html#papers",
    "href": "content/phd-planning.html#papers",
    "title": "PhD planning",
    "section": "Papers",
    "text": "Papers\n\nShocked by Electricity: The impact of new technology on individuals and local labour markets. Co authored with Kerstin Enflo and Jakob Molinder\nWired for Success: Sweden‚Äôs Electrical Innovators in the Age of Industrial Transformation\n\nHere is a chart showing the remaining time in my PhD and what is left to do.\n\n\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    title       Paper Timeline\n    excludes    weekends\n\n    section Paper Drafts\n    Complete draft of first paper          :active, 2023-09-11, 19d\n    Complete draft of second paper         :        2023-09-30, 2023-12-16\n    Complete draft of third paper          :        2024-01-17, 2024-04-15\n    Complete draft of fourth paper         :        2024-08-16, 2024-12-16\n    \n    section Paper Revisions\n    Revise second paper                    :        2024-04-16, 2024-05-16\n    Revise third paper                     :        2024-05-17, 2024-06-15    \n    Revise fourth paper                    :        2025-01-24, 2025-03-15    \n\n\n    section Seminars & Submission\n    Midway seminar                         :crit,     2024-01-24, 1d\n    Final seminar                          :crit,     2025-04-01, 1d\n    Intense working on the Kappa           :          2025-05-01, 2025-06-30\n    Submit Ph.D. for typesetting           :milestone,2025-07-01, 0d\n\n    section Defense\n    Defend thesis                          :milestone, 2025-10-01, 0d\n\n\n\n\n\n\n\n\n\nUpcoming deadlines and feedback\nPresent Wired for Success data processing and initial results at Swedish Economic History Meeting on 29th of September.\nComplete draft of Shocked by Electricity on 2 October 2023."
  },
  {
    "objectID": "content/phd-planning.html#coursework",
    "href": "content/phd-planning.html#coursework",
    "title": "PhD planning",
    "section": "Coursework",
    "text": "Coursework\nMandatory courses (52.5 credit points):\n\nEHEH001 Designing a PhD dissertation: Completed - 7.5 credits.\nEHEH002 Foundations of Economic History: 1/3 papers done . Assuming linear progression and that each paper is worth an equal amount of credits, I‚Äôve completed 1/3 of 15 credits = 5 credits.\nEEH002F research seminars: Ongoing . 0 credits for the purpose of this calculation.\nPhilosophy of Science: Not chosen yet - 0 credits.\nResearch Ethics: Not chosen yet - 0 credits.\nQuantitative course (SDU Summer School): Complete - 7.5 credits.\nQualitative Course (Archives, Primary Sources, Dataset - EEH033F): Complete - 4.5 credits.\n\nTotal for Mandatory : 7.5 + 5 + 0 + 0 + 0 + 7.5 + 4.5 = 24.5 credits\nOptional courses (22.5 credit points):\n1. EEH005F Economic Growth over Time and Space: Completed - 7.5 credits.\nTotal for Optional : 7.5 credits\nIn summary: - I‚Äôve completed 24.5 credits of the mandatory coursework. - I‚Äôve completed 7.5 credits of the optional coursework.\nThis means I‚Äôve completed a total of 32 credits out of the 75 credits I need."
  },
  {
    "objectID": "content/phd-planning.html#conferences-and-presentations",
    "href": "content/phd-planning.html#conferences-and-presentations",
    "title": "PhD planning",
    "section": "Conferences and presentations",
    "text": "Conferences and presentations\nEHS Conference 2023: Warwick\nEHES Conference 2023: Vienna\nSEHM 2023: Lund"
  },
  {
    "objectID": "posts/2025-02-27-react-production-functions/react-production-functions.html",
    "href": "posts/2025-02-27-react-production-functions/react-production-functions.html",
    "title": "Claude 3.7: React and Reactions",
    "section": "",
    "text": "One of the best parts of my work as a PhD student is teaching. In a course on regional development in Lund, we cover data visualization and map-making. Both of these are an art as much as they are a science, and I love creating little interactive tools to help students test out how changing small things can impact how we understand a map or chart.\nIn another course, a few years ago I built a Cobb-Douglas production function visualizer using Shiny from Rstudio. This app lets students adjust key parameters and immediately see how output changes. The production function itself is written as:\n\\[\nY = A \\times K^{\\alpha} \\times N^{1-\\alpha}\n\\]\nHere, A stands for total factor productivity, K represents capital, N is labor, and Œ± is the output elasticity of capital. In essence, the model demonstrates how output responds to variations in inputs, clearly showing the concept of diminishing returns.\nI shared the Shiny app online so that students could experiment with the function firsthand. You can check it out here, and if you‚Äôre really interested, look at my poor quality Shiny code from four years ago on github. The app pairs a brief explanation of the function with sliders for all four parameters and a chart that updates in real time.\nThis week, Claude 3.7 was released by Anthropic. It looks to be a big improvement over the previous 3.5 version in regard to coding. There has been a lot of speculation online that coders are cooked as a result of the new Claude Code command line tool that can understand a repository, create new features, write its own tests, and deploy apps.\nIn the desktop app, Claude began creating React1 artifacts as my colleague Jonas used it this week too. It‚Äôs somewhat scary to see the pace at which LLMs are improving at writing code.\nAs part of testing the 3.7 model, I asked Claude to recreate my production functions app from a simple prompt:\nClaude created exactly what I asked for in one prompt. You can play with it in your browser here. Another thing that is amazing is that it is a one click deployment so that you can access it too - sure it‚Äôs just a React app, but it‚Äôs still pretty cool that it just works out of the box.\nWhile it is not as polished as my Shiny app, it is a great starting point, and it would be easy to add more features to it. If you‚Äôre interested in the code, that Claude created, I‚Äôve included it at the end of this post."
  },
  {
    "objectID": "posts/2025-02-27-react-production-functions/react-production-functions.html#footnotes",
    "href": "posts/2025-02-27-react-production-functions/react-production-functions.html#footnotes",
    "title": "Claude 3.7: React and Reactions",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nA javascript framework for building user interfaces‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html",
    "href": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html",
    "title": "DRAFT: Analysis for a Solar PV & Battery System (South Africa)",
    "section": "",
    "text": "I asked GPT-4.5 with deep research to investigate a solar system for me, in order to test its capabilities, inspired by a post by Timothy B. Lee titled in a somewhat clickbait manner, These experts were stunned by OpenAI Deep Research. I was interested to read that the experts Timothy consulted were impressed with the domain specific knowledge shown by the model, searching widely across the web for information to answer the expert‚Äôs questions ranging from law to coding, to construction.\nMuch like my post on public transport between airports and city centers, as each new iteration of LLM is announced, I have asked the models to investigate quite a complex domestic engineering problem; specifying a solar system for a house in South Africa, and creating a mechanism to calculate the payback time of the system.\nThis post details the output that I received from GPT 4.5 Deep Research, and the analysis of the solar system that it proposed.\nI would really love some feedback on the Streamlit application that it built, so please do comment on the LinkedIn post (when it is up)."
  },
  {
    "objectID": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html#prompt",
    "href": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html#prompt",
    "title": "DRAFT: Analysis for a Solar PV & Battery System (South Africa)",
    "section": "Prompt",
    "text": "Prompt\nFollowing Tobias Zwingmann‚Äôs guide to prompting Large Language Models, I provided the following prompt to GPT-4.5:\n\nRole: you‚Äôre an expert on domestic solar energy systems.\nGoal: I want you to conduct some research for me, and write an engineering report. I am trying to decide on a solar panel and battery system to install in my freestanding house in South Africa.\nTasks: There are a few considerations;\n\nI need to understand the size of the system I need to run everything electric in my house, except water heating and air conditioning,\nI need to understand what the lifespan of the system will be,\nUsing this, I need to understand how long the payback period would be, considering the cost of installing the system, the savings on electricity while the system is in use, and the opportunity cost of that money invested in a high-yield savings account.\n\nDetails: Say that I live in a house that has a roof conducive to installing panels, that I pay a per kilowatt rate for my electricity from the grid and that I cannot sell back to the grid at the moment."
  },
  {
    "objectID": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html#response",
    "href": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html#response",
    "title": "DRAFT: Analysis for a Solar PV & Battery System (South Africa)",
    "section": "Response",
    "text": "Response\nHere is the response I received:\n\nIntroduction\nThis report presents an engineering analysis for a solar panel and battery system to power a freestanding house in South Africa. The homeowner‚Äôs goal is to supply 30 kWh of electricity per day and have 48 hours (2 days) of backup power for outages. The system will be grid-connected (for reliability) but excess solar energy cannot be fed back into the grid. All components are selected for efficiency and cost-effectiveness within a budget of R200,000. We outline the system sizing (solar panels, battery bank, inverter), component lifespans, and a financial analysis including payback period, electricity bill savings, and the opportunity cost of capital (at 2% above inflation).\n\n\nLoad Requirements and Backup Capacity\nDaily Energy Usage: The average daily consumption is 30 kWh, which is in line with Eskom‚Äôs estimate of ~30 kWh for a typical household (Load-Shedding Solutions | What Size Backup Battery Do I Need? - REVOV Battery Backup Power Solutions). This includes all household loads (lighting, appliances, etc.). To be fully self-sufficient on solar, the system must generate roughly this amount each day on average.\nBackup for 48 Hours: To power the home for 2 full days of autonomy (30 kWh/day √ó 2 days), approximately 60 kWh of usable energy storage is required if there were no solar input. In practice, during a daytime outage the solar panels will still produce power (if the sun is shining) to help supply loads and recharge the batteries. However, the battery bank must be sized to carry the house through two nights and periods of low sun. We will design for ~60 kWh usable storage, which means a slightly higher nominal battery capacity when accounting for depth-of-discharge limits (explained below).\nPower (kW) Requirements: In addition to energy (kWh), the system must handle the peak power draw of the home. A rough estimate for a typical family home with major appliances might be 5‚Äì8 kW peak load (higher if electric stoves or multiple air-cons run simultaneously). The inverter and panel array should be sized to comfortably supply this peak. We will target an inverter around 8‚Äì10 kVA to ensure the system can deliver sufficient power for typical household loads without bottleneck.\n\n\nSolar PV Array Sizing\nEnergy Production Goal: To produce ~30 kWh per day from solar, we estimate the required PV capacity based on local solar irradiance. South Africa has good sunshine; assuming an average of about 5 peak-sun-hours per day, a 1 kW solar array can produce roughly 5 kWh per day. Thus, to generate ~30 kWh/day, about 6 kW_p (kilowatts peak) of panels are needed (30 kWh √∑ 5 kWh/kW ‚âà 6 kW). In practice, we will oversize slightly to account for losses (inefficiencies, cloudy days, winter).\nRecommended PV Size: 8 kW_p solar array ‚Äì for example, ~20 panels of 400 W each. This size can produce on the order of 35‚Äì40 kWh on a good sunny day, providing a buffer to both run the house and charge the battery bank. (By comparison, a professionally designed off-grid system yielding ~42 kWh/day uses ~7.92 kW of panels (Off grid solar kit | 10kVA inverter 7.92kWp 32kWh lithium).) The 8 kW array ensures that even in winter or overcast conditions, there is a better chance to meet the 30 kWh load or at least partially recharge batteries. It also allows quick recharging of the 60 kWh battery after a prolonged outage, when the sun returns.\nPanel Type and Installation: We recommend high-efficiency monocrystalline PV panels (60 or 72-cell modules). These typically have 300‚Äì450 W output each; using ~72-cell (larger) panels reduces the number of modules needed (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). Panels should be mounted on a north-facing roof (in the southern hemisphere) at an optimal tilt to maximize year-round solar exposure (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). No specific brand is required; Tier-1 manufacturers (e.g.¬†JA Solar, Trina, Jinko, Canadian Solar, etc.) offer similar efficiency (~18‚Äì21%) and reliability, so the focus is on cost-effective per-watt pricing and good warranty terms.\nPerformance and Efficiency: The array will feed a MPPT (maximum power point tracking) charge controller or hybrid inverter, which manages the DC power from panels. Modern MPPT-enabled systems can achieve &gt;95% conversion efficiency of solar DC to battery storage or AC output. Some energy losses (maybe ~10-15%) are expected in cabling, controller, and inverter, which have been factored into the slight oversizing. The system should be designed such that during the day, solar directly powers the house and any surplus goes into charging the battery. Since export to the grid is not allowed, once the battery is full any further excess generation will be curtailed (essentially ‚Äúwasted‚Äù). Thus, 8 kW_p is a reasonable compromise: large enough to meet needs but not so large that huge excesses occur regularly. In summer months there may be some midday surplus when the battery is full and loads are low; in winter, the array size will be just sufficient to cover usage on many days.\n\n\nBattery Storage System\nRequired Capacity: For a full 48-hour backup of 30 kWh/day, we target about 60 kWh of usable storage. To achieve 60 kWh usable, the battery‚Äôs total capacity should be larger because it‚Äôs neither practical nor healthy for batteries to be fully discharged to 0%. Using a Lithium-ion (LiFePO‚ÇÑ) battery bank, we can safely discharge up to ~80‚Äì90% of the battery. Assuming ~80% Depth of Discharge (DoD) for longevity, the total installed battery capacity should be around:\n[ = ]\nThis ensures 60 kWh is available without pushing the battery to absolute empty. In an emergency outage, the system could go deeper if needed, but designing for 80% DoD gives a margin and extends battery life (Load-Shedding Solutions | What Size Backup Battery Do I Need? - REVOV Battery Backup Power Solutions) (LiFePO4 Battery Cycle Life & Durability).\nBattery Type: We recommend Lithium Iron Phosphate (LiFePO‚ÇÑ) batteries. LiFePO‚ÇÑ is widely used in solar applications for its long life and safety. Compared to traditional lead-acid batteries, lithium batteries offer: higher usable depth of discharge, more cycle life, no required maintenance (no topping up acid), no gas emissions, lighter weight, and better charge efficiency (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). They also can charge faster and don‚Äôt need to be fully recharged each cycle (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). While upfront cost per kWh is higher than lead-acid, the longevity and usable capacity make them more cost-effective over the system life. A typical Li-ion solar battery comes with a 10-year warranty and &gt;5,000 cycle lifespan (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions) (LiFePO4 Battery Cycle Life & Durability). For example, good quality LiFePO‚ÇÑ batteries can exceed 5,000 cycles at 80% DoD (LiFePO4 Battery Cycle Life & Durability), which equates to roughly 13‚Äì15 years of daily cycling. This far outlives lead-acid banks, which might only get a few hundred deep cycles before needing replacement.\nBattery Bank Configuration: ~75 kWh is a very large battery bank. In practice, this would be built by connecting multiple modular battery units in parallel. Many LiFePO‚ÇÑ solar batteries come in 5 kWh or 10 kWh modules (often 48V each). For instance, the system could use around five 15 kWh modules, or seven to eight 10 kWh modules, etc., connected to total ~75 kWh. This modular approach also allows some scalability ‚Äì the homeowner could start with a bit less capacity if needed for budget, then add modules later (since lithium systems are easy to scale as needed (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions)).\nCost-Effective Options: There are South African brands and products that provide large LiFePO‚ÇÑ batteries at competitive prices. For example, Freedom Won (a SA manufacturer) offers high-capacity LiFePO‚ÇÑ batteries (their ‚ÄúFreedom Lite‚Äù 48V units) ‚Äì a 32 kWh usable unit (40 kWh nominal) with 10-year warranty is one such model (Off grid solar kit | 10kVA inverter 7.92kWp 32kWh lithium). Pylontech (a popular brand) provides 3.5 kWh modules that can be stacked; though many would be needed for 60+ kWh, they are known for reliability. Second-life EV batteries are another cost-saving option: companies repurpose electric vehicle battery packs for home storage at lower cost per kWh (with somewhat reduced lifetime). Given the budget constraint, exploring a reputable second-life lithium battery supplier could significantly cut costs while still providing the required capacity.\nOperating Strategy: Normally, the battery will charge from solar during the day and discharge at night to supply the home. This achieves maximum self-consumption of solar energy (since no selling back). The system can be configured such that the battery prioritizes critical loads during outages, extending its effective backup time. For example, during a long outage, heavy non-essential loads (pool pumps, laundry heaters, etc.) can be turned off to conserve power, allowing essential loads to run for 48 hours or more. A 60 kWh usable battery could theoretically power all 30 kWh/day usage for 2 days, but prudent load management during outages will provide a buffer (e.g.¬†a 10 kWh battery can last 2‚Äì3 days for very basic needs (Load-Shedding Solutions | What Size Backup Battery Do I Need? - REVOV Battery Backup Power Solutions)). Our design ensures that in normal operation, the battery is cycled moderately (not 100% every day unless needed) to prolong its life.\n\n\nInverter and Balance of System Components\nHybrid Inverter/Charger: A hybrid inverter is required to tie everything together. This device inverts DC power from the panels/battery to AC for household use, and can also charge the batteries from either solar or grid when needed. A unit in the 8‚Äì10 kVA range is appropriate to handle the home‚Äôs peak loads. For example, a 10 kVA (8 kW) inverter was used in a reference off-grid system for ~42 kWh/day consumption (Off grid solar kit | 10kVA inverter 7.92kWp 32kWh lithium), which aligns well with our needs. Good brands include Victron (high-end), Sunsynk, GoodWe, SMA, and Deye among others ‚Äì many offer integrated solutions that combine an MPPT solar charge controller, battery management, and grid transfer switching in one device (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). This simplifies installation and provides seamless switchover to battery during outages (uninterruptible power supply function).\nThe inverter should output 230V AC, 50 Hz to match the South African grid. It also serves as the system‚Äôs brains: managing battery charge, preventing over-discharge, and throttling solar input if the battery is full and grid export is disallowed. Modern inverters operate at high efficiency (~95‚Äì98% conversion efficiency) and produce a pure sine-wave AC output. A quality inverter will have smart controls (often with a web/app interface to monitor performance and configure settings).\nCharge Controller: If the chosen inverter does not have built-in MPPT, a separate solar charge controller would be needed. This device ensures the panels charge the battery optimally, adjusting voltage/current and preventing overcharge (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). In our case, many hybrid inverters include MPPT functionality sized for the PV array, so we assume an all-in-one unit. The charge controller (standalone or inside inverter) will be sized for the array current ‚Äì for 8 kW PV, that might be ~2 x 100A MPPTs (if split into strings), or one larger MPPT that can handle ~150‚Äì170 A input at battery voltage.\nOther Equipment: The system also requires balance-of-system components: mounting racks for panels, wiring, combiner boxes and disconnects, fuses/breakers, and a battery management system (BMS) (the BMS is typically integrated with lithium batteries, ensuring cells are balanced and protecting from over/under-charge). Safety features include surge protection and earth leakage protection. Given the scale of the battery, a proper enclosure or battery room is needed (lithium batteries don‚Äôt emit gas, so no special ventilation is needed (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions), but temperature should be kept moderate for longevity).\nComponent Lifespans: The solar panels are very long-lived ‚Äì most carry 25-year performance warranties and can last 25‚Äì40 years (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). After 25 years a panel typically still produces ~80% of its original output. Lithium battery banks have a lifespan on the order of 10‚Äì15 years under daily cycling (with warranties ~10 years) (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). After that, the capacity might drop to ~70-80% of original, at which point the homeowner might replace or augment the battery. The inverter is often the limiting component ‚Äì good inverters last about 10 to 15 years (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions), so it may require one replacement over a 25-year project span. Smaller components (charge controllers, etc.) should also last 10+ years if they are high quality. Regular maintenance is minimal for this setup: cleaning panels occasionally, and possibly replacing cooling fans or checking connections on the inverter. The lack of moving parts in panels and the maintenance-free nature of LiFePO‚ÇÑ batteries keep ongoing upkeep low.\n\n\nFinancial Analysis\nInvesting in this solar-plus-battery system has significant upfront costs but yields long-term savings on electricity bills and protection from load-shedding. Below we break down the financial outlook:\nUpfront Cost Estimate (R): The budget is R200,000 (approximately $11,000). This needs to cover equipment and installation. Based on current market prices in South Africa, this budget is realistic for a 25‚Äì30 kWh/day off-grid-capable system (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions), though it requires careful component choices. For context, a typical 25 kWh/day off-grid setup with batteries costs around R200k‚ÄìR250k installed (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions). Our system is slightly larger (30 kWh/day and more storage), so cost optimization is key (e.g., choosing a slightly smaller initial battery or competitive suppliers). A possible allocation of the budget could be:\n\n~R80,000 for 8 kW of solar panels (at ~R10/W including mounting hardware),\n\n~R100,000 for ~60 kWh of battery storage (this assumes ~R1,600 per kWh, which might involve second-life batteries or a good bulk deal),\n\n~R20,000 for a hybrid inverter and balance-of-system (some inverter models in the 8kW range cost ~R15k‚ÄìR30k).\n\nThese are rough figures and prices vary, but it shows achieving the system near R200k is feasible. If premium brand batteries/inverter are chosen, costs could exceed the budget (and one might opt for ~40 kWh battery initially to stay under budget, accepting a bit less than 48h backup except for essential loads).\nElectricity Bill Savings: With solar supplying ~30 kWh each day, the homeowner will purchase far less energy from the grid. At the given grid tariff of R3.35 per kWh, 30 kWh/day corresponds to R100.5 per day avoided cost. Over a year, that is about R36,700 per year in savings (30√óR3.35√ó365). In practice, the savings could vary: if occasional bad weather forces some grid use, the savings might be slightly lower; conversely, if electricity prices inflate, the monetary savings increase. Importantly, these savings assume all solar energy is self-consumed (which our design facilitates by storing excess in the battery for night use). Since no excess can be sold, we sized the system such that generation roughly matches usage to maximize utilization. Any small surplus solar (when the battery is full on a sunny day) is ‚Äúlost‚Äù because it cannot be exported ‚Äì but this is a minor sacrifice for ensuring enough capacity for cloudy days.\nPayback Period (Simple): The simple payback time can be estimated as the initial cost divided by annual savings. Using R200,000 cost and ~R36,700/year savings, we get:\n\\[\\text{Payback (no interest)} = \\frac{R200,000}{R36,700/year} \\approx 5.5 \\text{ years}.\\]\nThis means in roughly five and a half years, the cumulative electricity cost savings equal the upfront investment. This is a very favorable payback ‚Äî notably, just a decade ago, solar systems had ~20-year paybacks, but with today‚Äôs high power costs and improved tech, payback times under 5‚Äì6 years are now achievable (Jozi home rental market is well set for an uptrend | Ennik Estates). In fact, reports indicate that a R200k solar-plus-battery system can pay for itself in ‚Äúless than five years‚Äù under current conditions (Jozi home rental market is well set for an uptrend | Ennik Estates). Our calculation is in the same ballpark, and if grid electricity prices continue to rise faster than inflation (as has been the trend), the payback could accelerate further.\nConsidering Opportunity Cost: We also account for the opportunity cost of capital ‚Äì the idea that the R200k could have been invested elsewhere. The homeowner expects a return of inflation + 2% if that money were invested. In real terms (assuming inflation-adjusted), that‚Äôs about a 2% real annual return. Factoring this in effectively means the solar investment should be compared to an alternative where the R200k earns ~R4,000 per year in real interest. When we account for this, the net benefit of the solar system each year is the electricity savings minus the forgone interest: approximately R36,700 ‚Äì R4,000 = R32,700 per year. Using this net savings for payback gives:\n\\[\\text{Payback (with 2\\% cost of capital)} \\approx \\frac{R200,000}{R32,700/year} \\approx 6.1 \\text{ years}.\\]\nSo, including opportunity cost, the payback is about 6 years. In other words, in 6 years the electricity savings not only recoup the R200k, but also compensate for the lost investment growth. Another way to see it: the internal rate of return (IRR) of the solar project is much higher than 2% ‚Äì in fact, roughly on the order of 15‚Äì20% ROI when considering the annual savings vs cost, which far outstrips a modest 2% investment gain. Thus, even with conservative financial considerations, the solar system is a sound investment.\nLong-Term Savings: After the payback period, the system will continue to save ~R36k+ per year (and likely more in nominal terms if utility rates rise). Over a 25-year lifespan of panels, the cumulative savings can be on the order of R1 million (not inflation-adjusted) if power tariffs increase. Of course, one must account for battery replacement costs around year 10‚Äì15 (which might be another R100k or so in future Rands, though future battery prices may drop). Even so, the net present value (NPV) remains strongly positive. Additionally, there are intangible benefits not monetized in the payback: the ability to keep the lights on and appliances running during Eskom load-shedding or outages. This resilience has value in avoiding lost productivity, spoiled food, or the need for alternative backup generators.\n\n\nConclusion\nRecommended Solution: Deploy an ~8 kW solar PV array, a lithium-ion battery bank ~60 kWh usable (75 kWh nominal), and an 8‚Äì10 kVA hybrid inverter. This configuration can supply the home‚Äôs 30 kWh daily needs and provide approximately 48 hours of autonomy during grid outages. The system components were chosen for efficiency and longevity: Tier-1 PV modules (25-year life), a LiFePO‚ÇÑ battery (10+ year life, &gt;5000 cycles (LiFePO4 Battery Cycle Life & Durability)), and a quality inverter (10-15 year life). The design avoids feeding power to the grid, instead storing excess solar in batteries for nighttime use, thereby maximizing self-consumption. At an estimated cost of ~R200k (installed), the system fits the budget by using cost-effective equipment and possibly leveraging modular expansion.\nPerformance: Under typical sun conditions, the solar panels will produce around 30‚Äì40 kWh per day (Off grid solar kit | 10kVA inverter 7.92kWp 32kWh lithium), meeting the household demand and charging the battery to prepare for night or any outages. During an outage, the system can run critical loads for 2 days comfortably, and with sunlight each day, essential usage can be maintained even beyond 48 hours (especially if non-critical loads are managed). This ensures the homeowner is largely shielded from load-shedding disruptions.\nFinancial Return: The solar investment is financially attractive. With a ~5‚Äì6 year payback period from electricity savings, it yields a strong return compared to the modest 2% real growth of money in a bank. The savings on electricity bills (on the order of R35k per year) effectively start paying back from day one, and once the system is paid off, the continued savings are like tax-free dividends. Given that current payback periods in South Africa for similar systems are reported to be well under 5 years (Jozi home rental market is well set for an uptrend | Ennik Estates) (What You Need for an Off-Grid Solar System in South Africa - REVOV Battery Backup Power Solutions), this project is likely to recover its cost well within the components‚Äô lifespan, even after accounting for battery replacement down the line.\nIn summary, the proposed solar PV and battery system is feasible within the R200k budget, meets the energy and backup requirements, and offers both economic and practical benefits. It will provide reliable power (with independence from grid outages) and long-term cost savings. The homeowner will enjoy peace of mind with a sustainable system that over its life will pay for itself and more."
  },
  {
    "objectID": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html#streamlit-application",
    "href": "posts/2024-06-05-solar-payback-time/solar-payback-time-report.html#streamlit-application",
    "title": "DRAFT: Analysis for a Solar PV & Battery System (South Africa)",
    "section": "Streamlit Application",
    "text": "Streamlit Application\nThe model also built a Streamlit application to help me calculate the payback time of the system. You can access the application here.\nThere are quite a number of assumptions that I still need to work through‚Ä¶"
  },
  {
    "objectID": "content/research/Wired-for-success-abstract.html",
    "href": "content/research/Wired-for-success-abstract.html",
    "title": "Wired for Success - Sweden‚Äôs Electrical Innovators in the Age of Industrial Transformation",
    "section": "",
    "text": "This paper delves into the intersection of technological innovation and economic leveling in Sweden during the first half of the 20th century, with a particular focus on the electrification revolution. As electricity transitioned from a luxury to an essential component of production and lifestyle, it not only improved the marginal product of labor, but also led to more equitable income distribution and job creation. Central to this transition were the individuals who championed and adopted this new technology.\nThe purpose of this study is to meticulously trace the career trajectories of these innovators and early adopters, by exploring their personal backgrounds, professional connections, and global linkages. Leveraging an unconventional data source, the Vem √§r vem? (Who is who?) series, this paper curates a comprehensive database of biographical information on approximately 70,000 notable Swedes from 1945-1950 and 1962-1968. We make use of OpenAI‚Äôs GPT-3.5 turbo API to unabbreviate the biographical text (from mid-20th-century Swedish), and structure it into a format that is amenable to network analysis (the Schema.org/Person schema). The resulting dataset contains information on the individuals education, employment, and social connections, as well as their geographical location and industry affiliation.\nBy systematically analyzing this rich data, we reconstruct a multi-layered network of individuals, their geographical loci, the companies they were associated with, and the industries they were part of. This innovative approach elucidates the dynamic interplay between technological progress, economic development, and social mobility, while shedding light on the critical role played by human agents in technological adoption and diffusion. The study enhances our understanding of the historical roots of Sweden‚Äôs economic leveling and can provide insights into the complex processes underlying similar technological transitions globally."
  },
  {
    "objectID": "content/research/Wired-for-success-abstract.html#abstract",
    "href": "content/research/Wired-for-success-abstract.html#abstract",
    "title": "Wired for Success - Sweden‚Äôs Electrical Innovators in the Age of Industrial Transformation",
    "section": "",
    "text": "This paper delves into the intersection of technological innovation and economic leveling in Sweden during the first half of the 20th century, with a particular focus on the electrification revolution. As electricity transitioned from a luxury to an essential component of production and lifestyle, it not only improved the marginal product of labor, but also led to more equitable income distribution and job creation. Central to this transition were the individuals who championed and adopted this new technology.\nThe purpose of this study is to meticulously trace the career trajectories of these innovators and early adopters, by exploring their personal backgrounds, professional connections, and global linkages. Leveraging an unconventional data source, the Vem √§r vem? (Who is who?) series, this paper curates a comprehensive database of biographical information on approximately 70,000 notable Swedes from 1945-1950 and 1962-1968. We make use of OpenAI‚Äôs GPT-3.5 turbo API to unabbreviate the biographical text (from mid-20th-century Swedish), and structure it into a format that is amenable to network analysis (the Schema.org/Person schema). The resulting dataset contains information on the individuals education, employment, and social connections, as well as their geographical location and industry affiliation.\nBy systematically analyzing this rich data, we reconstruct a multi-layered network of individuals, their geographical loci, the companies they were associated with, and the industries they were part of. This innovative approach elucidates the dynamic interplay between technological progress, economic development, and social mobility, while shedding light on the critical role played by human agents in technological adoption and diffusion. The study enhances our understanding of the historical roots of Sweden‚Äôs economic leveling and can provide insights into the complex processes underlying similar technological transitions globally."
  },
  {
    "objectID": "content/generative-ai-articles/uk-china-shipping-forecast.html",
    "href": "content/generative-ai-articles/uk-china-shipping-forecast.html",
    "title": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024",
    "section": "",
    "text": "The freight shipping industry is undergoing significant changes that are shaping the way goods are moved globally. For UK importers, understanding these trends is not just about staying informed ‚Äî it‚Äôs about staying ahead. This guide synthesizes the current and forecasted trends to help UK businesses navigate the complex shipping landscape in the coming year."
  },
  {
    "objectID": "content/generative-ai-articles/uk-china-shipping-forecast.html#trends-in-the-freight-shipping-industry",
    "href": "content/generative-ai-articles/uk-china-shipping-forecast.html#trends-in-the-freight-shipping-industry",
    "title": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024",
    "section": "Trends in the Freight Shipping Industry",
    "text": "Trends in the Freight Shipping Industry\nAs UK importers plan for the next 6 to 12 months, it‚Äôs crucial to be aware of the key trends in the freight shipping industry:\n\nCapacity and Rates: With new ships entering the fleet, rates are declining significantly. Drewry‚Äôs forecast suggests a -60.6% drop in Global FEU Rate YoY for Q2 2023.\nGrowth Prospects: Maritime trade is projected to grow, with an estimated 2.4% increase in 2023. UNCTAD‚Äôs report highlights this along with the need for balancing sustainability and economic demands.\nCost Reductions: Post-pandemic cost increases are set to decline, potentially reducing freight costs to levels not seen since the early 2000s. Logistics Management elaborates on this expectation.\nDry Bulk Rates: Expected to improve towards the second half of 2023. CRU‚Äôs analysis provides insights into these trends.\nFreight Rate Forecast: The Baltic Dry Index is predicted to rebound, yet container rates may continue to fall. S&P Global offers a detailed forecast.\nSector Outlook: Fitch Ratings indicates a challenging environment for container shipping, with a more stable outlook for tankers and dry bulk. Fitch Ratings‚Äô outlook provides the details.\nFreight Forwarding Market: Anticipated growth in the global market, with a projection to reach USD 168.99 billion in 2023. Mordor Intelligence covers the forecasted market trends.\nContinued Challenges and Freight Recession: The industry faces a freight recession, with economic conditions expected to persist into 2024. CNBC Supply Chain Survey discusses these challenges."
  },
  {
    "objectID": "content/generative-ai-articles/uk-china-shipping-forecast.html#suggestions-for-uk-importers",
    "href": "content/generative-ai-articles/uk-china-shipping-forecast.html#suggestions-for-uk-importers",
    "title": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024",
    "section": "Suggestions for UK Importers",
    "text": "Suggestions for UK Importers\nIn response to these trends, UK importers are advised to:\n\nDiversify Supply Chains: Mitigate risks by sourcing from multiple countries and suppliers.\nInvest in Digitalization: Utilize digital tools for efficient shipment management. The move towards paperless shipping and AI is transforming the industry.\nFocus on Sustainability: Align with greener shipping options and comply with new environmental regulations.\nEnhance Cybersecurity: Protect operations through robust cybersecurity measures as the industry moves towards greater digitization.\nBuild Resilient Operations: Develop strategies to cope with economic headwinds, including revisiting contracts and hedging against price fluctuations.\nLast-Mile Innovations: Explore options to enhance customer satisfaction in B2C through last-mile delivery innovations.\nMonitor Rates and Capacity: Stay informed on rate changes and capacity to optimize shipping schedules and budgeting."
  },
  {
    "objectID": "content/generative-ai-articles/uk-china-shipping-forecast.html#china-specific-suggestions-for-uk-importers",
    "href": "content/generative-ai-articles/uk-china-shipping-forecast.html#china-specific-suggestions-for-uk-importers",
    "title": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024",
    "section": "China-Specific Suggestions for UK Importers",
    "text": "China-Specific Suggestions for UK Importers\nUK businesses importing from China should be particularly mindful of the following regulatory aspects:\n\nForeign Investment Law: Keep informed about China‚Äôs Foreign Investment Law, which aims to level the playing field between foreign and domestic firms, and includes measures on forced technology transfer, IP protection, and government procurement. Foreign Investment Law\nNegative Lists: Check the Foreign Investment Negative List (FINL) and the Market Access Negative List (MANL) for areas restricted or prohibited to foreign investment. Business sectors not on the lists are open to foreign participation. China‚Äôs Negative Lists\nMarket Opportunities: Utilize China‚Äôs economic recovery as an opportunity for trade, especially in innovative products and high-end consumer goods. Export to China guide\n\nUnderstanding these elements is crucial for navigating the complex trade and investment environment in China.\nThough there is concern about China‚Äôs economic future, particularly with a potential demographic collapse and increased authoritarianism, these concerns seem overstated. See the video from the Money and Macro Youtube channel below for more."
  },
  {
    "objectID": "content/generative-ai-articles/uk-china-shipping-forecast.html#additional-considerations",
    "href": "content/generative-ai-articles/uk-china-shipping-forecast.html#additional-considerations",
    "title": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024",
    "section": "Additional Considerations",
    "text": "Additional Considerations\n\nRegulatory Changes: Stay updated on Brexit-related trade policies that could impact tariffs and customs procedures.\nTechnology Partnerships: Explore partnerships with tech firms for cutting-edge analytics and improved logistical efficiencies.\nCurrency Fluctuations: Implement foreign exchange risk management strategies to mitigate the impact of currency volatility on international transactions.\n\nIncorporating these additional considerations into your strategy will ensure a more robust approach to overcoming the challenges and seizing the opportunities in the dynamic world of freight shipping."
  },
  {
    "objectID": "content/generative-ai-articles/uk-china-shipping-forecast.html#conclusion",
    "href": "content/generative-ai-articles/uk-china-shipping-forecast.html#conclusion",
    "title": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024",
    "section": "Conclusion",
    "text": "Conclusion\nUK importers must be proactive and flexible, with a strategic approach to supply chain management. By staying informed of industry trends and adapting to new challenges and opportunities, businesses can maintain a competitive edge in the dynamic freight shipping landscape."
  },
  {
    "objectID": "content/consulting.html",
    "href": "content/consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "üìäüë®üíªüé®üí°ü§ùüìàüìö\nI am keen to work as consultant in data analysis and visualization, as well as web design.\nAs a data analyst with experience in both medical and historical data, I offer expertise in tidying, analyzing, and reporting on complex data sets. I specialize in crafting engaging visualizations that help organizations understand their data and make informed decisions.\nMy experience includes working with a not-for-profit medical office, where I was responsible for analyzing and reporting on a complex database of medical records. Additionally, I have experience crafting more than 30 data visualizations for Our Long Walk to Economic Freedom by Prof.¬†Johan Fourie, a non-fiction book about the economic history of South Africa, which has subsequently been turned into a textbook.\nFurthermore, I have designed, built, and maintained the website for the 15th Swedish Economic History Meeting for the Swedish Economic History Association.\nIf you‚Äôre looking for a consultant to help you make sense of your data, please don‚Äôt hesitate to get in touch through the links on the about me page. I‚Äôd be happy to discuss your needs and see if we‚Äôre a good fit."
  },
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome to Interlude One",
    "section": "",
    "text": "This is a line from Interlude One off of Alt-J‚Äôs first album, An Awesome Wave. I like this lyric a great deal!\nI aspire to balance in my own life. This site is a space to share what I find interesting, and encourage me to keep learning and sharing. It includes exploratory data analyses, R tutorials, and answers to some data specific questions. I hope that you enjoy your look around."
  },
  {
    "objectID": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "href": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "title": "ChatGPT fills in for me as a blog post author",
    "section": "",
    "text": "ChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\nToday I asked ChatGPT to to the same.\nIt took 30 seconds.\nHere are the results!\n\n\n\n\n\n\n\n\n\nPenguins created with DALL¬∑E with the prompt ‚ÄúA diagram of three penguins in the style of Paul C√©zanne, hyper realistic, beautiful art‚Äù\n\n\n‚ÄòPrompt engineering‚Äô describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\nHere, I asked ChatGPT to:\n\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\n\n\n\n\nI recieved text and code that ran almost perfectly:\nIt began\n\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we‚Äôll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\nTo start, let‚Äôs load the necessary packages :\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\n\nNext, we‚Äôll load the Palmer Penguins dataset and take a look at the variables we have to work with:\n\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_l‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema‚Ä¶  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema‚Ä¶  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema‚Ä¶  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\n\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we‚Äôll focus on the relationship between bill_length_mm and body_mass_g.\nTo understand the coefficient plot, we‚Äôll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We‚Äôll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\n\n\n\n\n\n\n\n\n\nNow we‚Äôll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\n\n\nreg_results &lt;- lm(body_mass_g ~ bill_length_mm, data = penguins)\nreg_tidy &lt;- broom::tidy(reg_results, conf.int = TRUE)\n\n\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\nTo add the error bars, we‚Äôll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\n\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\n\n\n\n\nAnd that‚Äôs it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\n\n\n\n\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\n\n\n\n\nWow! Amazing. I learned a bunch about ggplot and R from reading the code which accompanied David Robinson‚Äôs screencasts in 2019 and 2020. I am excited for the generation of data analysts beginning their journey now - being able to ask for tailored instructions for a task is a real boon!"
  },
  {
    "objectID": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html#purpose",
    "href": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html#purpose",
    "title": "ChatGPT fills in for me as a blog post author",
    "section": "",
    "text": "ChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\nToday I asked ChatGPT to to the same.\nIt took 30 seconds.\nHere are the results!\n\n\n\n\n\n\n\n\n\nPenguins created with DALL¬∑E with the prompt ‚ÄúA diagram of three penguins in the style of Paul C√©zanne, hyper realistic, beautiful art‚Äù\n\n\n‚ÄòPrompt engineering‚Äô describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\nHere, I asked ChatGPT to:\n\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\n\n\n\n\nI recieved text and code that ran almost perfectly:\nIt began\n\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we‚Äôll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\nTo start, let‚Äôs load the necessary packages :\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\n\nNext, we‚Äôll load the Palmer Penguins dataset and take a look at the variables we have to work with:\n\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nhead(penguins)\n\n# A tibble: 6 √ó 8\n  species island    bill_length_mm bill_depth_mm flipper_l‚Ä¶¬π body_‚Ä¶¬≤ sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema‚Ä¶  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema‚Ä¶  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema‚Ä¶  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# ‚Ä¶ with abbreviated variable names ¬π‚Äãflipper_length_mm, ¬≤‚Äãbody_mass_g\n\n\n\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we‚Äôll focus on the relationship between bill_length_mm and body_mass_g.\nTo understand the coefficient plot, we‚Äôll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We‚Äôll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\n\n\n\n\n\n\n\n\n\nNow we‚Äôll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\n\n\n\n\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\n\n\nreg_results &lt;- lm(body_mass_g ~ bill_length_mm, data = penguins)\nreg_tidy &lt;- broom::tidy(reg_results, conf.int = TRUE)\n\n\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\nTo add the error bars, we‚Äôll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\n\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\n\n\n\n\nAnd that‚Äôs it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\n\n\n\n\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\n\n\n\n\nWow! Amazing. I learned a bunch about ggplot and R from reading the code which accompanied David Robinson‚Äôs screencasts in 2019 and 2020. I am excited for the generation of data analysts beginning their journey now - being able to ask for tailored instructions for a task is a real boon!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "",
    "text": "Thank you to Adam A. Kemberling for providing some useful feedback on how to explain the panelset syntax and suggesting the use of dplyr::walk() instead of a for loop. His suggestion is here.\nThis is a poor solution. In the case where there are only a small number of possible categories, it works. However, if there are many potential tab names and many are empty, having to click through and find content is not pleasing at all!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#purpose",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#purpose",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Purpose",
    "text": "Purpose\nI want to create a parametrized report in Rmarkdown. The report is populated after querying a database. In short, the Rmarkdown document must have a tab for each category of a categorical variable, and draw a ggplot in the tab. The number of categories depends on the result of the query.\nWe can simulate the problem by removing one of the islands from the palmerpenguins dataset:\nThere are three islands:\n\n\n\n\n\n\n\n\nIsland name\nNumber of penguins\n\n\n\n\nBiscoe\n168\n\n\nDream\n124\n\n\nTorgersen\n52\n\n\n\n\n\nWe will subset the data to choose only two islands:\n\ndf &lt;- palmerpenguins::penguins %&gt;% \n    filter(island != \"Biscoe\")\n\ndf %&gt;%\n  distinct(island) %&gt;%\n  knitr::kable(col.names = c(\"Remaining islands\"))\n\n\n\n\nRemaining islands\n\n\n\n\nTorgersen\n\n\nDream"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#problem",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#problem",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Problem",
    "text": "Problem\nIf I hardcode the number of tabs one of the plots is drawn but has no data!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Biscoe",
    "text": "Biscoe\n\nplot_penguins(\"Biscoe\")"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe-1",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe-1",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Biscoe",
    "text": "Biscoe\n\nplot_penguins(\"Biscoe\")"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#better-solution",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#better-solution",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Better solution",
    "text": "Better solution\nA better solution is to generate the tabs programmatically, with code that writes itself depending on the contents of the returned dataset.\nI am creating the tabs with Garrick Aden-Buie‚Äôs xaringanExtra package. Have a look at the specifics in the presentation below.\n\nknitr::include_url(\"https://pkg.garrickadenbuie.com/xaringanExtra/panelset/?panelset=r-code#2\")\n\n\n\nOther useful resources incude this Science Loft post and this Stack Overflow question.\n\nTools\nIn vanilla Rmarkdown you can create the tabs with a simple heading structure.\n\n## Here are the penguins {.panelset} \n\n### Biscoe\nplot_penguins(\"Biscoe\")\n### Dream\nplot_penguins(\"Dream\")\n\nI opt to use Pandoc‚Äôs fenced div structure instead, to avoid inadvertently creating additional tabs with a third level heading. It looks like this:\n\n::::: {.panelset}\n\n::: {.panel}\n\n[Biscoe]{.panel-name}\n\nplot_penguins(\"Biscoe\")\n:::\n    \n::: {.panel}\n\n[Dream]{.panel-name}\n\nplot_penguins(\"Dream\")\n\n:::\n\n:::::\n\nIt is worth highlighting that the tabset\n\nbegins with ::::: {.panelset} and,\nends with five colons :::::\n\nEach individual tab needs to\n\nopen with ::: {.panel},\ninclude a [Panel name specified here]{.panel-name}, and\nclose with three colons :::"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#how-does-it-work",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#how-does-it-work",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "How does it work?",
    "text": "How does it work?\nNow I can programmatically create my tabs and plots within the fences. I start with a function that makes the tabs. Then I open the tabset, get a list of the islands present, and walk across each one applying my make_tab function before closing the tabset.\n\nmake_tab &lt;- function(isl) {         # function to make the tabs\n  cat(\"::: {.panel}\\n\")             # Open tab\n  cat(\"##\", isl, \"{.panel-name}\\n\") # Label tab\n  p &lt;- plot_penguins(isl)           # Create plot\n  print(p)                          # Display plot\n  cat(\"\\n\")                         # Space\n  cat(\":::\\n\")                      # Close tab\n}\n\n::::: {.panelset}\n\nisl_list &lt;- unique(df$island) %&gt;%   # Get a list of islands present in the dataset\n    as.character()\n\ndplyr::walk(isl_list, make_tab)     # make the tabs for each island that is there\n\n:::::\n\n\nGood to remember\n\nWrap code you want executed in cat(), and print() your ggplot objects.\nSet code options to results = 'asis' for the code chunk that makes your tabs. According to the Rmarkdown Cookbook:\n\n\nThis option tells knitr not to wrap your text output in verbatim code blocks, but treat it ‚Äúas is.‚Äù This can be particularly useful when you want to generate content dynamically from R code.\n\n\nPut in lots of line breaks to separate Rmarkdown content, e.g.¬†frequent uses of cat(\"\\n\") between elements."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#does-it-work",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#does-it-work",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Does it work?",
    "text": "Does it work?\nYes! Look at that, just two tabs and no empty plots!\n\nisl_list &lt;- unique(df$island) %&gt;% as.character()\n\nwalk(isl_list, make_tab)\n\nTorgersen\n\n\n\n\n\n\n\n\n\nDream\n\n\n\n\n\n\n\n\n\nI hope that this was useful. Please create an issue on Github if you have any suggestions."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#torgersen-2",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#torgersen-2",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Torgersen",
    "text": "Torgersen"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#dream-2",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#dream-2",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Dream",
    "text": "Dream"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#postscript",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#postscript",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Postscript",
    "text": "Postscript\n\nThis is the function that plots the figures:\n\nplot_penguins &lt;- function(isl) {\n  df %&gt;%\n    filter(island == isl) %&gt;%\n    ggplot(aes(bill_length_mm, \n               bill_depth_mm, \n               colour = body_mass_g, \n               shape = species)) +\n    geom_point(cex = 2) +\n    geom_smooth(method = \"lm\", se = F) +\n    scale_color_viridis_c(labels = scales::number_format()) +\n    labs(\n      x = \"Bill length (mm)\",\n      y = \"Bill depth (mm)\",\n      colour = \"Body mass (g)\",\n      shape = \"Penguin species\"\n    )\n}"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "Welcome to the first in a series of data screencasts where I attempt to show you how great the R language is.\nThis post follows along with the data screencast and includes the code methodically.\nIt begins by reading in the data, then tidying it up, analysing it, making some visualizations and then performing some predictive modelling.\n\n\nI‚Äôm signed up to a fantastic newsletter called ‚ÄúData is Plural‚Äù curated by journalist Jeremy Singer-Vine. Truly, it is a treasure trove of interesting public datasets from all over the internet. You can sign up to the newsletter here if this sounds up your alley.\nThis week it included a link to a fun selection of chocolate bar reviews, which Jeremy described as:\n\nChocolate bar reviews. The Manhattan Chocolate Society‚Äôs Brady Brelinski has reviewed 2,500+ bars of craft chocolate since 2006, and compiles his findings into a copy-paste-able table that lists each bar‚Äôs manufacturer, bean origin, percent cocoa, ingredients, review notes, and numerical rating.\n\nA live link to the database is shown below.\n\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")\n\n\n\n\n\n\nI‚Äôve copied and pasted the data into an Excel spreadsheet, accessible on my Github if you want to download it an analyse it yourself.\nThe underlying data is served in a Javascript container on the website rather than vanilla HTML. This makes it a little bit more difficult to scrape with a package like rvest, for example. Hence the Excel spreadsheet.\nTo begin the analysis, we‚Äôll read in the Excel file using the readxl package, and the here package that helps us with file paths.1\nWe don‚Äôt need to load the packages via the library(readxl) command because we‚Äôre only going to use them once or twice. Instead we can call the name of the package followed by two colons and the command, as shown below.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# read in the data\ndf &lt;- readxl::read_excel(here::here(\"posts\", \n                                    \"2022-01-17-flavours-of-cocoa\", \n                                    \"data\", \n                                    \"chocolate-bar-ratings.xlsx\"))\n\n# display the first six rows of tibble\nhead(df)\n\n# A tibble: 6 √ó 10\n    REF Company‚Ä¶¬π Compa‚Ä¶¬≤ Revie‚Ä¶¬≥ Count‚Ä¶‚Å¥ Speci‚Ä¶‚Åµ Cocoa‚Ä¶‚Å∂ Ingre‚Ä¶‚Å∑ Most ‚Ä¶‚Å∏ Rating\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1  2454 5150      U.S.A.     2019 Tanzan‚Ä¶ Kokoa ‚Ä¶    0.76 3- B,S‚Ä¶ rich c‚Ä¶   3.25\n2  2458 5150      U.S.A.     2019 Domini‚Ä¶ Zorzal‚Ä¶    0.76 3- B,S‚Ä¶ cocoa,‚Ä¶   3.5 \n3  2454 5150      U.S.A.     2019 Madaga‚Ä¶ Bejofo‚Ä¶    0.76 3- B,S‚Ä¶ cocoa,‚Ä¶   3.75\n4  2542 5150      U.S.A.     2021 Fiji    Matasa‚Ä¶    0.68 3- B,S‚Ä¶ chewy,‚Ä¶   3   \n5  2546 5150      U.S.A.     2021 Venezu‚Ä¶ Sur de‚Ä¶    0.72 3- B,S‚Ä¶ fatty,‚Ä¶   3   \n6  2546 5150      U.S.A.     2021 Uganda  Semuli‚Ä¶    0.8  3- B,S‚Ä¶ mildly‚Ä¶   3.25\n# ‚Ä¶ with abbreviated variable names ¬π‚Äã`Company (Manufacturer)`,\n#   ¬≤‚Äã`Company Location`, ¬≥‚Äã`Review Date`, ‚Å¥‚Äã`Country of Bean Origin`,\n#   ‚Åµ‚Äã`Specific Bean Origin or Bar Name`, ‚Å∂‚Äã`Cocoa Percent`, ‚Å∑‚ÄãIngredients,\n#   ‚Å∏‚Äã`Most Memorable Characteristics`\n\n\nThis gives us a tibble (similar to a dataframe) with 10 columns (4 numeric and 6 character) and 2,530 individual reviews.\nThe column names are a big ugly though:\n\n# show column names\ndf %&gt;% colnames()\n\n [1] \"REF\"                              \"Company (Manufacturer)\"          \n [3] \"Company Location\"                 \"Review Date\"                     \n [5] \"Country of Bean Origin\"           \"Specific Bean Origin or Bar Name\"\n [7] \"Cocoa Percent\"                    \"Ingredients\"                     \n [9] \"Most Memorable Characteristics\"   \"Rating\"                          \n\n\nWe can use the janitor package to make the column names snake case (lower case with words separated by an underscore).\n\n# clean names\ndf &lt;- df %&gt;% \n    janitor::clean_names()\n\n# show names again\ndf %&gt;% colnames()\n\n [1] \"ref\"                              \"company_manufacturer\"            \n [3] \"company_location\"                 \"review_date\"                     \n [5] \"country_of_bean_origin\"           \"specific_bean_origin_or_bar_name\"\n [7] \"cocoa_percent\"                    \"ingredients\"                     \n [9] \"most_memorable_characteristics\"   \"rating\"                          \n\n\nNow that we have a nice tibble with clean names, we can ask what the data itself looks like. There are many ways to get summary statistics of a dataset. I love the skim function from the skimr package.\n\n# skim the dataset\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nref\n0\n1\n1429.80\n757.65\n5.00\n802.0\n1454.00\n2079.00\n2712\n‚ñÜ‚ñá‚ñá‚ñá‚ñá\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006.00\n2012.0\n2015.00\n2018.00\n2021\n‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ\n\n\ncocoa_percent\n0\n1\n0.72\n0.06\n0.42\n0.7\n0.70\n0.74\n1\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\nrating\n0\n1\n3.20\n0.45\n1.00\n3.0\n3.25\n3.50\n4\n‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñá\n\n\n\n\n\nGreat! Our reviews are almost all complete.\n\nOnly 3 percent are missing information on the ingredients.\nThe reviews begin in 2006, the mean review is from 2014, and the latest is from 2021.\nThe percent of the bar comprising of cocoa ranges from 42 to 100, with a mean of 72.\nWe have 62 unique countries of origin for the beans, and 67 countries of manufacture.\nThere are 21 unique combinations of ingredients, comprising of seven elements in total.\n\n\n\n\nLet‚Äôs have a look at that ingredients column.\n\n# count elements of ingredients column\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 √ó 2\n   ingredients      n\n   &lt;chr&gt;        &lt;int&gt;\n 1 3- B,S,C       999\n 2 2- B,S         718\n 3 4- B,S,C,L     286\n 4 5- B,S,C,V,L   184\n 5 4- B,S,C,V     141\n 6 &lt;NA&gt;            87\n 7 2- B,S*         31\n 8 4- B,S*,C,Sa    20\n 9 3- B,S*,C       12\n10 3- B,S,L         8\n# ‚Ä¶ with 12 more rows\n\n\nSo we have a number of ingredients, a dash, and then a key for what the ingredients are. Consulting the website reveals that there are seven possible ingredients:\n\n\n\n\n\nkey\nvalue\n\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\nThese key and value combinations are very sensible - if we have a lot of data we can save space by using the keys instead of the whole string. However, I would prefer to have them written out, because we‚Äôre going to split them into their own columns a little bit later.\nWe can use the str_replace_all function from the stringr package to replace items in the list of ingredients with names.\n\ndf &lt;- df %&gt;% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nLet‚Äôs look again at our ingredients column:\n\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 √ó 2\n   ingredients                                        n\n   &lt;chr&gt;                                          &lt;int&gt;\n 1 3- beans,sugar,cocoa_butter                      999\n 2 2- beans,sugar                                   718\n 3 4- beans,sugar,cocoa_butter,lecithin             286\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\n 5 4- beans,sugar,cocoa_butter,vanilla              141\n 6 &lt;NA&gt;                                              87\n 7 2- beans,non_sugar_sweetener                      31\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\n10 3- beans,sugar,lecithin                            8\n# ‚Ä¶ with 12 more rows\n\n\nFantastic! Now we have the number of ingredients, a dash, and then each ingredient by name in one column. Let‚Äôs separate this information into two columns so that we can use the number of ingredients as a feature.\nThe separate function from the tidyr package is made just for this purpose. It takes three arguments:\n\nthe name of the column to separate.\nnew column names corresponding to the number of elements.\nthe separator between elements.\n\n\ndf &lt;- df %&gt;% \n  separate(col = ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"),\n           sep = \"-\") %&gt;% \n    # parse_number looks for a number inside a character column and discards the rest\n  mutate(n_ingredients = parse_number(n_ingredients),\n         # str_squish removes whitespace around the elements in the ingredients column\n         ingredients = str_squish(ingredients))\n\ndf %&gt;% \n  select(n_ingredients, ingredients)\n\n# A tibble: 2,530 √ó 2\n   n_ingredients ingredients                      \n           &lt;dbl&gt; &lt;chr&gt;                            \n 1             3 beans,sugar,cocoa_butter         \n 2             3 beans,sugar,cocoa_butter         \n 3             3 beans,sugar,cocoa_butter         \n 4             3 beans,sugar,cocoa_butter         \n 5             3 beans,sugar,cocoa_butter         \n 6             3 beans,sugar,cocoa_butter         \n 7             3 beans,sugar,cocoa_butter         \n 8             4 beans,sugar,cocoa_butter,lecithin\n 9             4 beans,sugar,cocoa_butter,lecithin\n10             4 beans,sugar,cocoa_butter,lecithin\n# ‚Ä¶ with 2,520 more rows\n\n\nNow we have a numeric column with the number of ingredints and a column called ingredients with each element separated by a comma.\nFinally, let‚Äôs break the ingredients from a comma separated list into a binary variable for each ingredient. We can use the recipes package that is part of the tidymodels metapackage - a framework for doing statistical modelling in a tidy manner.\nFirst we break our ingredients into 6 columns. The problem we run into is that for bars that contain different ingredients, the order of the ingredients split into the columns is not constant.\n\n# df &lt;- df %&gt;% \n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\n#            sep = \",\") \n\n# df %&gt;% \n#     select(company_manufacturer , starts_with(\"ingredient_\")) %&gt;%\n#     slice(c(1L, 51L, 54L))\n\nPerhaps there is a better way to do this? Separate rows and pivot wider?\n\ndf %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    count(ingredients)\n\n# A tibble: 8 √ó 2\n  ingredients             n\n  &lt;chr&gt;               &lt;int&gt;\n1 beans                2443\n2 cocoa_butter         1668\n3 lecithin              493\n4 non_sugar_sweetener    76\n5 salt                   37\n6 sugar                2360\n7 vanilla               353\n8 &lt;NA&gt;                   87\n\ndf &lt;- df %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    filter(!is.na(ingredients)) %&gt;% \n    pivot_wider(names_from = ingredients, values_from = ingredients) %&gt;% \n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nTidymodels\n\n# library(tidymodels)\n# dummy_multi_choice_rec &lt;- recipe(~ ., data = df) %&gt;%\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %&gt;%\n#   prep()\n# \n# df &lt;- bake(dummy_multi_choice_rec, new_data = NULL)\n\n\n\n\n\n\n\ndf %&gt;% \n  count(rating, sort = T)\n\n# A tibble: 12 √ó 2\n   rating     n\n    &lt;dbl&gt; &lt;int&gt;\n 1   3.5    552\n 2   3      506\n 3   3.25   453\n 4   2.75   320\n 5   3.75   295\n 6   2.5    156\n 7   4      111\n 8   2       29\n 9   2.25    14\n10   1.5      5\n11   1        1\n12   1.75     1\n\n\nScore range between 1 and 4, and the modal value is 3.5.\n\n# histogram\ndf %&gt;% \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\n    labs(x = \"Chocolate bar rating\",\n         y = \"Number of bars\")\n\n\n\n\n\n\n\n\n\n\n\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    knitr::kable(digits = 2)\n\n\n\n\nreview_date\nmean_rating\n\n\n\n\n2006\n3.06\n\n\n2007\n3.17\n\n\n2008\n3.04\n\n\n2009\n3.09\n\n\n2010\n3.19\n\n\n2011\n3.27\n\n\n2012\n3.21\n\n\n2013\n3.21\n\n\n2014\n3.20\n\n\n2015\n3.24\n\n\n2016\n3.24\n\n\n2017\n3.36\n\n\n2018\n3.19\n\n\n2019\n3.13\n\n\n2020\n3.26\n\n\n2021\n3.32\n\n\n\n\n\nIt certainly seems like the mean rating is increasing over time. What is driving this?\nWe can make a plot of the figures above to see the increasing trend.\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    ggplot(aes(review_date, mean_rating)) +\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\n    labs(x = \"Date of review\",\n         y = \"Mean rating\")\n\n\n\n\n\n\n\n\nLet‚Äôs make a boxplot to see how the spread of scores has changed over time.\n\ndf %&gt;%\n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_jitter(alpha = .2) +\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\n\n\n\n\n\n\n\n\nIt seems as if the share of bars with very low scores has decreased over time, while the median value has remained relatively stable over time, shown by the bar in the centre of the boxplots.\nWhat about making a joy plot or ridgeline plot with the ggridges package? This allows us to see how the spread of values has changed over time.\n\nlibrary(ggridges)\n\ndf %&gt;%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\n\n\n\n\nThis confirms what we saw in the boxplots above: fewer low scores in more recent years mean that the mean has increased, while the top of the distributions remain largely the same.\n\n\n\n\ndf %&gt;% \n  mutate(cocoa_percent = round(cocoa_percent, 1)) %&gt;% \n  count(cocoa_percent, n_ingredients) %&gt;% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\n\n\n\n\n\n\n\n\ndf %&gt;% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 √ó 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                246\n 2 Peru                     231\n 3 Dominican Republic       220\n 4 Ecuador                  201\n 5 Madagascar               171\n 6 Blend                    144\n 7 Nicaragua                100\n 8 Bolivia                   79\n 9 Colombia                  78\n10 Tanzania                  78\n# ‚Ä¶ with 52 more rows\n\ndf %&gt;% \n  add_count(country_of_bean_origin) %&gt;%\n  # only include countries with more than 60 bars\n  filter(n &gt; 60) %&gt;% \n  group_by(country_of_bean_origin) %&gt;% \n  summarise(mean_rating = mean(rating)) %&gt;% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %&gt;% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tmap)\ndata(\"World\")\n\nworld &lt;- World %&gt;% as_tibble()\n\nTo join our data on chocolate to this map, we need to get coutnry codes, using the countrycode package.\n\nlibrary(countrycode)\n\ndf &lt;- df %&gt;% \n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\n\n\nlibrary(sf)\n\ndf_map &lt;- df %&gt;% \n    group_by(iso_a3) %&gt;%\n    add_count() %&gt;% \n    summarise(mean_rating = mean(rating),\n              n = n) %&gt;% \n    ungroup() %&gt;% \n    distinct() %&gt;% \n    left_join(world, by = \"iso_a3\")\n\n\ndf_map %&gt;% \n    filter(n &gt; 3) %&gt;% \n    st_as_sf() %&gt;% ggplot() +\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n    geom_sf(aes(fill = mean_rating)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    labs(fill = \"Mean country rating\")\n\n\n\n\n\n\n\n\n\n\n\n\n\ndf_characteristics &lt;- df %&gt;% \n  select(c(most_memorable_characteristics, rating)) %&gt;% \n  separate_rows(most_memorable_characteristics, sep = \",\") %&gt;% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\n\ndf_characteristics %&gt;% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 √ó 2\n   most_memorable_characteristics     n\n   &lt;chr&gt;                          &lt;int&gt;\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# ‚Ä¶ with 938 more rows\n\n\nWe can start with a naive analysis that looks only at average score per word. These are the highest scoring words.\n\n# df_characteristics %&gt;% \n#   group_by(most_memorable_characteristics) %&gt;% \n#   add_count() %&gt;% \n#   mutate(avg_rating = mean(rating)) %&gt;% \n#   ungroup() %&gt;% \n#   slice_max(avg_rating, n = 12, with_ties = F)\n\ndf_characteristics %&gt;% \n  group_by(most_memorable_characteristics) %&gt;% \n  add_count() %&gt;% \n  filter(n &gt; 3) %&gt;% \n  mutate(avg_rating = mean(rating)) %&gt;% \n  ungroup() %&gt;% \n  distinct(most_memorable_characteristics, avg_rating) %&gt;% \n  slice_max(avg_rating, n = 12, with_ties = F) %&gt;% \n    mutate(avg_rating = round(avg_rating, 2)) %&gt;% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds &lt;- vfold_cv(df_characteristics)\n\nglmnet_recipe &lt;- \n  recipe(formula = rating ~ ., data = df_characteristics) %&gt;% \n  step_tokenize(most_memorable_characteristics) %&gt;% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %&gt;% \n  step_tf(most_memorable_characteristics) %&gt;% \n  step_normalize(all_predictors(), -all_nominal())\n\n\nglmnet_recipe %&gt;% prep() %&gt;% juice()\n\n# A tibble: 6,839 √ó 101\n   rating tf_most_memo‚Ä¶¬π tf_mo‚Ä¶¬≤ tf_mo‚Ä¶¬≥ tf_mo‚Ä¶‚Å¥ tf_mo‚Ä¶‚Åµ tf_mo‚Ä¶‚Å∂ tf_mo‚Ä¶‚Å∑ tf_mo‚Ä¶‚Å∏\n    &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 2   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 3   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 4   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 5   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 6   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 7   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 8   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 9   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n10   3           -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n# ‚Ä¶ with 6,829 more rows, 92 more variables:\n#   tf_most_memorable_characteristics_bitter &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_black &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bland &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bold &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bright &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_brownie &lt;dbl&gt;, ‚Ä¶\n\n\n\nglmnet_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") \n\nglmnet_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(glmnet_recipe) %&gt;% \n  add_model(glmnet_spec) \n\nglmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune &lt;- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\n\nglmnet_tune %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nglmnet_model_final &lt;- finalize_workflow(glmnet_workflow, glmnet_tune %&gt;% \n  select_best())\n\nfinal_fit &lt;- glmnet_model_final %&gt;% \n  fit(df_characteristics)\n\nWhat does the fit look like? These are the terms that have the greatest effect on bar rating.\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %&gt;%\n  mutate(sign = estimate &gt; 0) %&gt;%\n  group_by(sign) %&gt;%\n  mutate(estimate = abs(estimate)) %&gt;% \n  slice_max(estimate, n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\n\n\n\n\nWow! have a look at the terms up top - ‚Äúcreamy‚Äù, ‚Äúcomplex‚Äù and ‚Äúrich‚Äù are good chocolate words. On the other side, ‚Äúbitter‚Äù, ‚Äúoff‚Äù and ‚Äúchemical‚Äù are terms that lower the score of the bar."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#beans",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#beans",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "I‚Äôm signed up to a fantastic newsletter called ‚ÄúData is Plural‚Äù curated by journalist Jeremy Singer-Vine. Truly, it is a treasure trove of interesting public datasets from all over the internet. You can sign up to the newsletter here if this sounds up your alley.\nThis week it included a link to a fun selection of chocolate bar reviews, which Jeremy described as:\n\nChocolate bar reviews. The Manhattan Chocolate Society‚Äôs Brady Brelinski has reviewed 2,500+ bars of craft chocolate since 2006, and compiles his findings into a copy-paste-able table that lists each bar‚Äôs manufacturer, bean origin, percent cocoa, ingredients, review notes, and numerical rating.\n\nA live link to the database is shown below.\n\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#ingest-the-data",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#ingest-the-data",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "I‚Äôve copied and pasted the data into an Excel spreadsheet, accessible on my Github if you want to download it an analyse it yourself.\nThe underlying data is served in a Javascript container on the website rather than vanilla HTML. This makes it a little bit more difficult to scrape with a package like rvest, for example. Hence the Excel spreadsheet.\nTo begin the analysis, we‚Äôll read in the Excel file using the readxl package, and the here package that helps us with file paths.1\nWe don‚Äôt need to load the packages via the library(readxl) command because we‚Äôre only going to use them once or twice. Instead we can call the name of the package followed by two colons and the command, as shown below.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# read in the data\ndf &lt;- readxl::read_excel(here::here(\"posts\", \n                                    \"2022-01-17-flavours-of-cocoa\", \n                                    \"data\", \n                                    \"chocolate-bar-ratings.xlsx\"))\n\n# display the first six rows of tibble\nhead(df)\n\n# A tibble: 6 √ó 10\n    REF Company‚Ä¶¬π Compa‚Ä¶¬≤ Revie‚Ä¶¬≥ Count‚Ä¶‚Å¥ Speci‚Ä¶‚Åµ Cocoa‚Ä¶‚Å∂ Ingre‚Ä¶‚Å∑ Most ‚Ä¶‚Å∏ Rating\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1  2454 5150      U.S.A.     2019 Tanzan‚Ä¶ Kokoa ‚Ä¶    0.76 3- B,S‚Ä¶ rich c‚Ä¶   3.25\n2  2458 5150      U.S.A.     2019 Domini‚Ä¶ Zorzal‚Ä¶    0.76 3- B,S‚Ä¶ cocoa,‚Ä¶   3.5 \n3  2454 5150      U.S.A.     2019 Madaga‚Ä¶ Bejofo‚Ä¶    0.76 3- B,S‚Ä¶ cocoa,‚Ä¶   3.75\n4  2542 5150      U.S.A.     2021 Fiji    Matasa‚Ä¶    0.68 3- B,S‚Ä¶ chewy,‚Ä¶   3   \n5  2546 5150      U.S.A.     2021 Venezu‚Ä¶ Sur de‚Ä¶    0.72 3- B,S‚Ä¶ fatty,‚Ä¶   3   \n6  2546 5150      U.S.A.     2021 Uganda  Semuli‚Ä¶    0.8  3- B,S‚Ä¶ mildly‚Ä¶   3.25\n# ‚Ä¶ with abbreviated variable names ¬π‚Äã`Company (Manufacturer)`,\n#   ¬≤‚Äã`Company Location`, ¬≥‚Äã`Review Date`, ‚Å¥‚Äã`Country of Bean Origin`,\n#   ‚Åµ‚Äã`Specific Bean Origin or Bar Name`, ‚Å∂‚Äã`Cocoa Percent`, ‚Å∑‚ÄãIngredients,\n#   ‚Å∏‚Äã`Most Memorable Characteristics`\n\n\nThis gives us a tibble (similar to a dataframe) with 10 columns (4 numeric and 6 character) and 2,530 individual reviews.\nThe column names are a big ugly though:\n\n# show column names\ndf %&gt;% colnames()\n\n [1] \"REF\"                              \"Company (Manufacturer)\"          \n [3] \"Company Location\"                 \"Review Date\"                     \n [5] \"Country of Bean Origin\"           \"Specific Bean Origin or Bar Name\"\n [7] \"Cocoa Percent\"                    \"Ingredients\"                     \n [9] \"Most Memorable Characteristics\"   \"Rating\"                          \n\n\nWe can use the janitor package to make the column names snake case (lower case with words separated by an underscore).\n\n# clean names\ndf &lt;- df %&gt;% \n    janitor::clean_names()\n\n# show names again\ndf %&gt;% colnames()\n\n [1] \"ref\"                              \"company_manufacturer\"            \n [3] \"company_location\"                 \"review_date\"                     \n [5] \"country_of_bean_origin\"           \"specific_bean_origin_or_bar_name\"\n [7] \"cocoa_percent\"                    \"ingredients\"                     \n [9] \"most_memorable_characteristics\"   \"rating\"                          \n\n\nNow that we have a nice tibble with clean names, we can ask what the data itself looks like. There are many ways to get summary statistics of a dataset. I love the skim function from the skimr package.\n\n# skim the dataset\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nref\n0\n1\n1429.80\n757.65\n5.00\n802.0\n1454.00\n2079.00\n2712\n‚ñÜ‚ñá‚ñá‚ñá‚ñá\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006.00\n2012.0\n2015.00\n2018.00\n2021\n‚ñÉ‚ñÖ‚ñá‚ñÜ‚ñÖ\n\n\ncocoa_percent\n0\n1\n0.72\n0.06\n0.42\n0.7\n0.70\n0.74\n1\n‚ñÅ‚ñÅ‚ñá‚ñÅ‚ñÅ\n\n\nrating\n0\n1\n3.20\n0.45\n1.00\n3.0\n3.25\n3.50\n4\n‚ñÅ‚ñÅ‚ñÖ‚ñá‚ñá\n\n\n\n\n\nGreat! Our reviews are almost all complete.\n\nOnly 3 percent are missing information on the ingredients.\nThe reviews begin in 2006, the mean review is from 2014, and the latest is from 2021.\nThe percent of the bar comprising of cocoa ranges from 42 to 100, with a mean of 72.\nWe have 62 unique countries of origin for the beans, and 67 countries of manufacture.\nThere are 21 unique combinations of ingredients, comprising of seven elements in total."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#data-cleaning-and-feature-engineering",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#data-cleaning-and-feature-engineering",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "Let‚Äôs have a look at that ingredients column.\n\n# count elements of ingredients column\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 √ó 2\n   ingredients      n\n   &lt;chr&gt;        &lt;int&gt;\n 1 3- B,S,C       999\n 2 2- B,S         718\n 3 4- B,S,C,L     286\n 4 5- B,S,C,V,L   184\n 5 4- B,S,C,V     141\n 6 &lt;NA&gt;            87\n 7 2- B,S*         31\n 8 4- B,S*,C,Sa    20\n 9 3- B,S*,C       12\n10 3- B,S,L         8\n# ‚Ä¶ with 12 more rows\n\n\nSo we have a number of ingredients, a dash, and then a key for what the ingredients are. Consulting the website reveals that there are seven possible ingredients:\n\n\n\n\n\nkey\nvalue\n\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\nThese key and value combinations are very sensible - if we have a lot of data we can save space by using the keys instead of the whole string. However, I would prefer to have them written out, because we‚Äôre going to split them into their own columns a little bit later.\nWe can use the str_replace_all function from the stringr package to replace items in the list of ingredients with names.\n\ndf &lt;- df %&gt;% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nLet‚Äôs look again at our ingredients column:\n\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 √ó 2\n   ingredients                                        n\n   &lt;chr&gt;                                          &lt;int&gt;\n 1 3- beans,sugar,cocoa_butter                      999\n 2 2- beans,sugar                                   718\n 3 4- beans,sugar,cocoa_butter,lecithin             286\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\n 5 4- beans,sugar,cocoa_butter,vanilla              141\n 6 &lt;NA&gt;                                              87\n 7 2- beans,non_sugar_sweetener                      31\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\n10 3- beans,sugar,lecithin                            8\n# ‚Ä¶ with 12 more rows\n\n\nFantastic! Now we have the number of ingredients, a dash, and then each ingredient by name in one column. Let‚Äôs separate this information into two columns so that we can use the number of ingredients as a feature.\nThe separate function from the tidyr package is made just for this purpose. It takes three arguments:\n\nthe name of the column to separate.\nnew column names corresponding to the number of elements.\nthe separator between elements.\n\n\ndf &lt;- df %&gt;% \n  separate(col = ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"),\n           sep = \"-\") %&gt;% \n    # parse_number looks for a number inside a character column and discards the rest\n  mutate(n_ingredients = parse_number(n_ingredients),\n         # str_squish removes whitespace around the elements in the ingredients column\n         ingredients = str_squish(ingredients))\n\ndf %&gt;% \n  select(n_ingredients, ingredients)\n\n# A tibble: 2,530 √ó 2\n   n_ingredients ingredients                      \n           &lt;dbl&gt; &lt;chr&gt;                            \n 1             3 beans,sugar,cocoa_butter         \n 2             3 beans,sugar,cocoa_butter         \n 3             3 beans,sugar,cocoa_butter         \n 4             3 beans,sugar,cocoa_butter         \n 5             3 beans,sugar,cocoa_butter         \n 6             3 beans,sugar,cocoa_butter         \n 7             3 beans,sugar,cocoa_butter         \n 8             4 beans,sugar,cocoa_butter,lecithin\n 9             4 beans,sugar,cocoa_butter,lecithin\n10             4 beans,sugar,cocoa_butter,lecithin\n# ‚Ä¶ with 2,520 more rows\n\n\nNow we have a numeric column with the number of ingredints and a column called ingredients with each element separated by a comma.\nFinally, let‚Äôs break the ingredients from a comma separated list into a binary variable for each ingredient. We can use the recipes package that is part of the tidymodels metapackage - a framework for doing statistical modelling in a tidy manner.\nFirst we break our ingredients into 6 columns. The problem we run into is that for bars that contain different ingredients, the order of the ingredients split into the columns is not constant.\n\n# df &lt;- df %&gt;% \n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\n#            sep = \",\") \n\n# df %&gt;% \n#     select(company_manufacturer , starts_with(\"ingredient_\")) %&gt;%\n#     slice(c(1L, 51L, 54L))\n\nPerhaps there is a better way to do this? Separate rows and pivot wider?\n\ndf %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    count(ingredients)\n\n# A tibble: 8 √ó 2\n  ingredients             n\n  &lt;chr&gt;               &lt;int&gt;\n1 beans                2443\n2 cocoa_butter         1668\n3 lecithin              493\n4 non_sugar_sweetener    76\n5 salt                   37\n6 sugar                2360\n7 vanilla               353\n8 &lt;NA&gt;                   87\n\ndf &lt;- df %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    filter(!is.na(ingredients)) %&gt;% \n    pivot_wider(names_from = ingredients, values_from = ingredients) %&gt;% \n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nTidymodels\n\n# library(tidymodels)\n# dummy_multi_choice_rec &lt;- recipe(~ ., data = df) %&gt;%\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %&gt;%\n#   prep()\n# \n# df &lt;- bake(dummy_multi_choice_rec, new_data = NULL)"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#analysing-the-data",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#analysing-the-data",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "df %&gt;% \n  count(rating, sort = T)\n\n# A tibble: 12 √ó 2\n   rating     n\n    &lt;dbl&gt; &lt;int&gt;\n 1   3.5    552\n 2   3      506\n 3   3.25   453\n 4   2.75   320\n 5   3.75   295\n 6   2.5    156\n 7   4      111\n 8   2       29\n 9   2.25    14\n10   1.5      5\n11   1        1\n12   1.75     1\n\n\nScore range between 1 and 4, and the modal value is 3.5.\n\n# histogram\ndf %&gt;% \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\n    labs(x = \"Chocolate bar rating\",\n         y = \"Number of bars\")\n\n\n\n\n\n\n\n\n\n\n\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    knitr::kable(digits = 2)\n\n\n\n\nreview_date\nmean_rating\n\n\n\n\n2006\n3.06\n\n\n2007\n3.17\n\n\n2008\n3.04\n\n\n2009\n3.09\n\n\n2010\n3.19\n\n\n2011\n3.27\n\n\n2012\n3.21\n\n\n2013\n3.21\n\n\n2014\n3.20\n\n\n2015\n3.24\n\n\n2016\n3.24\n\n\n2017\n3.36\n\n\n2018\n3.19\n\n\n2019\n3.13\n\n\n2020\n3.26\n\n\n2021\n3.32\n\n\n\n\n\nIt certainly seems like the mean rating is increasing over time. What is driving this?\nWe can make a plot of the figures above to see the increasing trend.\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    ggplot(aes(review_date, mean_rating)) +\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\n    labs(x = \"Date of review\",\n         y = \"Mean rating\")\n\n\n\n\n\n\n\n\nLet‚Äôs make a boxplot to see how the spread of scores has changed over time.\n\ndf %&gt;%\n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_jitter(alpha = .2) +\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\n\n\n\n\n\n\n\n\nIt seems as if the share of bars with very low scores has decreased over time, while the median value has remained relatively stable over time, shown by the bar in the centre of the boxplots.\nWhat about making a joy plot or ridgeline plot with the ggridges package? This allows us to see how the spread of values has changed over time.\n\nlibrary(ggridges)\n\ndf %&gt;%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\n\n\n\n\nThis confirms what we saw in the boxplots above: fewer low scores in more recent years mean that the mean has increased, while the top of the distributions remain largely the same.\n\n\n\n\ndf %&gt;% \n  mutate(cocoa_percent = round(cocoa_percent, 1)) %&gt;% \n  count(cocoa_percent, n_ingredients) %&gt;% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\n\n\n\n\n\n\n\n\ndf %&gt;% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 √ó 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                246\n 2 Peru                     231\n 3 Dominican Republic       220\n 4 Ecuador                  201\n 5 Madagascar               171\n 6 Blend                    144\n 7 Nicaragua                100\n 8 Bolivia                   79\n 9 Colombia                  78\n10 Tanzania                  78\n# ‚Ä¶ with 52 more rows\n\ndf %&gt;% \n  add_count(country_of_bean_origin) %&gt;%\n  # only include countries with more than 60 bars\n  filter(n &gt; 60) %&gt;% \n  group_by(country_of_bean_origin) %&gt;% \n  summarise(mean_rating = mean(rating)) %&gt;% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %&gt;% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\n\n\n\n\n\n\n\n\nlibrary(tmap)\ndata(\"World\")\n\nworld &lt;- World %&gt;% as_tibble()\n\nTo join our data on chocolate to this map, we need to get coutnry codes, using the countrycode package.\n\nlibrary(countrycode)\n\ndf &lt;- df %&gt;% \n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\n\n\nlibrary(sf)\n\ndf_map &lt;- df %&gt;% \n    group_by(iso_a3) %&gt;%\n    add_count() %&gt;% \n    summarise(mean_rating = mean(rating),\n              n = n) %&gt;% \n    ungroup() %&gt;% \n    distinct() %&gt;% \n    left_join(world, by = \"iso_a3\")\n\n\ndf_map %&gt;% \n    filter(n &gt; 3) %&gt;% \n    st_as_sf() %&gt;% ggplot() +\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n    geom_sf(aes(fill = mean_rating)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    labs(fill = \"Mean country rating\")"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#word-model",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#word-model",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "df_characteristics &lt;- df %&gt;% \n  select(c(most_memorable_characteristics, rating)) %&gt;% \n  separate_rows(most_memorable_characteristics, sep = \",\") %&gt;% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\n\ndf_characteristics %&gt;% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 √ó 2\n   most_memorable_characteristics     n\n   &lt;chr&gt;                          &lt;int&gt;\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# ‚Ä¶ with 938 more rows\n\n\nWe can start with a naive analysis that looks only at average score per word. These are the highest scoring words.\n\n# df_characteristics %&gt;% \n#   group_by(most_memorable_characteristics) %&gt;% \n#   add_count() %&gt;% \n#   mutate(avg_rating = mean(rating)) %&gt;% \n#   ungroup() %&gt;% \n#   slice_max(avg_rating, n = 12, with_ties = F)\n\ndf_characteristics %&gt;% \n  group_by(most_memorable_characteristics) %&gt;% \n  add_count() %&gt;% \n  filter(n &gt; 3) %&gt;% \n  mutate(avg_rating = mean(rating)) %&gt;% \n  ungroup() %&gt;% \n  distinct(most_memorable_characteristics, avg_rating) %&gt;% \n  slice_max(avg_rating, n = 12, with_ties = F) %&gt;% \n    mutate(avg_rating = round(avg_rating, 2)) %&gt;% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds &lt;- vfold_cv(df_characteristics)\n\nglmnet_recipe &lt;- \n  recipe(formula = rating ~ ., data = df_characteristics) %&gt;% \n  step_tokenize(most_memorable_characteristics) %&gt;% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %&gt;% \n  step_tf(most_memorable_characteristics) %&gt;% \n  step_normalize(all_predictors(), -all_nominal())\n\n\nglmnet_recipe %&gt;% prep() %&gt;% juice()\n\n# A tibble: 6,839 √ó 101\n   rating tf_most_memo‚Ä¶¬π tf_mo‚Ä¶¬≤ tf_mo‚Ä¶¬≥ tf_mo‚Ä¶‚Å¥ tf_mo‚Ä¶‚Åµ tf_mo‚Ä¶‚Å∂ tf_mo‚Ä¶‚Å∑ tf_mo‚Ä¶‚Å∏\n    &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 2   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 3   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 4   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 5   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 6   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 7   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 8   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 9   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n10   3           -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n# ‚Ä¶ with 6,829 more rows, 92 more variables:\n#   tf_most_memorable_characteristics_bitter &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_black &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bland &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bold &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bright &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_brownie &lt;dbl&gt;, ‚Ä¶\n\n\n\nglmnet_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") \n\nglmnet_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(glmnet_recipe) %&gt;% \n  add_model(glmnet_spec) \n\nglmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune &lt;- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\n\nglmnet_tune %&gt;% \n  autoplot()\n\n\n\n\n\n\n\n\n\nglmnet_model_final &lt;- finalize_workflow(glmnet_workflow, glmnet_tune %&gt;% \n  select_best())\n\nfinal_fit &lt;- glmnet_model_final %&gt;% \n  fit(df_characteristics)\n\nWhat does the fit look like? These are the terms that have the greatest effect on bar rating.\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %&gt;%\n  mutate(sign = estimate &gt; 0) %&gt;%\n  group_by(sign) %&gt;%\n  mutate(estimate = abs(estimate)) %&gt;% \n  slice_max(estimate, n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\n\n\n\n\nWow! have a look at the terms up top - ‚Äúcreamy‚Äù, ‚Äúcomplex‚Äù and ‚Äúrich‚Äù are good chocolate words. On the other side, ‚Äúbitter‚Äù, ‚Äúoff‚Äù and ‚Äúchemical‚Äù are terms that lower the score of the bar."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#footnotes",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#footnotes",
    "title": "Flavours of cocoa",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe here package allows us to abstract from the specific file path on our local computer and use a generic path that will work on any computer that we download our R project to. For example, instead of specifying the path ‚ÄúC:/Users/Jonathan/Documents/R-work/jjayes_distill/posts/2022-01-17-flavours-of-cocoa/data/chocolate-bar-ratings.xlsx‚Äù we can just call the here function from the package with the same name - here(‚Äúposts‚Äù, ‚Äú2022-01-17-flavours-of-cocoa‚Äù, ‚Äúdata‚Äù, ‚Äúchocolate-bar-ratings.xlsx‚Äù). This is wonderful for switching between, for example, a Windows and a Mac, where the slashes are in opposite directions and can cause some frustration!‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-05-06-swedenmap/swedenmap.html",
    "href": "posts/2021-05-06-swedenmap/swedenmap.html",
    "title": "Mapping Sweden‚Äôs Regional GDP",
    "section": "",
    "text": "The scatter plot below shows the relationship between population in the Swedish counties in 1571 (x-axis) and regional GDP share (y-axis).\nHovering over a point shows the name of the county, and highlights its location on the map of Sweden to the right (shown with their current borders).\nClicking on a county on the map will take you to it‚Äôs page on Wikipedia.\n\n\n\n\n\n\nSource: Enflo and Missiaia (2018), Data accessible here.\nFigure inspiration and code from Claus Wilke\n\n\n\nIn the scatter plot above we can see that the two northernmost counties, though large in land are are very sparsely populated. The line of best fit shows that V√§stmanland County has a high relative share of GDP compared to it‚Äôs population, and that Kalmar L√§n has a high population relative to it‚Äôs share of GDP in 1571."
  },
  {
    "objectID": "posts/2021-05-06-swedenmap/swedenmap.html#what-was-the-relationship-between-population-and-regional-gdp-in-sweden-in-1571",
    "href": "posts/2021-05-06-swedenmap/swedenmap.html#what-was-the-relationship-between-population-and-regional-gdp-in-sweden-in-1571",
    "title": "Mapping Sweden‚Äôs Regional GDP",
    "section": "",
    "text": "The scatter plot below shows the relationship between population in the Swedish counties in 1571 (x-axis) and regional GDP share (y-axis).\nHovering over a point shows the name of the county, and highlights its location on the map of Sweden to the right (shown with their current borders).\nClicking on a county on the map will take you to it‚Äôs page on Wikipedia.\n\n\n\n\n\n\nSource: Enflo and Missiaia (2018), Data accessible here.\nFigure inspiration and code from Claus Wilke\n\n\n\nIn the scatter plot above we can see that the two northernmost counties, though large in land are are very sparsely populated. The line of best fit shows that V√§stmanland County has a high relative share of GDP compared to it‚Äôs population, and that Kalmar L√§n has a high population relative to it‚Äôs share of GDP in 1571."
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html",
    "href": "posts/2021-04-21-chips/chips.html",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "",
    "text": "In this document I walk through the process to recreate a data visualization from Our World in Data. I use the web scraping package rvest to get the data from Wikipedia and some neat tidyverse tools.\nThe graphic depicts the metal‚Äìoxide‚Äìsilicon (MOS) transistor count on commercially available microprocessors over time. This progression is known as Moore‚Äôs law, which observed that the transistor count doubles approximately every two years.\n\n\n\n\nGraphic showing Moore‚Äôs Law - Our World In Data\n\n\n\nFirst, I show the wikipedia page that contains the information that I want to scrape (and contains the Our World in Data graphic).\n\n\n\n\nWikipedia article\n\n\n\nThe Wikipedia article on transistor counts includes a table containing 193 microprocessors released between 1971 and 2020, shown below. Crucially, it specifies for each chip the release year and transistor count.\n\n\n\n\nTable showing transistor count"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#purpose",
    "href": "posts/2021-04-21-chips/chips.html#purpose",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "",
    "text": "In this document I walk through the process to recreate a data visualization from Our World in Data. I use the web scraping package rvest to get the data from Wikipedia and some neat tidyverse tools.\nThe graphic depicts the metal‚Äìoxide‚Äìsilicon (MOS) transistor count on commercially available microprocessors over time. This progression is known as Moore‚Äôs law, which observed that the transistor count doubles approximately every two years.\n\n\n\n\nGraphic showing Moore‚Äôs Law - Our World In Data\n\n\n\nFirst, I show the wikipedia page that contains the information that I want to scrape (and contains the Our World in Data graphic).\n\n\n\n\nWikipedia article\n\n\n\nThe Wikipedia article on transistor counts includes a table containing 193 microprocessors released between 1971 and 2020, shown below. Crucially, it specifies for each chip the release year and transistor count.\n\n\n\n\nTable showing transistor count"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#scraping",
    "href": "posts/2021-04-21-chips/chips.html#scraping",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Scraping",
    "text": "Scraping\nThis chunk of code grabs the HTML of the Wikipedia page, selects all the tables on the page and saves the one we want to analyze as a tibble, a very handy dataframe alternative. Rvest‚Äôs html_table function returns all of the elements of a web table, neat!\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Transistor_count\"\n\nhtml &lt;- read_html(url)\n\nhtml_tables &lt;- html %&gt;% \n  html_table()\n\ndf_transistors &lt;- html_tables[[4]]"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#data-processing",
    "href": "posts/2021-04-21-chips/chips.html#data-processing",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Data processing",
    "text": "Data processing\nAs visible in the screen capture of the table above, a few cells also hyper-link to sources for the stats. These square brackets prevent R from reading the column as a set of numbers, instead the cell contents are returned as characters. In the chunk below I use readr‚Äôs parse_number function to extract the numbers from each cell and store them as a double.\n\ndf_transistors &lt;- df_transistors %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(processor = str_squish(str_remove(processor, \"\\\\(.*\"))) %&gt;% \n  mutate_at(c(\"transistor_count\", \"date_ofintroduction\", \"process_nm\", \"area_mm2\"), parse_number)"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#data-viz-preparation",
    "href": "posts/2021-04-21-chips/chips.html#data-viz-preparation",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Data viz preparation",
    "text": "Data viz preparation\nIn this chunk I lump together the chip designers into the five largest groups and an ‚Äòother‚Äô category with forcats‚Äô fct_lump function. It returns these categories as factors. To make the graphic readable, I sample 50 of the processors from the table of 193 to annotate onto the plot I also store some graph labels.\n\ndf_transistors %&gt;% \n  count(designer, sort = T)\n\n# A tibble: 44 √ó 2\n   designer              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Intel                64\n 2 Apple                22\n 3 AMD                  21\n 4 IBM                  19\n 5 Fujitsu              10\n 6 Huawei                7\n 7 Motorola              7\n 8 Qualcomm              7\n 9 DEC                   5\n10 Texas Instruments     5\n# ‚Ä¶ with 34 more rows\n\ndf_transistors &lt;- df_transistors %&gt;% \n  mutate(designer = fct_lump(designer, 5))\n\nprocessor_labels &lt;- sample(df_transistors$processor, 50)\n\ntitle &lt;- \"Moore's Law\"\n\nsubtitle &lt;- \"The number of transistors on integrated circuit chips (1971-2020)\"\n\ncaption &lt;- \"Source: Wikipedia - Transistor Count: https://en.wikipedia.org/wiki/Transistor_count\""
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#plotting",
    "href": "posts/2021-04-21-chips/chips.html#plotting",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Plotting",
    "text": "Plotting\nHere I draw the graphic using ggplot2.\n\n# jpeg(\n#     filename=\"C:/Users/User/Documents/Recon/jjayes_distill/_posts/2021-04-21-chips/figures/Transistors_colour.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_transistors %&gt;% \n  ggplot(aes(date_ofintroduction, transistor_count)) +\n  geom_smooth(group = 1, alpha = .1) +\n  geom_point(aes(colour = designer, shape = designer), cex = 3) +\n  geom_text_repel(data = df_transistors %&gt;% \n                    filter(processor %in% processor_labels), \n                  aes(date_ofintroduction, transistor_count, label = processor),\n                  cex = 3, colour = \"#505050\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  scale_shape_manual(values = seq(15,20, by = 1)) +\n  scale_y_log10(labels = scales::comma_format()) +\n  labs(x = \"Year\",\n       y = \"Transistor Count (log scale)\",\n       shape = \"Chip Designer\",\n       colour = \"Chip Designer\",\n       caption = caption,\n       title = title,\n       subtitle = subtitle)\n\n\n\n\n\n\n\n# dev.off()\n\nI include a smoothed line of best fit (with geom_smooth) below the points to show how progression in transistor count has slowed since 2005, despite the number of chips designed each year increasing.\nI use geom_text_repel to prevent the labels overlapping each other, and put the y-axis on a log scale to draw attention to the approximate doubling of transistor count each year."
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#conclusion",
    "href": "posts/2021-04-21-chips/chips.html#conclusion",
    "title": "Recreating Our World In Data‚Äôs Beautiful Transistor Count Graphic",
    "section": "Conclusion",
    "text": "Conclusion\nThanks for reading along! I hope you learned some neat tricks, and see how easy it is to grab data from the web to visualize."
  },
  {
    "objectID": "posts/2021-03-18-car-price-explorer/car-price-explorer.html",
    "href": "posts/2021-03-18-car-price-explorer/car-price-explorer.html",
    "title": "Car Price Comparison Tool",
    "section": "",
    "text": "Here‚Äôs a link to the web app, along with an image of what it looks like.\nThe app has three components. In the scatter plot you can see the price of each car advertised on Gumtree on the y-axis against its odometer reading on the x. Below this is a box and whisker plot of price to show the distribution of prices for the cars in the range. To the right is a table showing the details of each advert along with a link. You can choose the make, model, province, odometer range and year of manufacture in the panel on the left.\n\n\n\n\nWeb App\n\n\n\nI hope that you find it useful!"
  },
  {
    "objectID": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html",
    "href": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html",
    "title": "South African Car Colour Exploration",
    "section": "",
    "text": "While driving to Cape Town this past weekend in my beautiful white Hyundai i10, a German friend asked why every second car in South Africa was white in colour. While I didn‚Äôt have a compelling reason, I did have a some data to assess whether it was true that half of cars on the road are white.\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\nI show a useful trick for filling in missing data that creates gaps in the area chart using dplyr‚Äôs pivot_wider and pivot_longer commands.\nHere is the final interactive graphic:\n\ndf_plotly &lt;- read_rds(\"data/car_colour/df_plotly.rds\")\n\np_load(plotly)\n\ng &lt;- df_plotly %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\ndf &lt;- readRDS(\"data/car_colour/gumtree_ads_2020-10-01-11-33-am.rds\")\n\ndf &lt;- df %&gt;% filter(!is.na(make),\n                    !is.na(colour),\n                    !is.na(model)) %&gt;% \n             # Removing the duplicated province names   \n             mutate(province = str_remove(province, \"\\\\,.*\"))\n\n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n44644\n\n\nNumber of columns\n30\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n22\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nlink\n0\n1.00\n68\n86\n0\n2218\n0\n\n\nad_url\n0\n1.00\n78\n193\n0\n38392\n0\n\n\nsite\n0\n1.00\n7\n7\n0\n1\n0\n\n\nseller_type\n0\n1.00\n19\n30\n0\n2\n0\n\n\ntitle\n0\n1.00\n4\n100\n0\n26140\n0\n\n\ntext\n0\n1.00\n10\n3885\n0\n38343\n0\n\n\nad_date\n0\n1.00\n19\n19\n0\n42416\n0\n\n\nseller_name\n0\n1.00\n1\n50\n0\n8872\n0\n\n\nlocation\n0\n1.00\n24\n61\n0\n428\n0\n\n\nseller_age\n0\n1.00\n17\n22\n0\n79\n0\n\n\nn_all_time_ads\n0\n1.00\n1\n7\n0\n3481\n0\n\n\nscrape_time\n0\n1.00\n19\n19\n0\n35812\n0\n\n\nlocation_2\n0\n1.00\n11\n47\n0\n428\n0\n\n\nfor_sale_by\n226\n0.99\n5\n6\n0\n2\n0\n\n\nmake\n0\n1.00\n3\n13\n0\n37\n0\n\n\nmodel\n0\n1.00\n1\n20\n0\n459\n0\n\n\nbody_type\n226\n0.99\n3\n12\n0\n11\n0\n\n\ntransmission\n206\n1.00\n6\n9\n0\n2\n0\n\n\nfuel_type\n104\n1.00\n6\n8\n0\n4\n0\n\n\ncolour\n0\n1.00\n3\n8\n0\n16\n0\n\n\nna\n44644\n0.00\nNA\nNA\n0\n0\n0\n\n\nprovince\n0\n1.00\n7\n13\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npage\n0\n1.00\n14.51\n12.66\n1\n4.00\n11.0\n22.00\n50\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\nad_number\n0\n1.00\n23570.95\n14487.85\n1\n11471.75\n22889.5\n34258.25\n56454\n‚ñá‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nprice\n229\n0.99\n218706.68\n692268.63\n1\n109900.00\n165000.0\n259900.00\n140000000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nn_photos\n1099\n0.98\n8.63\n3.08\n1\n7.00\n9.0\n12.00\n12\n‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñá\n\n\nn_views\n0\n1.00\n588.85\n1281.69\n0\n91.00\n233.5\n580.00\n46956\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nn_active_ads\n0\n1.00\n130.20\n225.27\n1\n5.50\n29.0\n100.00\n997\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nyear\n100\n1.00\n2013.76\n6.66\n1900\n2011.00\n2016.0\n2018.00\n2021\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nkilometers\n1189\n0.97\n92267.35\n157493.13\n1\n26890.50\n72000.0\n130000.00\n9999999\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nThe dataset has some really nice information on the cars advertised, including their characteristics, as well as the location and name of the seller.\nFor this graphic, we only need the car‚Äôs year and colour.\n\n\n\nWe start by plotting the number of cars in each colour advertised on Gumtree. In this plot I show the 12 most common colours.\nIt is evident that white and silver cars are very common. In as far as our data reflects cars on the road, white cars make up nearly 1/3 of all cars advertised! A little less than the half quoted in the discussion above, but certainly the dominant colour.\n\ndf %&gt;%\n  mutate(colour = fct_lump(colour, 12)) %&gt;%\n  select(colour) %&gt;% \n  count(colour, sort = T) %&gt;% \n  mutate(colour = fct_reorder(colour, n)) %&gt;% \n  ggplot(aes(n, colour)) +\n  geom_segment(aes(xend = 0, yend = colour)) +\n  geom_point(size = 4, colour = \"orange\") +\n  geom_text(aes(label = scales::comma(n)), hjust = -.3) +\n  scale_x_continuous(labels = scales::comma_format(), limits = c(0, 16000)) +\n  labs(x = \"Number of vehicles\",\n       title = \"What are the most common vehicle colours advertised on Gumtree?\",\n       subtitle = \"Data from 45,000 adverts from across the country\",\n       y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nNext we want to know if some colours have become more popular over time. Given that our data source is a snapshot of cars for sale at one point in time and on one online website, this is unlikely to be representative of the entire population of cars in South Africa. However, it will be interesting to see the trends that do exist in this data.\nWe start with a simple plot of the number of cars advertised in the none more common colours, by year.\n\ndf_colours &lt;- df %&gt;% \n  group_by(year, colour) %&gt;% \n  summarise(n = n()) %&gt;%\n  ungroup() %&gt;% \n  filter(between(year, 1990, 2020)) \n\nIt makes sense that the majority of adverts are for cars that are produced recently, and it is striking that white and silver cars see such a dramatic increase, even relative to the third most common car colour, grey.\n\ndf_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n  filter(colour != \"Other\") %&gt;% \n  mutate(colour = fct_reorder(colour, n, .fun = sum, .desc = T)) %&gt;% \n  ggplot(aes(year, n, fill = colour)) +\n  geom_area() +\n  facet_wrap(~ colour)\n\n\n\n\n\n\n\n\n\n\n\nIn this plot we show the evolution of car colour between 1990 and the present.\nThere are a few problems with this plot. First, there are too many colours, the least common of which have just a few entries. Second, the missing values for the less common colours create unappealing white gaps in the bottom left of the plot.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(colour = fct_reorder(colour, nn, .fun = sum, .desc = T)) %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\n\n\n\n\nTo solve this problem, we can fill in the missing values by making the dataset wider and then long gain.\nUsing dplyr‚Äôs pivot_wider and the values_fill = 0 option, we create a wide dataset with one row for each year, shown below.\nWe go from a long dataset like this:\n\nhead(df_colours)\n\n# A tibble: 6 √ó 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      3\n\n\nTo a wide one, like this:\n\ndf_colours &lt;- df_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n    group_by(year, colour) %&gt;% \n    mutate(n = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  distinct() %&gt;% \n  pivot_wider(names_from = colour, values_from = n, values_fill = 0) \n\nhead(df_colours)\n\n# A tibble: 6 √ó 11\n   year Black  Blue  Gold Green  Grey Other   Red Silver White Burgundy\n  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n1  1990     2    14     3     3     5     4    10      3    10        0\n2  1991     3    11     2     4     4     3     9      3    12        1\n3  1992     1    16     2     4     5     7    11      3    14        1\n4  1993     0    13     2     1     4     1    13      7    18        1\n5  1994     2    15     0     8     7     1    13      6    41        6\n6  1995     6    36     0    17     6     3    32      7    34        7\n\n\nNow we have a ‚Äòwide‚Äô dataset with the missing values filled in with zeros\nWe can then make it long again, with the pivot_longer command, specifying that we want the original column names back.\n\ndf_colours &lt;- df_colours %&gt;% \n  pivot_longer(!year, names_to = \"colour\", values_to = \"n\")\n\nhead(df_colours)\n\n# A tibble: 6 √ó 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      4\n\n\n\n\n\nNow we have a plot with no white gaps, and an appropriate number of colours. However, it would be nice to have the colours on the plot match the colours of the cars, and have some labels to make it clear to the reader how to interpret the graphic.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\n\n\n\n\nWe use ggplot‚Äôs scale_fill_manual to specify the hex codes for the colours in the order that they appear in the plot.\nWe add in a title and axes labels, and make the plot interactive with ggplotly.\n\ng &lt;- df_colours %&gt;% \n  filter(year &gt; 1994) %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\n\n\nWow! Blue and Green cars really lost out to white and Silver ones over time. Of the cars advertised that were produced in 2020, nearly 42% are white!\nBlack cars seemed to have a peak in popularity around 2007, and then decline again, coinciding with the first round of load-shedding in South Africa.\nThanks for reading along, and I hope you enjoyed learning about ggplot as well as the cars on South Africa‚Äôs roads!\n\n\n\nHere is the static figure with accompanying caption.\n\n# jpeg(\n#     filename=\"figures/stacked_area_2.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_colours %&gt;%\n  filter(year &gt; 1994) %&gt;%\n  group_by(year) %&gt;%\n  add_tally(n) %&gt;%\n  ungroup() %&gt;%\n  mutate(share = n / nn) %&gt;%\n  mutate(colour = fct_relevel(colour, \"Silver\")) %&gt;%\n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#95A5A6\", \"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#EAECEE\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\n\n\n\n\n\n\n# dev.off()\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html#purpose",
    "href": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html#purpose",
    "title": "South African Car Colour Exploration",
    "section": "",
    "text": "While driving to Cape Town this past weekend in my beautiful white Hyundai i10, a German friend asked why every second car in South Africa was white in colour. While I didn‚Äôt have a compelling reason, I did have a some data to assess whether it was true that half of cars on the road are white.\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\nI show a useful trick for filling in missing data that creates gaps in the area chart using dplyr‚Äôs pivot_wider and pivot_longer commands.\nHere is the final interactive graphic:\n\ndf_plotly &lt;- read_rds(\"data/car_colour/df_plotly.rds\")\n\np_load(plotly)\n\ng &lt;- df_plotly %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\ndf &lt;- readRDS(\"data/car_colour/gumtree_ads_2020-10-01-11-33-am.rds\")\n\ndf &lt;- df %&gt;% filter(!is.na(make),\n                    !is.na(colour),\n                    !is.na(model)) %&gt;% \n             # Removing the duplicated province names   \n             mutate(province = str_remove(province, \"\\\\,.*\"))\n\n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n44644\n\n\nNumber of columns\n30\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n22\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nlink\n0\n1.00\n68\n86\n0\n2218\n0\n\n\nad_url\n0\n1.00\n78\n193\n0\n38392\n0\n\n\nsite\n0\n1.00\n7\n7\n0\n1\n0\n\n\nseller_type\n0\n1.00\n19\n30\n0\n2\n0\n\n\ntitle\n0\n1.00\n4\n100\n0\n26140\n0\n\n\ntext\n0\n1.00\n10\n3885\n0\n38343\n0\n\n\nad_date\n0\n1.00\n19\n19\n0\n42416\n0\n\n\nseller_name\n0\n1.00\n1\n50\n0\n8872\n0\n\n\nlocation\n0\n1.00\n24\n61\n0\n428\n0\n\n\nseller_age\n0\n1.00\n17\n22\n0\n79\n0\n\n\nn_all_time_ads\n0\n1.00\n1\n7\n0\n3481\n0\n\n\nscrape_time\n0\n1.00\n19\n19\n0\n35812\n0\n\n\nlocation_2\n0\n1.00\n11\n47\n0\n428\n0\n\n\nfor_sale_by\n226\n0.99\n5\n6\n0\n2\n0\n\n\nmake\n0\n1.00\n3\n13\n0\n37\n0\n\n\nmodel\n0\n1.00\n1\n20\n0\n459\n0\n\n\nbody_type\n226\n0.99\n3\n12\n0\n11\n0\n\n\ntransmission\n206\n1.00\n6\n9\n0\n2\n0\n\n\nfuel_type\n104\n1.00\n6\n8\n0\n4\n0\n\n\ncolour\n0\n1.00\n3\n8\n0\n16\n0\n\n\nna\n44644\n0.00\nNA\nNA\n0\n0\n0\n\n\nprovince\n0\n1.00\n7\n13\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npage\n0\n1.00\n14.51\n12.66\n1\n4.00\n11.0\n22.00\n50\n‚ñá‚ñÉ‚ñÇ‚ñÅ‚ñÅ\n\n\nad_number\n0\n1.00\n23570.95\n14487.85\n1\n11471.75\n22889.5\n34258.25\n56454\n‚ñá‚ñá‚ñá‚ñÜ‚ñÇ\n\n\nprice\n229\n0.99\n218706.68\n692268.63\n1\n109900.00\n165000.0\n259900.00\n140000000\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nn_photos\n1099\n0.98\n8.63\n3.08\n1\n7.00\n9.0\n12.00\n12\n‚ñÅ‚ñÇ‚ñÉ‚ñÉ‚ñá\n\n\nn_views\n0\n1.00\n588.85\n1281.69\n0\n91.00\n233.5\n580.00\n46956\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nn_active_ads\n0\n1.00\n130.20\n225.27\n1\n5.50\n29.0\n100.00\n997\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\nyear\n100\n1.00\n2013.76\n6.66\n1900\n2011.00\n2016.0\n2018.00\n2021\n‚ñÅ‚ñÅ‚ñÅ‚ñÅ‚ñá\n\n\nkilometers\n1189\n0.97\n92267.35\n157493.13\n1\n26890.50\n72000.0\n130000.00\n9999999\n‚ñá‚ñÅ‚ñÅ‚ñÅ‚ñÅ\n\n\n\n\n\nThe dataset has some really nice information on the cars advertised, including their characteristics, as well as the location and name of the seller.\nFor this graphic, we only need the car‚Äôs year and colour.\n\n\n\nWe start by plotting the number of cars in each colour advertised on Gumtree. In this plot I show the 12 most common colours.\nIt is evident that white and silver cars are very common. In as far as our data reflects cars on the road, white cars make up nearly 1/3 of all cars advertised! A little less than the half quoted in the discussion above, but certainly the dominant colour.\n\ndf %&gt;%\n  mutate(colour = fct_lump(colour, 12)) %&gt;%\n  select(colour) %&gt;% \n  count(colour, sort = T) %&gt;% \n  mutate(colour = fct_reorder(colour, n)) %&gt;% \n  ggplot(aes(n, colour)) +\n  geom_segment(aes(xend = 0, yend = colour)) +\n  geom_point(size = 4, colour = \"orange\") +\n  geom_text(aes(label = scales::comma(n)), hjust = -.3) +\n  scale_x_continuous(labels = scales::comma_format(), limits = c(0, 16000)) +\n  labs(x = \"Number of vehicles\",\n       title = \"What are the most common vehicle colours advertised on Gumtree?\",\n       subtitle = \"Data from 45,000 adverts from across the country\",\n       y = \"\")\n\n\n\n\n\n\n\n\n\n\n\nNext we want to know if some colours have become more popular over time. Given that our data source is a snapshot of cars for sale at one point in time and on one online website, this is unlikely to be representative of the entire population of cars in South Africa. However, it will be interesting to see the trends that do exist in this data.\nWe start with a simple plot of the number of cars advertised in the none more common colours, by year.\n\ndf_colours &lt;- df %&gt;% \n  group_by(year, colour) %&gt;% \n  summarise(n = n()) %&gt;%\n  ungroup() %&gt;% \n  filter(between(year, 1990, 2020)) \n\nIt makes sense that the majority of adverts are for cars that are produced recently, and it is striking that white and silver cars see such a dramatic increase, even relative to the third most common car colour, grey.\n\ndf_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n  filter(colour != \"Other\") %&gt;% \n  mutate(colour = fct_reorder(colour, n, .fun = sum, .desc = T)) %&gt;% \n  ggplot(aes(year, n, fill = colour)) +\n  geom_area() +\n  facet_wrap(~ colour)\n\n\n\n\n\n\n\n\n\n\n\nIn this plot we show the evolution of car colour between 1990 and the present.\nThere are a few problems with this plot. First, there are too many colours, the least common of which have just a few entries. Second, the missing values for the less common colours create unappealing white gaps in the bottom left of the plot.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(colour = fct_reorder(colour, nn, .fun = sum, .desc = T)) %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\n\n\n\n\nTo solve this problem, we can fill in the missing values by making the dataset wider and then long gain.\nUsing dplyr‚Äôs pivot_wider and the values_fill = 0 option, we create a wide dataset with one row for each year, shown below.\nWe go from a long dataset like this:\n\nhead(df_colours)\n\n# A tibble: 6 √ó 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      3\n\n\nTo a wide one, like this:\n\ndf_colours &lt;- df_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n    group_by(year, colour) %&gt;% \n    mutate(n = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  distinct() %&gt;% \n  pivot_wider(names_from = colour, values_from = n, values_fill = 0) \n\nhead(df_colours)\n\n# A tibble: 6 √ó 11\n   year Black  Blue  Gold Green  Grey Other   Red Silver White Burgundy\n  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n1  1990     2    14     3     3     5     4    10      3    10        0\n2  1991     3    11     2     4     4     3     9      3    12        1\n3  1992     1    16     2     4     5     7    11      3    14        1\n4  1993     0    13     2     1     4     1    13      7    18        1\n5  1994     2    15     0     8     7     1    13      6    41        6\n6  1995     6    36     0    17     6     3    32      7    34        7\n\n\nNow we have a ‚Äòwide‚Äô dataset with the missing values filled in with zeros\nWe can then make it long again, with the pivot_longer command, specifying that we want the original column names back.\n\ndf_colours &lt;- df_colours %&gt;% \n  pivot_longer(!year, names_to = \"colour\", values_to = \"n\")\n\nhead(df_colours)\n\n# A tibble: 6 √ó 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      4\n\n\n\n\n\nNow we have a plot with no white gaps, and an appropriate number of colours. However, it would be nice to have the colours on the plot match the colours of the cars, and have some labels to make it clear to the reader how to interpret the graphic.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\n\n\n\n\nWe use ggplot‚Äôs scale_fill_manual to specify the hex codes for the colours in the order that they appear in the plot.\nWe add in a title and axes labels, and make the plot interactive with ggplotly.\n\ng &lt;- df_colours %&gt;% \n  filter(year &gt; 1994) %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\n\n\nWow! Blue and Green cars really lost out to white and Silver ones over time. Of the cars advertised that were produced in 2020, nearly 42% are white!\nBlack cars seemed to have a peak in popularity around 2007, and then decline again, coinciding with the first round of load-shedding in South Africa.\nThanks for reading along, and I hope you enjoyed learning about ggplot as well as the cars on South Africa‚Äôs roads!\n\n\n\nHere is the static figure with accompanying caption.\n\n# jpeg(\n#     filename=\"figures/stacked_area_2.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_colours %&gt;%\n  filter(year &gt; 1994) %&gt;%\n  group_by(year) %&gt;%\n  add_tally(n) %&gt;%\n  ungroup() %&gt;%\n  mutate(share = n / nn) %&gt;%\n  mutate(colour = fct_relevel(colour, \"Silver\")) %&gt;%\n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#95A5A6\", \"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#EAECEE\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\n\n\n\n\n\n\n# dev.off()\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html",
    "title": "Swedish trade in goods and exchange students",
    "section": "",
    "text": "In a presentation last week organized by the Laboratory for the Economics of Africa‚Äôs Past I learned about the persistence of economic connectivity between areas from the Roman era until today. It was fascinating. The presenter, Prof.¬†Erik Hornung, mentioned offhand that the choice of where to spend a semester exchange was likely a function of the economic and social connections between your home country and your host country ‚Äì whether you know someone there, have family, or are familiar due to cultural similarities.\nI am in Sweden this semester, on an exchange at Lund University, in the South of the country. This is due in part to my supervisor‚Äôs connections with the economic historians here.\nI wanted to see if I could back up this assertion about exchange students with some data. More specifically, I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund from there, this semester.\nAs you see in the graphic below, there appears to be a strong positive correlation between Swedish imports of goods and imports of exchange students at Lund.\n\ndf_plotly &lt;- read_rds(\"data/Sweden_trade_exchange/plotly_data.rds\")\n\nc &lt;- df_plotly %&gt;% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent)) +\n  geom_smooth(group = 1, method = \"lm\", se = F) +\n  geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(title = \"Correlation between Sweden's imports of goods and exchange students\",\n       x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"Population\")\n\nc\n\n\n\n\n\n\n\n\nThe most populous European nations of Germany and France lead the rankings, with 46 and 37 exchange students respectively. The largest non-European senders are China and India, followed by the United States. South Africa has the largest representation for Africa, with 4 students.\nRead on to see the creation of the graphic."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#purpose",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#purpose",
    "title": "Swedish trade in goods and exchange students",
    "section": "",
    "text": "In a presentation last week organized by the Laboratory for the Economics of Africa‚Äôs Past I learned about the persistence of economic connectivity between areas from the Roman era until today. It was fascinating. The presenter, Prof.¬†Erik Hornung, mentioned offhand that the choice of where to spend a semester exchange was likely a function of the economic and social connections between your home country and your host country ‚Äì whether you know someone there, have family, or are familiar due to cultural similarities.\nI am in Sweden this semester, on an exchange at Lund University, in the South of the country. This is due in part to my supervisor‚Äôs connections with the economic historians here.\nI wanted to see if I could back up this assertion about exchange students with some data. More specifically, I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund from there, this semester.\nAs you see in the graphic below, there appears to be a strong positive correlation between Swedish imports of goods and imports of exchange students at Lund.\n\ndf_plotly &lt;- read_rds(\"data/Sweden_trade_exchange/plotly_data.rds\")\n\nc &lt;- df_plotly %&gt;% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent)) +\n  geom_smooth(group = 1, method = \"lm\", se = F) +\n  geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(title = \"Correlation between Sweden's imports of goods and exchange students\",\n       x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"Population\")\n\nc\n\n\n\n\n\n\n\n\nThe most populous European nations of Germany and France lead the rankings, with 46 and 37 exchange students respectively. The largest non-European senders are China and India, followed by the United States. South Africa has the largest representation for Africa, with 4 students.\nRead on to see the creation of the graphic."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#data",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#data",
    "title": "Swedish trade in goods and exchange students",
    "section": "Data",
    "text": "Data\nTo answer my question, I link two pieces of data: trade statistics on Sweden‚Äôs imports, and the nationalities of the incoming exchange students at Lund.\n\nTrade data\nThe first I downloaded from an amazing trade tool called the Observatory of Economic Complexity (Linked here). It has wonderful visualizations of trade by type of goods and by country. Have a look at this tree map below for an example.\n\n\n\n\nTree Map\n\n\nThe tree map shows the origins of Sweden‚Äôs imports by value in 2018. Sweden cares about limiting carbon emissions, and so it makes sense that the majority of their imports are sourced within Europe.\n\n\n\nData on exchange students\nThe second data source is a rough proxy for the nationalities of my colleagues. I collected phone numbers from a big WhatsApp group called ‚ÄúLund University ‚Äô21‚Äù and extract the international dialing codes. There are several shortcomings to this data source ‚Äì self-selection into WhatsApp use may differ by country, as might the desire to be part of a large group. Further, WhatsApp groups are limited in size at 256 members, just more than half the total number of exchange students at Lund this semester. My sample is unlikely to perfectly represent my population of interest, but it is a good enough starting point.\n\nData processing\nIn the chunks of code below I scrape a list of international dialing codes from the web, along with other country level information including GDP per capita and land area.\n\n# website with data on dialing codes\nurl &lt;- \"https://countrycode.org/\"\n\n# scrape table with rvest\ntable &lt;- read_html(url) %&gt;% \n  html_nodes(\"table\") %&gt;% \n  html_table()\n\n# processing\ntable &lt;- table[[1]] %&gt;% as_tibble(.name_repair = \"minimal\") %&gt;% unnest()\n\ntable &lt;- table %&gt;%  as_tibble() %&gt;% janitor::clean_names()\n\n# cleaning names and formatting columns as numbers rather than characters\ntable_df &lt;- table %&gt;% \n  mutate(population = parse_number(population),\n         area_km2 = parse_number(area_km2),\n         gdp_usd = parse_number(gdp_usd),\n         gdp_usd = gdp_usd*10e9)\n\nHere is a table of the data scraped from countrycode.org.\n\n# display table\ndatatable(table_df)\n\n\n\n\n\nWe can show the regions of the world by the first digit of their dialing code in a neat map. Expand the chunk by clicking code to see how easy it is to make an interactive graphic with ggplotly.\n\n# packages for map plotting and matching countries.\np_load(rnaturalearth, countrycode)\n\n# extracting first digit of dialing code\ntable_df &lt;- table_df %&gt;% \n  mutate(iso_a3 = countrycode(country, origin = \"country.name\", destination = \"iso3c\")) %&gt;% \n  mutate(first_digit = substring(country_code, 1, 1))\n\n# creating dataframe with mapping geometry\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# joining up to table of dialing codes\nmap_df &lt;- world %&gt;% as_tibble() %&gt;% left_join(table_df, by = \"iso_a3\")\n\n# creating plot\na &lt;- map_df %&gt;% \n  filter(!is.na(first_digit)) %&gt;% \n  ggplot(aes(geometry = geometry, fill = first_digit)) +\n  geom_sf() +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(title = \"Countries coloured by first digit of international dialing code\",\n       fill = \"First digit\")\n\n# display interactive plot\nggplotly(a)\n\n\n\n\n\n\n\nDialing codes\nThe trickiest part was matching a country name with an international dialing code from the number alone. I used Google‚Äôs open source library for parsing, formatting, and validating international phone numbers. It is written in Java but someone has kindly written a wrapper for R. I show the process in the code chunk below, but do not display the phone numbers themselves for privacy reasons.\n\n# Sys.setenv(JAVA_HOME='C:\\\\Program Files\\\\Java\\\\jre1.8.0_281')\n# install.packages(\"dialrjars\", INSTALL_opts = c(\"--no-multiarch\"))\n\nlibrary(dialrjars)\nlibrary(dialr)\n\ndf &lt;- read_excel(\"data/Sweden_trade_exchange/Whatsapp numbers.xlsx\")\n\n# uniform formatting of numbers\ndf &lt;- df %&gt;% \n  mutate(clean = ifelse(str_detect(raw, \"\\\\+.*\"), raw, str_c(\"+\", raw))) %&gt;% select(number = clean)\n\ndf &lt;- df %&gt;% \n  mutate(phone = phone(number, \"SE\"))\n\n# get the region from number\ndf &lt;- df %&gt;% \n  mutate(region = get_region(phone))\n\n# count number of phone numbers per country\ncounts &lt;- df %&gt;% count(region, sort = T)\n\n# match name of country to two letter country code\ncounts &lt;- counts %&gt;% \n  mutate(name = countrycode(region, origin = \"iso2c\", destination = \"country.name\"),\n         iso_a3 = countrycode(region, origin = \"iso2c\", destination = \"iso3c\"))\n\ndatatable(counts)\n\n\n\n\n\nA quick plot of counts. Wow! Look at that. Go Germany!\n\ndf_col &lt;- counts %&gt;% \n  left_join(world %&gt;% as_tibble(), by = \"iso_a3\") %&gt;% \n  rename(name = name.x)\n\ndf_col %&gt;%   \n  filter(name != \"Sweden\") %&gt;% \n  mutate(name = fct_reorder(name, n),\n         continent = factor(continent, levels = c(\"Europe\", \"Asia\", \"North America\", \"Africa\", \"South America\"))) %&gt;% \n  ggplot(aes(n, name, fill = continent)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Number of exchange students in Lund Uni exchange WhatsApp group\",\n       subtitle = \"By country\",\n       y = \"\",\n       x = \"Number of exchange students\",\n       fill = \"Continent\")"
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#findings",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#findings",
    "title": "Swedish trade in goods and exchange students",
    "section": "Findings",
    "text": "Findings\n\nPlotting the correlation between imports of goods to Sweden and exchange students to Lund\nI plot the correlation between the value of Sweden‚Äôs imports in 2018 on the x-axis, and the number of exchange students on the y-axis. Both axes are on a log scale.\nBelow is an interactive version of the static plot above.\n\nc &lt;- df_plotly %&gt;% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent, text=paste(\"Country:\", name))) +\n  geom_smooth(group = 1, method = \"lm\", se = T) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"\",\n       colour = \"\")\n\n\nggplotly(c)\n\n\n\n\n# static figure\n\n\n# map_df_short &lt;- map_df %&gt;%\n#   select(iso_a3, continent, pop_est, gdp_md_est)\n# \n# jpeg(\n#   filename= \"data/Sweden_trade_exchange/reg_2.jpg\",\n#   width = 8,\n#   height = 6,\n#   units = \"in\",\n#   res = 500\n# )\n# \n# df %&gt;%\n#   inner_join(map_df_short) %&gt;%\n#   filter(iso_a3 != \"SWE\",\n#          Continent != \"Antarctica\") %&gt;%\n#   rename(exchange_students = n) %&gt;%\n#   ggplot(aes(trade_value, exchange_students)) +\n#   geom_point(aes(size = pop_est, colour = Continent)) +\n#   geom_smooth(group = 1, method = \"lm\", se = F) +\n#   geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n#   scale_y_log10() +\n#   scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n#   scale_size_continuous(labels = scales::comma_format()) +\n#   scale_color_brewer(palette = \"Paired\") +\n#   geom_hline(yintercept = 0, lty = 2) +\n#   labs(x = \"Value of imports into Sweden in 2018 (USD)\",\n#        y = \"Number of exchange students in Lund Whatsapp group\",\n#        size = \"Population\")\n# \n# dev.off()\n\nThank you for reading along!"
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html",
    "href": "posts/2021-04-03-container-ships/container-ships.html",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "This week we learned about a data leak at Facebook which took place in 2019, where more than 500 million phone numbers, email addresses and names were scraped from the site and leaked online. Then, on Thursday we heard about another 500 million records including names, email addresses and more personal details were scraped from Linkedin, though the company argues that this was not a data breach. If you want to learn more about scraping, and get in on the (non-nefarious) action yourself, read along. As a bonus you will learn about how cargo ships have become so large.\nIn this post I want to show how easy it is to scrape data from the internet. It is the first post in a series looking at ships. I walk through scraping data from Wikipedia, one of the best places on the internet to ingest tabular data from.\nBefore we begin with the scraping walkthrough, I want to visualize the data and show how cargo ships have become larger over time.\n\n\nThe small multiples plot below shows the evolution of cargo ship size from 1870 to today. On the x-axis is the length of the average cargo ship per decade from Wikipedia‚Äôs list of cargo ships1. On the y-axis is the average ship beam, or width at the widest point. The colour of the rectangle shows the deadweight tonnage of the average ship, or amount of cargo that the ship can carry.\nDeadweight tonnage or tons deadweight is a measure of how much weight a ship can carry. It is the sum of the weights of cargo, fuel, fresh water, ballast water, provisions, passengers, and crew.\n\n\n\n\n\n\n\n\n\nCargo ships have increased dramatically in size over time! The oldest ship in our dataset is the R. J. Hackett, one of the first Great Lakes freighters. It was just 63m long and 10m wide, with a wooden hull. According to historian Mark Thompson, the R. J. Hackett‚Äôs boxy hull, hatch-lined deck, and placement of the deckhouses meant the ship was ideally suited for moving cargo through inland waterways. This steamer greatly influenced the development of cargo ships which followed.\nToday, container ships like the Ever Given are nearly 400m long, 60m wide, and can carry more than 20,000 twenty-foot containers. That‚Äôs enough space for 745 million bananas!\nTEUs, or twenty-foot equivalent units, is the measure that shipping companies use to compare volume. A TEU is 6.1m long, 2.4m wide and usually 2.6m high. Source: Wikipedia\n\n\n\nIn the plots below we focus only on container ships built after 1970. This era saw the construction of the first ships purpose built to carry ISO containers, which could be loaded and unloaded rapidly at port, repacked and shipped onward on any compatible ship, truck or train. The ISO standard container transformed the shipping industry and replaced the prior status quo of break bulk carriers. One of the consequences was a dramatic reduction in demand for ‚Äústevedores‚Äù or ‚Äúlongshoremen‚Äù; workers who would manually unload cargo from break bulk carriers.\nIf you‚Äôre interested in containerization, I highly reccomend this episode from the podcast Trade Talks, and this eight-part series from Alexis Madrigal.\n\n\n\n\nHow have cargo ship deadweight tonnages, or how much cargo a ship can carry, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships can carry more cargo today than ever before. It‚Äôs hard to get my mind around 220 000 tons of cargo!\n\n\n\nHow has the length of cargo ships changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nThe Ever Given is among the longest container ships operating today at 400m in length. The linear fit line shows that there has been a steady increase in container ship length over time.\n\n\n\nHow have cargo ship beams, or widths of ships at their widest point, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships have also become wider, with lumping at beams of 32m, 40m and 59m.\n\n\n\n\nSo it certainly seems that cargo ships have been becoming larger over time. Interestingly, it appears that while the largest container ships continue to get larger and carry more cargo, there is still a need for relatively small ships. There are a significant number of container ships that can carry less than 50 000 tons launched since 2010, shown in the density plot below. We could say that there has been a bifurcation in ship capacity, with a few enormous ships, and a greater number of smaller ships operating in tandem today.\n\n\n\n\n\n\n\n\n\n\n\n\nEconomies of scale describes a decrease in the per unit cost as scale of an operation increases. This perfectly fits the shipping industry‚Äôs relentless path towards upsizing ships, cranes and ports. One of the reasons has to do with fluid dynamics. Hull resistance is one of the key factors impacting fuel consumption. For vessels in water, drag loss is less than proportional to increasing cargo carried. In other words, making a ship larger usually results in less fuel consumption per ton of cargo, holding all else constant. As we have seen in the figures above, container ships have become larger and larger as they carry more cargo per ship, in an effort to save fuel.\nOther methods of drag reduction include improved hull design, injecting air around the hull surface and reducing hull roughness from slime and weeds. See Resistence and powering of ships \nAccording to Marc Levinson, author of The Box: How the Shipping Container Made the World Smaller and the World Economy Bigger, the shippers applied this idea to every element of the industry. He says:\n\nBigger ships lowered the cost of carrying each container. Bigger ports with bigger cranes lowered the cost of handling each ship. Bigger containers ‚Äî the 20-foot box, shippers‚Äô favorite in the early 1970s, was yielding to the 40-footer ‚Äî cut down on crane movements and reduced the time needed to turn a vessel around in port, making more efficient use of capital. A virtuous circle had developed: lower costs per container permitted lower rates, which drew more freight, which supported yet more investments in order to lower unit costs even more. If ever there was a business in which economies of scale mattered, container shipping was it.\n\nThe consequences of containerization are fascinating ‚Äì including rapidly falling costs of trade, increasingly intermeshed global supply chains, a proliferation of robots at ports, and the environmental challenges associated with ships, trucks and trains meeting at transshipping nodes around the world.\nIn the remainder of this post I walk through scraping some of the data presented above."
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#tonnage-over-time",
    "href": "posts/2021-04-03-container-ships/container-ships.html#tonnage-over-time",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "How have cargo ship deadweight tonnages, or how much cargo a ship can carry, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships can carry more cargo today than ever before. It‚Äôs hard to get my mind around 220 000 tons of cargo!"
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#length-over-time",
    "href": "posts/2021-04-03-container-ships/container-ships.html#length-over-time",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "How has the length of cargo ships changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nThe Ever Given is among the longest container ships operating today at 400m in length. The linear fit line shows that there has been a steady increase in container ship length over time."
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#width-over-time",
    "href": "posts/2021-04-03-container-ships/container-ships.html#width-over-time",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "How have cargo ship beams, or widths of ships at their widest point, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships have also become wider, with lumping at beams of 32m, 40m and 59m."
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#footnotes",
    "href": "posts/2021-04-03-container-ships/container-ships.html#footnotes",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI focus on all cargo ships until the 1970s, whereafter I focus only on container ships. Additional data on container ships is scraped from VesselFinder.com‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "",
    "text": "Data visualisation is super fun and a great tool for communication. I have put together some slides for talk on data visualisation for economic historians at the Laboratory for the Economics of Africa‚Äôs Past. These resources are collected here to help when you think ‚Äòwhere do I start with R?‚Äô.\nI hope that you find these resources useful. They are divided up into different sections in the tabs below.\n\n\nThere are a multitude of different pieces of software and different workflows that you can use to create beautiful and intelligible data visualizations.\nI think R is a very good choice. There are three reasons I love R for data viz.\n\nggplot2 is a fantastic library for making data visualizations - it is customizable and logical.\nThe R community is very supportive and makes learning and improving a pleasure.\nThe entire process of importing and cleaning data, visualizing it, and then sharing it is made easy in Rstudio."
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#purpose",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#purpose",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "",
    "text": "Data visualisation is super fun and a great tool for communication. I have put together some slides for talk on data visualisation for economic historians at the Laboratory for the Economics of Africa‚Äôs Past. These resources are collected here to help when you think ‚Äòwhere do I start with R?‚Äô.\nI hope that you find these resources useful. They are divided up into different sections in the tabs below.\n\n\nThere are a multitude of different pieces of software and different workflows that you can use to create beautiful and intelligible data visualizations.\nI think R is a very good choice. There are three reasons I love R for data viz.\n\nggplot2 is a fantastic library for making data visualizations - it is customizable and logical.\nThe R community is very supportive and makes learning and improving a pleasure.\nThe entire process of importing and cleaning data, visualizing it, and then sharing it is made easy in Rstudio."
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#resources",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#resources",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "Resources",
    "text": "Resources\nIn the tabs below, I have collected some resources that I hope will be beneficial on your journey.\n\n\nInstalling R\n\nGetting started\nBefore we can do the fun stuff, we need to install R and RStudio.\nThe Modern Dive textbook has a great primer on getting started in R with an explanation of the difference between R and RStudio:\n\n\n\nModern Dive analogy\n\n\nThe Rstudio Education team also have a great beginner guide to R that covers installation.\n\n\n\nLearning R\n\nVideos and textbooks for learning how to use R\n\nThe R bible\nThe definitive guide to using R in a modern manner is R for Data Science by Hadley Wickham and Garrett Grolemund. I am not sure that is the best place to start, but it is certainly a helpful resource to come back to once you‚Äôre up and running.\n\n\n\nThe R for Data Science textbook\n\n\n\n\nRStudio Education\nRstudio Education have put together a set of instructions for learning R at different levels of competency. These include great guides for installing R, R Studio and the tidyverse packages right at the beginning.\nThe beginner, intermediate and expert start guides are linked here.\nThe RStudio team have put a lot of effort into creating resources that are useful for many R users, from helping teachers take their course work online, to financial analysis, to creating a personal website and academic publishing. Have a look around their website for more.\n\n\n\nRstudio Education graphics\n\n\n\n\nOther great video resources\nI am a visual learner and as a consequence I like to watch how things are done and copy them. Because Youtube allows for pausing and recapping at your own pace, I think this is a great way to learn. I‚Äôve collected some videos in these resources which I think are a good place to start.\nDavid Robinson‚Äôs Youtube Channel\nDavid Robinson does a weekly screencast where he shows how to do exploratory data analysis and makes plots in R.\nHere is a video about analysing text from reviews of ramen noodles. It has some useful tips for drawing tie-fighter plots and interpreting linear regressions.\n\n\nHe also has a talk about tricks in the tidyverse.\n\n\nJulia Silge‚Äôs blog\nJulia Silge‚Äôs blog is mostly about doing modelling in R, and text modelling in particular. It is a very useful place to look if you want to understand the tidymodels framework which provides access to a host of machine learning techniques in a very easy to use manner. She also has a Youtube channel.\nHere is a video from Julia looking at sentiment analysis in R:\n\n\nThe tidy text mining textbook\nWritten by Julia Silge and David Robinson, this book gives a lovely introduction to text mining in R. It has lots of examples and includes all the code necessary. As a bonus it is free!\n\n\n\nTidy Text Mining Textbook\n\n\nAndrew Couch‚Äôs Youtube Channel has a marvelous set of walk throughs of innovative and exciting packages in R. He‚Äôs a young data scientist who is creating fantastic content focussed on R and the tidyverse.\nHere is one of his videos on creating functions with the tidyverse.\n\n\n\n\nAnother resource collection!\nThe LearnR4Free website compiled by Mine Dogucu has a great set of resources that are free to use for the learner.\n\n\n\n\nDatasets\n\nUseful sources of data to practice your skills on\nHaving great datasets available helps to keep me interested in learning new skills. Here are three sources of interesting data:\nThe Tidy Tuesday Project\nTidy Tuesday is a project run by the R for Data Science foundation - each week they share a real world data set and encourage R users to practice their skills. It has a really fantastic community aspect and if you share your work on Twitter people are always helpful and kind. Others share their work and their code so you can see how their plots were made. In their words,\n\nThe intent of Tidy Tuesday is to provide a safe and supportive forum for individuals to practice their wrangling and data visualization skills independent of drawing conclusions.\n\n\n\n\nTidy Tuesday\n\n\nData is Plural\n‚ÄúData is Plural is a weekly newsletter of useful/curious datasets, published by Jeremy Singer-Vine‚Äù. This sums it up - really interesting stuff from all over the internet.\n\n\n\nData is plural\n\n\nNaked Data\n‚ÄúNaked Data is a weekly newsletter which rounds up the best in data-driven storytelling created in or about the Global South.‚Äù Get the best bits of twitter without having to spend all day scrolling. Really great newsletter based in SA.\n\n\n\nNaked Data\n\n\n\n\n\nVisualization\n\nGreat places to get inspiration, figures and code from\n\nFundementals of data visualization\nThere are many books on the theory and practice of data visualization. I think Fundamentals of Data Visualization by Claus Wilke is an excellent choice to start with. The preface reads:\n\nIf you are a scientist, an analyst, a consultant, or anybody else who has to prepare technical documents or reports, one of the most important skills you need to have is the ability to make compelling data visualizations, generally in the form of figures. Figures will typically carry the weight of your arguments. They need to be clear, attractive, and convincing.\n\nThe book provides great advice for matching your data to a good chart type. For how to make the chosen chart in R, have a look at his course, linked below.\n\n\n\nThe best course in data viz\nClause Wilke‚Äôs Course on Data Visualization in R is a fantastic accompaniment to the book linked above. The description reads:\n\nIn this class, students will learn how to visualize data sets and how to reason about and communicate with data visualizations. A substantial component of this class will be dedicated to learning how to program in R. In addition, students will learn how to compile analyses and visualizations into reports, how to make the reports reproducible, and how to post reports on a website or blog.\n\nIt has wonderful sets of slides including this delightful primer on visualizing uncertainty, and accompnaying code that can be downloaded and worked through. Bonus: all of the slides are hosted on github such that you can see how they‚Äôre put together.\n\n\nThe ggplot2 bible\nThe definitive guide to ggplot2, The ggplot2 Book is a fantastic resource for novices and experts alike. It is easy to navigate and has helpful examples with code used to create each figure. (I love to consult the section on annotations as I never remember exactly how they work for some reason.)\n\n\n\nThe ggplot2 book\n\n\n\n\nOther great viz resources\nThe Storytelling with Data blog has an excellent selection of articles that give tips and tricks on how to design graphics that are easy to understand.\nTheir Chart Guide is also a fantastic starting point for how to pick a chart.\n\n\n\nSWD Chart Guide\n\n\nJonathan Tan‚Äôs exploration of Household Solar Energy in the US is an amazing data storytelling piece that shows all of the code required to create each figure. It makes great use of annotations to explain each component of the visualizations.\n\n\n\nJonathan Tan‚Äôs ggridge plot\n\n\nThe R Graph Gallery is a useful website which showcases different kinds of visualizations and provides the code that you need to use to recreate the figure yourself.\n\n\n\nR Graph Gallery\n\n\n\n\n\n\nCustom themes\n\nFor style that is consistent and graphics that are clear\nIf you want to create a style of graphics that communicate clearly to the general public (or are consistent across your organization), have a look at the BBC Visual and Data Journalism cookbook for R graphics. The guide is filled with fantastic examples of visualizations you might like to emulate, and code to do so. They also walk through customizing fonts, colours and annotations for consistency over time.\n\n\n\nBBC style graphics\n\n\nIf you want to create your own theme, Thomas Mock has a lovely guide on graphs that communicate well and look good.\nHe walks through a great example of where adding custom images on the plot makes sense:\n\n\n\nNFL team images example\n\n\n\n\n\nAesthetics\n\nIf you‚Äôre interested in learning more about how to make your charts look great.\nWilliam Chase‚Äôs talk on ‚ÄòThe Glamour of Graphics‚Äô is a fantastic resource on choosing colours and how to make your graphs pop.\n\n\nStephen Tracy‚Äôs breakdown of DataViz Sins You Should Never Commit is a walk-through on how not to confuse your audience. It explains the use of colour and secondary axes in Microsoft Excel plots.\nIf you are interested in learning more about fonts, Practical Typography is a great place to start. If you‚Äôre interested in choosing a font for a website, have a look at Google‚Äôs free fonts here.\n\n\n\nLato is a free Google font\n\n\nThis page from the R Graph Gallery gives a breakdown of the RcolorBrewer package and when to use its different sets of palettes.\n\nThe color brewer webpage gives fantastic advice on how to choose colours for maps, using the same theory as the package above. It also gives you the hex codes for each colour that you may copy to use yourself.\n\nThis video on colour theory is a useful primer on colour in data viz.\n\n\n\n\n\nEconometrics in R\n\nIf you‚Äôre looking to implement econometric techqniques in R\nGrant McDermott‚Äôs Data science for economists course is a great place to start. He has one lecture on regression that is super useful.\n\n\n\nGrant McDermott\n\n\n\n\n\nR Shiny\n\nIf you‚Äôre looking to make interactive web pages with R and R Markdown\nThe simplest way to get a quick and dirty interactive dashboard together is to use the flexdashboard package. Here is a link to the package. One of the great advantages of flexdashboard is that you can get amazing interactivity with just htmlwidgets, you do not need to use a shiny-runtime. In other words, you can create and host a static html page anywhere, for example on GitHub Pages.\n\n\n\nFlexdashboard\n\n\nIf you want more interactivity, David Robinson‚Äôs video on global crop yields shows a simple Shiny app, with code here.\n\n\nAnother video example of R Shiny comes from Andrew Couch‚Äôs YouTube channel.\n\n\nMastering Shiny is a more advanced authoritative book on Shiny, written by Hadley Wickham. It‚Äôs a great place to get an understanding of the core of the package.\n\n\n\nMastering Shiny\n\n\n\n\n\nRecreating Published Figures\n\nWhen you can‚Äôt get the authors of an article to share their data\nSometimes you may want to recreate a figure for your own paper from a journal article, but the data is not publicly accessible.\nOne option is to snip the image from the article and cite it in your own. The disadvantage of this is that the figure will not fit your colour scheme and may be horribly pixalated.\nAn alternative is to recreate the figure yourself by extracting the data from the image.\nA truly fantastic free tool for this purpose is WebPlotDigitizer. This web application allows you to upload a figure and grab the data from it.\nHave a look at a short video I made on how to use it here:"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Fathom data are an analytics and data science training firm which offers training in two of the most popular open source languages for statistical and general purpose programming: #Rstats and #Python. In their words:\n\n\nWe use 100% #opensource tools and we contribute to a number of Open Source projects. There's no doubt that Open Source adds value to @fathomdata, but it's hard to know what value we're creating. Some interesting perspectives on quantifying value.https://t.co/hcw5hylTwn\n\n‚Äî Fathom Data (@fathomdata) January 24, 2022\n\n\nSome of the staff state a preference for R, and I wanted to see if this preference was borne out in the tweets on their corporate twitter account.\n\n\nTo quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n# get the tweets!\ndf &lt;- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\n\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew‚Äôs account.\n\ndf %&gt;% \n    count(screen_name) %&gt;% \n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\n\n\n\n\nAccount\nNumber of tweets\n\n\n\n\ndatawookie\n3198\n\n\nfathomdata\n448\n\n\n\n\n\n\n\n\nLet‚Äôs start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\n\nhashtags &lt;- df %&gt;%\n  filter(screen_name == \"fathomdata\") %&gt;%\n  unnest(cols = c(hashtags)) %&gt;%\n  filter(!is.na(hashtags)) %&gt;%\n  mutate(hashtags = str_to_lower(hashtags)) %&gt;%\n  count(hashtags, is_retweet) %&gt;%\n  mutate(is_retweet = if_else(is_retweet == TRUE,\n    \"Retweet\",\n    \"@fathomdata tweet\"\n  ))\n\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\nI‚Äôve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\n\nlibrary(tidytext)\n# ggsci package for nice blue fill scale\nlibrary(ggsci)\n\nhashtags %&gt;% \n  group_by(is_retweet) %&gt;% \n  slice_max(n, n = 12, with_ties = F) %&gt;% \n  ungroup() %&gt;% \n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %&gt;% \n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %&gt;% \n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\n  geom_col(show.legend = F) +\n  scale_fill_material(\"blue\") +\n  scale_color_manual(values = c(\"white\", \"red\")) +\n  scale_y_reordered() +\n  facet_wrap(~ is_retweet, scales = \"free\") +\n  labs(x = \"Number of tweets tagging\",\n       y = \"Hashtag\",\n       caption = \"Note: red outline on bar indicates language\")\n\n\n\n\nColumn chart of hashtag frequency by tweet type\n\n\n\n\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\n\nhashtags %&gt;%\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %&gt;%\n  mutate(hashtags = fct_reorder(hashtags, n),\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %&gt;%\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\n  geom_col(position = \"fill\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Share of tweets tagging langauge\",\n    fill = \"Language\",\n    y = NULL\n  ) +\n  scale_fill_npg(\n    guide = guide_legend(reverse = TRUE)\n  )\n\n\n\n\nStacked column chart comparing language hashtag frequency by tweet type\n\n\n\n\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\n\ndf %&gt;%\n  select(text) %&gt;%\n  mutate(\n    rstats = str_detect(text, \"rstats\"),\n    python = str_detect(text, \"python\")\n  ) %&gt;%\n  summarise(across(c(rstats, python), sum)) %&gt;%\n  pivot_longer(everything(),\n    names_to = \"language\",\n    values_to = \"n\"\n  ) %&gt;% \n    mutate(share = scales::percent(n / (496 + 134)))\n\n# A tibble: 2 √ó 3\n  language     n share\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;\n1 rstats     496 79%  \n2 python     134 21%  \n\n\n\n\n\nIn the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\n\ndf %&gt;%\n  filter(\n    str_detect(text, \"rstats\"),\n    screen_name == \"fathomdata\",\n    !is.na(retweet_screen_name)\n  ) %&gt;%\n  count(retweet_name, sort = T) %&gt;%\n  slice_head(n = 12) %&gt;%\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\n\n\n\n\nAccout name\nNumber of retweets\n\n\n\n\nAndrew B. Collier\n43\n\n\nR-bloggers\n19\n\n\nMegan Beckett\n16\n\n\nR posts you might have missed!\n10\n\n\nsatRday South Africa\n6\n\n\nMara Averick\n5\n\n\nAstrid Radermacher\n3\n\n\nRStudio\n3\n\n\nTom Mock\n3\n\n\nFathom Data\n2\n\n\nR-Ladies Cape Town\n2\n\n\nR Weekly Live\n2\n\n\n\n\n\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap.\n\n\n\nFollowing the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\n\n# taking out URLs\nremove_bits &lt;- \"&amp;|&lt;|&gt;\"\n\ntidy_tweets &lt;- df %&gt;%\n  filter(!str_detect(text, \"^RT\")) %&gt;%\n  mutate(text = str_remove_all(text, remove_bits)) %&gt;%\n  unnest_tokens(word, text, token = \"tweets\") %&gt;%\n    # remove stopwords\n  filter(\n    !word %in% stop_words$word,\n    !word %in% str_remove_all(stop_words$word, \"'\"),\n    str_detect(word, \"[a-z]\")\n  ) \n\nfrequency &lt;- tidy_tweets %&gt;% \n  group_by(screen_name) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(tidy_tweets %&gt;% \n              group_by(screen_name) %&gt;% \n              summarise(total = n())) %&gt;%\n  mutate(freq = n/total)\n\nfrequency &lt;- frequency %&gt;% \n  select(screen_name, word, freq) %&gt;% \n  pivot_wider(names_from = screen_name, values_from = freq) %&gt;%\n  arrange(desc(datawookie), fathomdata)\n\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew‚Äôs tweets.\n\nfrequency %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  theme(legend.position = \"none\") +\n    labs(x = \"Andrew Collier\",\n         y = \"Fathom Data\")\n\n\n\n\nComparing the frequency of words used by the Fathom Data account and Andrew Collier\n\n\n\n\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\n\nfrequency %&gt;% \n  mutate(highlight = word == \"@standardbankza\") %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\nHighlighting Standard Bank account mentions\n\n\n\n\nThis is clearly the case! Have a look at the top terms, haha!\n\nfrequency %&gt;%\n  filter(is.na(fathomdata)) %&gt;%\n  select(-fathomdata) %&gt;%\n  mutate(datawookie = scales::percent(datawookie)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\n\n\n\n\nWord\nRelative specificity to Andrew Collier‚Äôs tweets\n\n\n\n\nkak\n0.6723%\n\n\n#customerservice\n0.6666%\n\n\n@rainsouthafrica\n0.5759%\n\n\nmind\n0.5220%\n\n\nmbps\n0.4255%\n\n\nprepared\n0.3773%\n\n\nlinux\n0.3489%\n\n\n#itcanbe\n0.3262%\n\n\n#fixit\n0.2354%\n\n\nworst\n0.2241%\n\n\n\n\n\n\n\n\nIt appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats."
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#tweets",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#tweets",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "To quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n# get the tweets!\ndf &lt;- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\n\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew‚Äôs account.\n\ndf %&gt;% \n    count(screen_name) %&gt;% \n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\n\n\n\n\nAccount\nNumber of tweets\n\n\n\n\ndatawookie\n3198\n\n\nfathomdata\n448"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#hashtags",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#hashtags",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Let‚Äôs start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\n\nhashtags &lt;- df %&gt;%\n  filter(screen_name == \"fathomdata\") %&gt;%\n  unnest(cols = c(hashtags)) %&gt;%\n  filter(!is.na(hashtags)) %&gt;%\n  mutate(hashtags = str_to_lower(hashtags)) %&gt;%\n  count(hashtags, is_retweet) %&gt;%\n  mutate(is_retweet = if_else(is_retweet == TRUE,\n    \"Retweet\",\n    \"@fathomdata tweet\"\n  ))\n\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\nI‚Äôve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\n\nlibrary(tidytext)\n# ggsci package for nice blue fill scale\nlibrary(ggsci)\n\nhashtags %&gt;% \n  group_by(is_retweet) %&gt;% \n  slice_max(n, n = 12, with_ties = F) %&gt;% \n  ungroup() %&gt;% \n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %&gt;% \n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %&gt;% \n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\n  geom_col(show.legend = F) +\n  scale_fill_material(\"blue\") +\n  scale_color_manual(values = c(\"white\", \"red\")) +\n  scale_y_reordered() +\n  facet_wrap(~ is_retweet, scales = \"free\") +\n  labs(x = \"Number of tweets tagging\",\n       y = \"Hashtag\",\n       caption = \"Note: red outline on bar indicates language\")\n\n\n\n\nColumn chart of hashtag frequency by tweet type\n\n\n\n\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\n\nhashtags %&gt;%\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %&gt;%\n  mutate(hashtags = fct_reorder(hashtags, n),\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %&gt;%\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\n  geom_col(position = \"fill\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Share of tweets tagging langauge\",\n    fill = \"Language\",\n    y = NULL\n  ) +\n  scale_fill_npg(\n    guide = guide_legend(reverse = TRUE)\n  )\n\n\n\n\nStacked column chart comparing language hashtag frequency by tweet type\n\n\n\n\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\n\ndf %&gt;%\n  select(text) %&gt;%\n  mutate(\n    rstats = str_detect(text, \"rstats\"),\n    python = str_detect(text, \"python\")\n  ) %&gt;%\n  summarise(across(c(rstats, python), sum)) %&gt;%\n  pivot_longer(everything(),\n    names_to = \"language\",\n    values_to = \"n\"\n  ) %&gt;% \n    mutate(share = scales::percent(n / (496 + 134)))\n\n# A tibble: 2 √ó 3\n  language     n share\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;\n1 rstats     496 79%  \n2 python     134 21%"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#who-is-being-retweeted",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#who-is-being-retweeted",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "In the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\n\ndf %&gt;%\n  filter(\n    str_detect(text, \"rstats\"),\n    screen_name == \"fathomdata\",\n    !is.na(retweet_screen_name)\n  ) %&gt;%\n  count(retweet_name, sort = T) %&gt;%\n  slice_head(n = 12) %&gt;%\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\n\n\n\n\nAccout name\nNumber of retweets\n\n\n\n\nAndrew B. Collier\n43\n\n\nR-bloggers\n19\n\n\nMegan Beckett\n16\n\n\nR posts you might have missed!\n10\n\n\nsatRday South Africa\n6\n\n\nMara Averick\n5\n\n\nAstrid Radermacher\n3\n\n\nRStudio\n3\n\n\nTom Mock\n3\n\n\nFathom Data\n2\n\n\nR-Ladies Cape Town\n2\n\n\nR Weekly Live\n2\n\n\n\n\n\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap."
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#what-is-the-overlap-between-private-and-public",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#what-is-the-overlap-between-private-and-public",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Following the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\n\n# taking out URLs\nremove_bits &lt;- \"&amp;|&lt;|&gt;\"\n\ntidy_tweets &lt;- df %&gt;%\n  filter(!str_detect(text, \"^RT\")) %&gt;%\n  mutate(text = str_remove_all(text, remove_bits)) %&gt;%\n  unnest_tokens(word, text, token = \"tweets\") %&gt;%\n    # remove stopwords\n  filter(\n    !word %in% stop_words$word,\n    !word %in% str_remove_all(stop_words$word, \"'\"),\n    str_detect(word, \"[a-z]\")\n  ) \n\nfrequency &lt;- tidy_tweets %&gt;% \n  group_by(screen_name) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(tidy_tweets %&gt;% \n              group_by(screen_name) %&gt;% \n              summarise(total = n())) %&gt;%\n  mutate(freq = n/total)\n\nfrequency &lt;- frequency %&gt;% \n  select(screen_name, word, freq) %&gt;% \n  pivot_wider(names_from = screen_name, values_from = freq) %&gt;%\n  arrange(desc(datawookie), fathomdata)\n\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew‚Äôs tweets.\n\nfrequency %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  theme(legend.position = \"none\") +\n    labs(x = \"Andrew Collier\",\n         y = \"Fathom Data\")\n\n\n\n\nComparing the frequency of words used by the Fathom Data account and Andrew Collier\n\n\n\n\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\n\nfrequency %&gt;% \n  mutate(highlight = word == \"@standardbankza\") %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\nHighlighting Standard Bank account mentions\n\n\n\n\nThis is clearly the case! Have a look at the top terms, haha!\n\nfrequency %&gt;%\n  filter(is.na(fathomdata)) %&gt;%\n  select(-fathomdata) %&gt;%\n  mutate(datawookie = scales::percent(datawookie)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\n\n\n\n\nWord\nRelative specificity to Andrew Collier‚Äôs tweets\n\n\n\n\nkak\n0.6723%\n\n\n#customerservice\n0.6666%\n\n\n@rainsouthafrica\n0.5759%\n\n\nmind\n0.5220%\n\n\nmbps\n0.4255%\n\n\nprepared\n0.3773%\n\n\nlinux\n0.3489%\n\n\n#itcanbe\n0.3262%\n\n\n#fixit\n0.2354%\n\n\nworst\n0.2241%"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#conclusion",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#conclusion",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "It appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html",
    "title": "LinkedIn style tables for personal websites",
    "section": "",
    "text": "Katherine asked me if she could include the university logo in her education section on the about page on her website. The about page is made with the postcards package. I didn‚Äôt know how to do it but thought that a table might make the most sense.\nI really liked the style of the LinkedIn education sections:\n\nknitr::include_graphics(\"images/linkedin_education.PNG\")\n\n\n\n\nLinkedIn screenshot\n\n\n\n\nIn this post I walk through recreating this table with the gt package in R."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#purpose",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#purpose",
    "title": "LinkedIn style tables for personal websites",
    "section": "",
    "text": "Katherine asked me if she could include the university logo in her education section on the about page on her website. The about page is made with the postcards package. I didn‚Äôt know how to do it but thought that a table might make the most sense.\nI really liked the style of the LinkedIn education sections:\n\nknitr::include_graphics(\"images/linkedin_education.PNG\")\n\n\n\n\nLinkedIn screenshot\n\n\n\n\nIn this post I walk through recreating this table with the gt package in R."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#where-to-begin",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#where-to-begin",
    "title": "LinkedIn style tables for personal websites",
    "section": "Where to begin?",
    "text": "Where to begin?\nThe education section in the LinkedIn table has six components for each entry. They are:\n\nA logo photo\nA hyperlink to the institution though the image\nName of the institution (in bold)\nTitle of the degree\nDate (start and end year)\nGrade or comment\n\nLet‚Äôs create a tibble with these components.\n\nlibrary(tidyverse)\n\neducation &lt;- tribble(\n  ~logo, ~link, ~institution, ~degree, ~date, ~grade_comment,\n  # LU\n  \"images/lu.jfif\", \"https://ehl.lu.se/ekonomisk-historia\", \"Lund University\", \"Master's degree, Economics\", \"2021\", \"Exchange semester\",\n  # SU\n  \"images/su.jfif\", \"https://www.ekon.sun.ac.za/\", \"Stellenbosch University\", \"Master's degree, Economics\", \"2020\", \"Grade: &lt;em&gt;cum laude&lt;/em&gt;\",\n  # uct hnours\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Honours degree, Economics\", \"2018\", \"Grade: 2:1\",\n  # uct undergrad\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Bachelor's degree, Economics and Economic History\", \"2015-2017\", \"Grade: 2:1\",\n)"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#building-the-table",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#building-the-table",
    "title": "LinkedIn style tables for personal websites",
    "section": "Building the table",
    "text": "Building the table\nNow lets start on the table structure. What we‚Äôre gonna do is take the individual pieces and format them to html, then stick the text together in one cell and the logos/links in another.\n\neducation &lt;- education %&gt;%\n  mutate(\n    logo = glue::glue(\"&lt;a href = {link}&gt;\n                        &lt;img src='{logo}' width='50' height='50'&gt;\n                      &lt;/a&gt;\"),\n    institution = glue::glue(\"&lt;b&gt;{institution}&lt;/b&gt;\"),\n    date = glue::glue(\"&lt;span style='opacity: .7;'&gt; {date} &lt;/span&gt;\"),\n    grade_comment = glue::glue(\"&lt;span style='opacity: .7;'&gt; {grade_comment} &lt;/span&gt;\"),\n  )\n\nIn the cell above we create an image tag for the logo, putting a hyperlink tag for the link to the university website inside it. Here we use the glue package which is useful for working with strings constructed from other columns. You can think of it as gluing together the html tag parts and the content from our dataframe. Next we put the institution in bold, and grey out the dates and grades with the html style tag, 'opacity: .7;'.1\nNow we can stick these text components together into one cell (just called text) and keep only the logo and text columns.2 Finally we use the map function from the purrr package to transform the character strings into html. Printing the tibble shows that it now contains html elements inside each cell.\n\neducation &lt;- education %&gt;%\n  mutate(text = str_c(\n    institution, \"&lt;br&gt;\",\n    degree, \"&lt;br&gt;\",\n    date, \"&lt;br&gt;\",\n    grade_comment\n  )) %&gt;%\n  select(logo, text) %&gt;%\n  mutate(\n    logo = purrr::map(logo, gt::html),\n    text = purrr::map(text, gt::html)\n  )\n\neducation\n\n# A tibble: 4 √ó 2\n  logo       text      \n  &lt;list&gt;     &lt;list&gt;    \n1 &lt;html [1]&gt; &lt;html [1]&gt;\n2 &lt;html [1]&gt; &lt;html [1]&gt;\n3 &lt;html [1]&gt; &lt;html [1]&gt;\n4 &lt;html [1]&gt; &lt;html [1]&gt;\n\n\nNow we create the table by calling the gt() command.\n\nlibrary(gt)\n\neducation %&gt;% \n    gt()\n\n\n\n\n\n\n\n\n\n\n\nlogo\ntext\n\n\n\n\n\nLund University\nMaster's degree, Economics\n 2021 \n Exchange semester \n\n\n\nStellenbosch University\nMaster's degree, Economics\n 2020 \n Grade: cum laude \n\n\n\nUniversity of Cape Town\nHonours degree, Economics\n 2018 \n Grade: 2:1 \n\n\n\nUniversity of Cape Town\nBachelor's degree, Economics and Economic History\n 2015-2017 \n Grade: 2:1 \n\n\n\n\n\n\n\nGreat, there it is!"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#finalizing-the-table",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#finalizing-the-table",
    "title": "LinkedIn style tables for personal websites",
    "section": "Finalizing the table",
    "text": "Finalizing the table\nWe still want to make some changes to tidy it up a bit:\n\nAdding a table header\nAligning the text to the left\nRemoving the column labels\n\n\neducation %&gt;%\n  gt() %&gt;%\n  tab_header(md(\"**Education**\")) %&gt;%\n  cols_align(align = c(\"left\"), columns = text) %&gt;%\n  cols_label(\n    logo = \"\",\n    text = \"\",\n  )\n\n\n\n\n  \n    \n      Education\n    \n    \n  \n  \n    \n      \n      \n    \n  \n  \n    \n  \n\nLund UniversityMaster's degree, Economics 2021  Exchange semester \n    \n  \n\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \n    \n  \n\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \n    \n  \n\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1 \n  \n  \n  \n\n\n\n\nLovely! Now we have the table we can use in the about page of a blog or personal website, with nice clickable logos that take you to the website of the institution."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#footnotes",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#footnotes",
    "title": "LinkedIn style tables for personal websites",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe opt to use the &lt;span&gt; tag rather than the &lt;p&gt; tag because &lt;span&gt; does not introduce any extra space.‚Ü©Ô∏é\nWe use str_c from the stringr package for its nice syntax, we could have used glue here again‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2022-12-15-coef-plots/coef-plots.html",
    "href": "posts/2022-12-15-coef-plots/coef-plots.html",
    "title": "Coefficient plots in ggplot",
    "section": "",
    "text": "Recently a colleague asked how they could quickly draw a coefficient plot in R. In Stata, this is relatively simple - just run your regression and use the command coefplot afterwards. This produces a graphic that is perfectly acceptable for exploratory data analysis, but leaves something to be desired if you want to use it in a publication.\nThis post shows you how to draw coefficient plots in R and ggplot, and is extensible for use with regressions beyond the basic lm command.\n\nData\nFor this example we will use data from the lovely Palmer Penguins package from Allison Horst. The penguins dataset is a great toy dataset for exploration and visualization, based on genuine data collected by Dr.¬†Kristen Gorman at the Palmer Station in Antarctica.\n\n\n\n\n\n\n\n\n\n\n\nBasic regression\nWe begin with a basic regression where our dependent variable is penguin body weight in grams, and independent variables are the dimensions of the penguins‚Äô bills, flipper length, as well as species and sex.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n\n# read in data\ndf &lt;- palmerpenguins::penguins\n\n# basic regression\nbasic_reg &lt;- df %&gt;% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .)\n\nbasic_reg\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + sex, data = .)\n\nCoefficients:\n      (Intercept)     bill_length_mm      bill_depth_mm  flipper_length_mm  \n         -1460.99              18.20              67.22              15.95  \n speciesChinstrap      speciesGentoo            sexmale  \n          -251.48            1014.63             389.89  \n\n\nThis produces a rather messy output. It includes both the regression specification and the coefficients.\n\n\nTidy with broom package\nWe can use the broom package to return a tibble, a neat data object that is easy to work with.\n\nlibrary(broom)\n\ntidy_reg &lt;- df %&gt;% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %&gt;% \n    tidy()\n\ntidy_reg\n\n# A tibble: 7 √ó 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2\n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4\n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3\n6 speciesGentoo       1015.     130.        7.83 6.85e-14\n7 sexmale              390.      47.8       8.15 7.97e-15\n\n\nGreat! This output is much easier to deal with.\n\n\nCoefficient plot\nLet‚Äôs try and make a coefficient plot.\n\ntidy_reg %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot\"\n  )\n\n\n\n\n\n\n\n\nWe can see that relative to Adelie penguins (the base category), Gentoo penguins weigh more and Chinstrap penguins weigh less.\nFurther, male penguins weigh more than females.\n\n\nError bars\nTo get the error bars, we specify that we want a confidence interval when we use the tidy command from the broom package, like so: tidy(conf.int = TRUE)\n\ntidy_reg_conf_int &lt;- df %&gt;% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %&gt;% \n    tidy(conf.int = TRUE)\n\ntidy_reg_conf_int\n\n# A tibble: 7 √ó 7\n  term              estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2 -2585.      -337. \n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2     4.22      32.2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4    28.4      106. \n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8    10.2       21.7\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3  -411.       -92.0\n6 speciesGentoo       1015.     130.        7.83 6.85e-14   760.      1270. \n7 sexmale              390.      47.8       8.15 7.97e-15   296.       484. \n\n\nNow in our tibble we get columns called conf.low and conf.high.\nTo plot these, we use an additional geometry in our ggplot, called geom_errorbarh. Here the h at the end specifies we want it in the horizontal direction. We map the conf.low and conf.high variables to xmin and xmax respectively.\n\ntidy_reg_conf_int %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\n\n\n\n\nBecause none of the error bars cross the dotted line at zero, we conclude that the point estimates of the effects are significantly different from zero at the five percent level.\n\n\nCategories of predictor\nFinally, let‚Äôs facet the variables by type.\nWe use the case_when command from dplyr to call any variable containing mm a numeric variable, and the others categorical. We colour our bars and points by variable type, and facet_wrap to make a small multiple chart. This makes it easy to differentiate between the different types of variables.\n\ntidy_reg_conf_int %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  # create a type variable with case_when\n  mutate(type = case_when(\n    str_detect(term, \"mm\") ~ \"Numeric variable\",\n    TRUE ~ \"Categorical variable\"\n  )) %&gt;%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term, colour = type)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  # remove the legend as the facet show that information\n  theme(legend.position = \"none\") +\n  # facet by type\n  facet_wrap(~type, scales = \"free_y\") +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\",\n    subtitle = \"By variable type\"\n  )\n\n\n\n\n\n\n\n\nIn this way we can see that the scaling of the variables can have an impact on how we perceive the results. Because the bill and flipper dimensions are measured in mm, and the body mass in grams, we are seeing the effect of an additional mm of bill depth, for example, on body mass in grams, which appears quite small.\n\n\nConclusion\nGreat - I hope that this was useful! It‚Äôs possible to easily customise your coefficient plots in ggplot, adding labels and colours to help your reader understand your regression results in a glance."
  },
  {
    "objectID": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "href": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "title": "ChatGPT cooks me dinner",
    "section": "",
    "text": "Intro\nI ask ChatGPT to help my cook dinner based on the ingredients I had in the fridge - it was a great success!\nHave a look below at how it went!\nI also asked it to write this blog post, the output for which I include below.\n\n\nProcess\n\n\n  \nVideo\n\n\n\n\n\n\n\n\nVideo\n\n\nSuggested blog post text\nCooking is always a fun and creative experience, but sometimes it can be challenging to come up with new and exciting recipes. That‚Äôs why having a helpful tool like ChatGPT can make a big difference in the kitchen.\nRecently, I had the opportunity to put ChatGPT to the test as a cooking assistant. I provided a list of ingredients that I had on hand and asked for a recipe suggestion. ChatGPT quickly came up with a recipe for a Gammon and Vegetable Stew with Mint, along with detailed instructions for cooking the dish.\nI was impressed with the speed and accuracy of ChatGPT‚Äôs suggestion, and even more impressed with the quality of the recipe. The stew turned out to be delicious, hearty, and comforting - exactly what I was looking for on a cold winter day.\nBut ChatGPT‚Äôs assistance didn‚Äôt stop there. I asked if I could add a splash of Chardonnay to the stew and ChatGPT not only gave me a positive response, but also provided instructions on when and how to add the wine, along with a reminder to use a good quality Chardonnay to enhance the overall taste of the dish.\nOverall, my experience with ChatGPT as a cooking assistant was extremely positive. It was able to provide me with a recipe and instructions that resulted in a delicious and satisfying meal. I was impressed with its speed, accuracy, and helpful suggestions, and I would definitely use ChatGPT again for recipe ideas and cooking instructions.\n\n\nConclusion\nWhat an amazing tool!"
  },
  {
    "objectID": "content/generative-ai-articles-listing.html",
    "href": "content/generative-ai-articles-listing.html",
    "title": "Interlude One",
    "section": "",
    "text": "UK Importers‚Äô Guide to Navigating Freight Shipping Trends 2023-2024\n\n\n\n\n\nA post about trends in the shipping industry targetted at importers in the UK. \n\n\n\n\n\nNov 9, 2023\n\n\nJonathan Jayes & ChatGPT\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/research.html",
    "href": "content/research.html",
    "title": "Interlude One",
    "section": "",
    "text": "Wired for Success - Sweden‚Äôs Electrical Innovators in the Age of Industrial Transformation\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nRatio Seminar: Hur p√•verkar AI-utvecklingen jobben?\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWired for Success: Sweden‚Äôs Electrical Innovators in the Age of Industrial Transformation\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n\n\n\n\n\n\nWarwick Poster\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/research/Warwick-poster.html",
    "href": "content/research/Warwick-poster.html",
    "title": "Warwick Poster",
    "section": "",
    "text": "I am delighted to present a poster at the EHS conference in Warwick this year!\nYou can have have a look at the poster on this page below. I also include the slides for a presentation of the paper on the same theme which I am writing along with my two supervisors, Kerstin Enflo and Jakob Molinder."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interlude One",
    "section": "",
    "text": "Untitled\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nHow Rare-Earth Element Dependency Threatens the Global Wind Rollout\n\n\n\n\n\n\n\n\n\n\n\nApr 23, 2025\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nHISCO lookup\n\n\n\n\n\n\n\n\n\n\n\nMar 28, 2025\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nDRAFT: Analysis for a Solar PV & Battery System (South Africa)\n\n\n\n\n\n\n\n\n\n\n\nMar 11, 2025\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nClaude 3.7: React and Reactions\n\n\n\n\n\n\n\n\n\n\n\nFeb 27, 2025\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nTransit Trek\n\n\n\n\n\n\n\n\n\n\n\nFeb 23, 2025\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nNeverTooSmall Couch Colours\n\n\n\n\n\n\n\n\n\n\n\nNov 19, 2024\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nA modern question and answer site: SCB Users Group\n\n\n\n\n\nA post about creating a question and answer site for users of Statistics Sweden data \n\n\n\n\n\nNov 7, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nGeocoding: two ways\n\n\n\n\n\nA post about geocoding place names using R and python to access the OSM and Google Maps geocoding APIs. \n\n\n\n\n\nOct 2, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nTransforming National Budget Documents into Queryable Knowledge\n\n\n\n\n\nA post about automating the data retrieval process from PDF documents using LangChain and the OpenAI API. \n\n\n\n\n\nJul 5, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nThe Economics of Instant Hot Water: Quooker vs Kettle\n\n\n\n\n\nHow long will it take for an instant hot water tap to save some money? \n\n\n\n\n\nJul 4, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nBuilding a YouTube Video Transcript Summarizer (with some help)\n\n\n\n\n\nA post on what it means to be a data scientist in the age of such capable generative AI. \n\n\n\n\n\nMay 23, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT as translator and text wizard\n\n\n\n\n\nA post showing how ChatGPT can be used to create structred data from text. \n\n\n\n\n\nMar 10, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nMap projections in R\n\n\n\n\n\nA post on how to change projections for mapping and save out the new shapefiles. \n\n\n\n\n\nJan 31, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nElectricity generation per capita\n\n\n\n\n\nA post on electricity generation per capita across the world since 2000. \n\n\n\n\n\nJan 30, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nOur World In Data choropleth\n\n\n\n\n\nA post on how recreate the Our World In Data characteristic choropleth in R. \n\n\n\n\n\nJan 29, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT cooks me dinner\n\n\n\n\n\nüç≤ Master the kitchen like a pro with the help of ChatGPT! üç¥ I tried it out and the results will blow your mind! ü§Ø Find out what happened when I asked a language model for recipe ideas and cooking instructions. üò≤ You won‚Äôt believe how delicious my Gammon and Vegetable Stew with Mint turned out! üçΩÔ∏è Don‚Äôt miss out on this must-read blog post! üëÄ \n\n\n\n\n\nJan 20, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nChatGPT fills in for me as a blog post author\n\n\n\n\n\nI ask ChatGPT to write a blog post in the same manner as one I wrote last week. The results will amaze you! \n\n\n\n\n\nDec 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nCoefficient plots in ggplot\n\n\n\n\n\nDrawing coefficient plots in R and ggplot. \n\n\n\n\n\nDec 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nLinkedIn style tables for personal websites\n\n\n\n\n\nA post on how to us the {gt} table package to make tables for the about page of a pesonal website or blog. \n\n\n\n\n\nMay 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nProgrammatically generate tabs in Rmarkdown\n\n\n\n\n\nA walk-though for writing code that writes it‚Äôs own code. \n\n\n\n\n\nMar 4, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nFlavours of cocoa\n\n\n\n\n\nAn exploration of chocolate bar reviews \n\n\n\n\n\nFeb 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nR vs Python? R & Python!\n\n\n\n\n\nMining the twitter archive of Fathom Data \n\n\n\n\n\nJan 17, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nStudents Speak\n\n\n\n\n\nText analysis of students‚Äô diary entries during the Covid-19 lockdown in South Africa \n\n\n\n\n\nJul 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nResources for Learning R and Mastering ggplot\n\n\n\n\n\nA collection of videos and resources to get you started using R and the tidyverse for exploratory data analysis and visualization. \n\n\n\n\n\nMay 21, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nMapping Sweden‚Äôs Regional GDP\n\n\n\n\n\nVisualizing the relationship between population and regional GDP share in 1571, based on data from Enflo and Missiaia (2018). \n\n\n\n\n\nMay 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nShipping and Scraping - Part 1 in a Series on Shipping\n\n\n\n\n\nIn this post I walk through scraping data on cargo ships from Wikipedia as part of a series on shipping. I make use of R, the rvest package for webscraping and the SelectorGadget tool for CSS selection. \n\n\n\n\n\nApr 8, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nSwedish trade in goods and exchange students\n\n\n\n\n\nI wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund Univesity from there, this semester. \n\n\n\n\n\nFeb 20, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nSouth African Car Colour Exploration\n\n\n\n\n\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree. \n\n\n\n\n\nNov 24, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nCar Price Comparison Tool\n\n\n\n\n\nThis web app helps South Africans gauge the median price for a second hand car. You can compare price by odometer reading, make and model, and province. Ad links are included in a table beside the output. \n\n\n\n\n\nOct 10, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nRecreating Our World In Data‚Äôs Beautiful Transistor Count Graphic\n\n\n\n\n\nScraping data from Wikipedia and recreating OWID‚Äôs figure with ggplot2. \n\n\n\n\n\nApr 8, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n\n\n\n\n\n\nWelcome to Interlude One\n\n\n\n\n\n‚ÄúLike all good fruit, the balance of life is in the ripe and ruin‚Äù \n\n\n\n\n\nMar 13, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2023-11-07-q-and-a/q-and-a.html",
    "href": "posts/2023-11-07-q-and-a/q-and-a.html",
    "title": "A modern question and answer site: SCB Users Group",
    "section": "",
    "text": "Long email chains are a mess to search through. Researchers grappling with Statistics Sweden (SCB) datasets often exchange insights that vanish into inboxes only accessible to those in the email chain. We are building the SCB Users Group to change that. It‚Äôs a platform where shared questions yield communal answers, accessible to everyone online.\n\n\n\n\n\n\nDigital artwork by ChatGPT with DALL¬∑E 3, depicting a lively forum discussion, inspired by the detail-oriented style of painter Gustave Caillebotte.\n\n\n\n\n\n\nThe concept is simple: think StackOverflow, but with a focus on SCB data. Researchers ask, answer, and, in the process, craft a self-sustaining knowledge base.\nWe have built an alpha version and are looking to get a small amount of money for a custom domain and hosting costs.\nThe current version of the site is accessible here. I include a screenshot of the homepage below.\n\n\n\nHomepage\n\n\n\n\n\nWe chose Google Firebase Firestore for its simple storage and Firebase for authentication. The upshot? A secure, efficient backend without the headache of password management or the risk of exposing passwords inadvertently. There is also a generous free tier offered for storage on Firebase.\nThe Firestore nosql database has a simple set up with a collection for users with a unique id, usernames and emails. The questions collection has a unique id for each question, the information pertaining to each question like title and body and date, as well as a subcollection for the number of upvotes and id of the user who upvoted to prevent multiple upvotes from the same user. Each answer to the question belongs to a sub collection of the question called ‚Äòanswers‚Äô in the nosql database.\n\n\n\nSvelteKit powers our frontend; its dynamic nature is perfect for crafting reactive user interfaces. SvelteFire slots let us tailor content based on user status, enhancing interaction without clutter.\nFor example, compare the below in which logged in users can upvote questions and answers and add the their own answers with the view of logged out users who can just view the content.\n\n\n\n\n\n\n\n\n\n\n\n(a) Logged in\n\n\n\n\n\n\n\n\n\n\n\n(b) Logged out\n\n\n\n\n\n\n\nFigure¬†1: Question page.\n\n\n\n\n\n\nThe interface is intuitive. If you‚Äôre logged in, you‚Äôre part of the conversation. If not, you‚Äôre still in the know.\n\n\n\nNetlify hosts our site, chosen for its scalability and future-proofing abilities. It‚Äôs a platform that grows with us, promising smooth custom domain integration down the line.\n\n\n\nMarkdown rendering is next on our list. It‚Äôs about enriching the discourse with equations, links, and well-formatted content.\n\n\n\nOur secret weapon during development? ChatGPT-4 vision. It‚Äôs a tireless coding partner, turning visual and textual prompts into Svelte components that click into place. Iterating is swift.\n\n\n\nThe project is public on GitHub in this repo. Feel free to add a comment in the discussions tab with a suggestion about how we can improve before launch."
  },
  {
    "objectID": "posts/2023-11-07-q-and-a/q-and-a.html#introduction",
    "href": "posts/2023-11-07-q-and-a/q-and-a.html#introduction",
    "title": "A modern question and answer site: SCB Users Group",
    "section": "",
    "text": "Long email chains are a mess to search through. Researchers grappling with Statistics Sweden (SCB) datasets often exchange insights that vanish into inboxes only accessible to those in the email chain. We are building the SCB Users Group to change that. It‚Äôs a platform where shared questions yield communal answers, accessible to everyone online.\n\n\n\n\n\n\nDigital artwork by ChatGPT with DALL¬∑E 3, depicting a lively forum discussion, inspired by the detail-oriented style of painter Gustave Caillebotte.\n\n\n\n\n\n\nThe concept is simple: think StackOverflow, but with a focus on SCB data. Researchers ask, answer, and, in the process, craft a self-sustaining knowledge base.\nWe have built an alpha version and are looking to get a small amount of money for a custom domain and hosting costs.\nThe current version of the site is accessible here. I include a screenshot of the homepage below.\n\n\n\nHomepage\n\n\n\n\n\nWe chose Google Firebase Firestore for its simple storage and Firebase for authentication. The upshot? A secure, efficient backend without the headache of password management or the risk of exposing passwords inadvertently. There is also a generous free tier offered for storage on Firebase.\nThe Firestore nosql database has a simple set up with a collection for users with a unique id, usernames and emails. The questions collection has a unique id for each question, the information pertaining to each question like title and body and date, as well as a subcollection for the number of upvotes and id of the user who upvoted to prevent multiple upvotes from the same user. Each answer to the question belongs to a sub collection of the question called ‚Äòanswers‚Äô in the nosql database.\n\n\n\nSvelteKit powers our frontend; its dynamic nature is perfect for crafting reactive user interfaces. SvelteFire slots let us tailor content based on user status, enhancing interaction without clutter.\nFor example, compare the below in which logged in users can upvote questions and answers and add the their own answers with the view of logged out users who can just view the content.\n\n\n\n\n\n\n\n\n\n\n\n(a) Logged in\n\n\n\n\n\n\n\n\n\n\n\n(b) Logged out\n\n\n\n\n\n\n\nFigure¬†1: Question page.\n\n\n\n\n\n\nThe interface is intuitive. If you‚Äôre logged in, you‚Äôre part of the conversation. If not, you‚Äôre still in the know.\n\n\n\nNetlify hosts our site, chosen for its scalability and future-proofing abilities. It‚Äôs a platform that grows with us, promising smooth custom domain integration down the line.\n\n\n\nMarkdown rendering is next on our list. It‚Äôs about enriching the discourse with equations, links, and well-formatted content.\n\n\n\nOur secret weapon during development? ChatGPT-4 vision. It‚Äôs a tireless coding partner, turning visual and textual prompts into Svelte components that click into place. Iterating is swift.\n\n\n\nThe project is public on GitHub in this repo. Feel free to add a comment in the discussions tab with a suggestion about how we can improve before launch."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Jayes",
    "section": "",
    "text": "‚ÄúI enjoy some of the old and I enjoy the new. And if I can find a balance between it, that‚Äôs where I find my satisfaction.‚Äù\n‚Äì Anderson .Paak - The Dreamer (feat. Talib Kweli & Timan Family Choir)"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "Jonathan Jayes",
    "section": "",
    "text": "‚ÄúI enjoy some of the old and I enjoy the new. And if I can find a balance between it, that‚Äôs where I find my satisfaction.‚Äù\n‚Äì Anderson .Paak - The Dreamer (feat. Talib Kweli & Timan Family Choir)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jonathan Jayes",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\n\n\n\nLund University\nPhD, Economic History\n 2023-now \n Under prof. Kerstin Enflo \n\n\n\nLund University\nMaster's degree, Economics\n 2021 \n Exchange semester \n\n\n\nStellenbosch University\nMaster's degree, Economics\n 2020 \n Grade: cum laude \n\n\n\nUniversity of Cape Town\nHonours degree, Economics\n 2018 \n Grade: 2:1 \n\n\n\nUniversity of Cape Town\nBachelor's degree, Economics and Economic History\n 2015-2017 \n Grade: 2:1"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jonathan Jayes",
    "section": "Experience",
    "text": "Experience\nCodera Analytics | Data Science Intern at Codera Analytics| December 2023 - March 2023\nFathom Data | Data Science Intern at Fathom Data| Feb 2022 - April 2022\nLaboratory for the Economics of Africa‚Äôs Past | Researcher under Prof Johan Fourie| January 2020 - July 2021\nFTI Consulting | Junior Economist in Economic and Financial Consulting Practice | March 2019 - January 2020"
  },
  {
    "objectID": "content/research/Ratio-seminar.html",
    "href": "content/research/Ratio-seminar.html",
    "title": "Ratio Seminar: Hur p√•verkar AI-utvecklingen jobben?",
    "section": "",
    "text": "I‚Äôm thrilled to present a summary of my research on the impact of electrification on the labour market at the Ratio seminar on May 23rd 2023. The seminar will look at the impact of AI on the labour market, and I am providing some context on a previous technological revolution. The prompt for the seminar is as follows:\nNu g√•r det fort. Utvecklingen av AI rusar fram och med det dess effekter p√• arbetsmarknad och √∂vrigt samh√§lle, och det g√•r fortare √§n n√•got tidigare. Vad betyder det? Vilka yrken kommer att p√•verkas mest ‚Äì respektive minst av AI-utvecklingen? Och vad kr√§vs f√∂r att de samh√§lleliga och socioekonomiska f√∂ljderna av framtidens AI ska bli bra? Vad kan vi l√§ra av historien?\nHave a look at my slides below."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "",
    "text": "In 2023, video content is king. The trouble is, videos are not always the most convenient format to search through for information, especially if you‚Äôre after a specific snippet of content. Scrubbing back and forth through a video isn‚Äôt heaps of fun.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts an automaton robot examining a spreadsheet\n\n\n\n\nThis week I wanted to condense the key points from an interesting video on AI and white collar jobs in Sweden. The podcast episode in which Magnus Lodefalk and Erik Engberg discuss the coming dissemination of AI through the economy, and its consequences for the labourt market, is super interesting and I highly reccomend it.\n\n\n\n\nWhat I wanted was to create a tool to grab the English auto translated transcript of the podcast and summarize it, highlighting the most important takeaways. With the help of OpenAI‚Äôs GPT-4, I was able to create the tool for the job in an afternoon.\n\n\nI‚Äôm aware that the afternoon spent building the tool certainly took longer than rewatching the video and taking notes to summarize myself. However, I enjoyed making the tool, and I can now use it for any similar task in the future. I also learned a lot about how to interact with OpenAI‚Äôs API, which is a valuable skill in itself.\nYou can have a look at my github repo here, and you are welcome to use the tool yourself if you have your own OpenAI API key. The instructions are in the readme. The remainder of this post describes the tool and some thoughts about what it means to make these kinds of tools in the age of such capable generative AI."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#introduction",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#introduction",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "",
    "text": "In 2023, video content is king. The trouble is, videos are not always the most convenient format to search through for information, especially if you‚Äôre after a specific snippet of content. Scrubbing back and forth through a video isn‚Äôt heaps of fun.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts an automaton robot examining a spreadsheet\n\n\n\n\nThis week I wanted to condense the key points from an interesting video on AI and white collar jobs in Sweden. The podcast episode in which Magnus Lodefalk and Erik Engberg discuss the coming dissemination of AI through the economy, and its consequences for the labourt market, is super interesting and I highly reccomend it.\n\n\n\n\nWhat I wanted was to create a tool to grab the English auto translated transcript of the podcast and summarize it, highlighting the most important takeaways. With the help of OpenAI‚Äôs GPT-4, I was able to create the tool for the job in an afternoon.\n\n\nI‚Äôm aware that the afternoon spent building the tool certainly took longer than rewatching the video and taking notes to summarize myself. However, I enjoyed making the tool, and I can now use it for any similar task in the future. I also learned a lot about how to interact with OpenAI‚Äôs API, which is a valuable skill in itself.\nYou can have a look at my github repo here, and you are welcome to use the tool yourself if you have your own OpenAI API key. The instructions are in the readme. The remainder of this post describes the tool and some thoughts about what it means to make these kinds of tools in the age of such capable generative AI."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#how-the-tool-works",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#how-the-tool-works",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "How the Tool Works",
    "text": "How the Tool Works\nThe YouTube Caption Summaries tool is a streamlined tool that condenses the key points from a Youtube video transcript. As an input, you provide a Youtube URL, and the tool returns a text file with the summary. It makes use of the youtube-transcript-api to grab the transcript, and the OpenAI API to summarize it, leveraging the GPT-3.5 turbo model.\nThis tool isn‚Äôt limited to English content. If you‚Äôre dealing with a non-English video, you can specify the original language, and the tool will automatically grab the auto generated English transcript before summarizing it.\n\n\n\n\n\ngraph TB\n  A[Input YouTube URL] --&gt; B[YouTube Data API: Get Transcript]\n  B --&gt; C[OpenAI API: Summarize Transcript]\n  C --&gt; D[Save Summary to Text File]\n\n\n\n\n\n\n\n\nAn Example\nTo illustrate, here‚Äôs an example of the tool in action. The video in question is a 10 minute talk by Timoth√©e Parrique titled ‚ÄúBest of #BeyondGrowth 2023‚Äù. You can watch the video below.\n\n\nHere‚Äôs the summary produced by the tool:\nHere are the key points from Timoth√©e Parrique‚Äôs speech:\n\nThe concept of ‚Äúgreen growth‚Äù is deeply ingrained in environmental strategies like SDG 8, the Paris Agreement, and the European Green Deal.\nHowever, Parrique criticizes the concept, arguing that the notion of economic growth fully detached from nature is baseless and misleading.\nHe contends that truly sustainable economic growth requires absolute decoupling of production and consumption from all environmental pressures, done quickly and maintained over time.\nHe disputes the feasibility of green growth, using minor reductions in European countries‚Äô greenhouse gas emissions as evidence.\nHe suggests that achieving a 55 percent reduction in emissions by 2030 necessitates significant GDP ‚Äúde-growth‚Äù.\nHe warns that neglecting the ecological impact of consumption exacerbates the problem.\nHe introduces the idea of ‚Äúde-growth‚Äù or a steady-state economy as a realistic strategy to reduce environmental stress and attain environmental targets, specifically in Europe.\nThe strategy aims at reducing production and consumption to prevent ecological overshoot and stabilize the economy within planetary limits without compromising living standards.\nHe advocates for high-income countries to decrease consumption to allow less affluent nations to sustainably grow and fulfill their needs.\nThis strategy‚Äôs goal is to guarantee everyone‚Äôs well-being within the boundaries of the planet.\nThe text criticizes the ‚Äúcoupling‚Äù narrative as dangerous, potentially inducing complacency by assuring people that everything is okay.\nParrique sees ‚Äúgreen growth‚Äù as macroeconomic greenwashing, preventing essential radical changes.\nHe argues that precious time is being lost by making minor tweaks to the system instead of concentrating on the pressing need for transformation.\nParrique poses the question of whether the priority should be economic growth or nature preservation."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#performance-pros-and-cons",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#performance-pros-and-cons",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Performance: Pros and Cons",
    "text": "Performance: Pros and Cons\nThe summary above does a good job of capturing the main points that Parrique makes in his speech.\nOne issue is the quality of the data being fed into the summariser. The captioning process on Youtube is optional - with many uploaders foregoing any captions. The auto generated translations are not perfect either. There is a fantastic 99 % invisible epdisode explaining the history of closed captioning on Youtube, as well as the various improvements that have been made over the years. It certainly isn‚Äôt perfect, however, even if the transcription isn‚Äôt 100% accurate, it‚Äôs still a great starting point for a summary. When the transript is processed by the OpenAI API, the model is looking for the key points and can overcome transcription errors that are not crucial to the main message.\nOverall, I am pleased with the quality of the summaries, and in the future I might work on providing the title of the video and it‚Äôs description as additional context for the summarization model."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#journey-of-development",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#journey-of-development",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Journey of Development",
    "text": "Journey of Development\nThe development of this tool is a testament to the power of generative AI and a bit of curiosity. I did it in an afternoon, chatting with my virtual programming partner; GPT-4 offering up Python code based on my instructions.\nWhile I am not an expert python coder, I could have produced this tool from first principles a year ago (in the pre-generative pretrained model era). It could have required a couple days worth of work, familiarizing myself with the various APIs, putting together a python package structure.\nInstead, I could glue the different parts together in a couple of hours, and spend the rest of the time tweaking the code to get the best results."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#implications",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#implications",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Implications",
    "text": "Implications\nThis is the kind of project I would have used in a job interview to showcase my skills. Back in the day, being able to develop a tool like this would have been a testament to one‚Äôs coding skills. However, given that I didn‚Äôt code this from first principles, could I still do the same today? Is the product less impressive, knowing that a significant part of the coding was handled by an AI model?\nThe answer, I believe, is that the landscape of software tooling is changing. What‚Äôs important is not that the code was generated with AI assistance, but that the tool was developed effectively and solves a real problem. Coding from first principles will always have its place, but leveraging AI tools, just like any other tools, demonstrates practical problem-solving skills and adaptability. It‚Äôs a recognition that we are moving into an era where working in partnership with AI will be an essential part of many jobs, not just in the tech industry."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#conclusion",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#conclusion",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Conclusion",
    "text": "Conclusion\nSolving little problems with a bit of code and a really cheap API call is a lot of fun. I‚Äôm looking forward to seeing what other tools I can build with the OpenAI API."
  },
  {
    "objectID": "content/publications.html",
    "href": "content/publications.html",
    "title": "Publications",
    "section": "",
    "text": "I have one paper that I‚Äôm proud to have published in World Development with my master‚Äôs supervisor Johan Fourie. Have a look at it below."
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html",
    "title": "Transit Trek",
    "section": "",
    "text": "For a long time, I have had an idea simmering in the back of my mind: could I get a large language model to collect transit data for European airports without me spending countless hours scouring the web? Recently, with OpenAI‚Äôs o3-mini-high model now capable of reasoning its way through more complex tasks, I decided it was time to put this idea to the test."
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html#the-spark-of-inspiration",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html#the-spark-of-inspiration",
    "title": "Transit Trek",
    "section": "The Spark of Inspiration",
    "text": "The Spark of Inspiration\nThe inspiration came from a personal experience. My girlfriend Kathy lives in Paris, while I call Lund, Sweden home. Over the past couple of years, I‚Äôve made many trips to Paris ‚Äì mostly by plane (I do my best to minimize my environmental footprint in other ways). One recurring frustration was the transit experience at Paris CDG. Arriving in the evening often meant missing the last RER B train, leaving me to try and find a fellow frustrated flier to share an expensive taxi ride with into the city. By contrast, Copenhagen‚Äôs airport offers a seamless metro connection that runs round the clock from the airport into the city (the trains drive themselves!).\nThis contrast led me to wonder: how do transit options from European airports compare? How do factors like travel time, cost, and service frequency vary from one city to another?"
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html#the-challenge",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html#the-challenge",
    "title": "Transit Trek",
    "section": "The Challenge",
    "text": "The Challenge\nMy goal was clear, but I didn‚Äôt want to have to walk through each step myself, wading through blogs, official websites and Google Maps to collect the data. I wanted to focus on the insights, not the grunt work. I had in mind a two-step process:\n\nData Collection:\n\nStart with the list of Europe‚Äôs busiest airports from Wikipedia.\nFor each airport, gather details on the distance to the city center and the transit options available‚Äîcovering travel time, frequency, and fare estimates for modes like trains, buses, taxis, and metros.\n\nInteractive Visualization:\n\nCreate a dashboard with an interactive scatterplot and a clickable map for users to compare transit options.\nInclude charts that, for example, highlight the biggest fare differences or the taxi price per kilometer by airport.\n\n\nI pictured a simple app to peak at the data, and perhaps a scatter plot that I could use to compare different metrics."
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html#testing-ai-capabilities",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html#testing-ai-capabilities",
    "title": "Transit Trek",
    "section": "Testing AI Capabilities",
    "text": "Testing AI Capabilities\nBack when OpenAI‚Äôs in March of 2023, just after signing up for ChatGPT Plus, I set it the task. I wanted a solution that could handle the tedious parts‚Äîgoogling, scraping, and assembling a consistent dataset‚Äîso I could focus on the insights. Unfortunately, GPT-3.5 struggled; its inability to search the web and inconsistent outputs meant that my vision remained out of reach.\nI started down the path of building a pipeline that used a search engine API to find the data, pass it to GPT-3.5 for processing, and asked the LLM to produce the structred data for each airport. Prior to the support for structured output, the JSON that I got back was hit and miss both in terms of content and structure. This required a lot of manual intervention.\nI then tried a few other models, including Claude Sonnet 3.5, which did a reasonable job but still required more intervention than I would have liked. This weekend I asked OpenAI‚Äôs o3-mini-high model to give it a go. I gave it a clear prompt:\n\n‚ÄúI want to create a dataset of transit information for the 50 busiest airports in Europe to the city centers they serve. Each result should include the distance in kilometers, and for each transit method‚Äîsuch as taxis, trains, trams, or buses‚Äîprovide travel time, frequency, and one-way fare in euros, all formatted in a consistent JSON schema.‚Äù\n\nTo my surprise, the model succeeded for most airports, delivering data for 44 out of 50 ‚Äì a decent starting point.\nHere is an example of the JSON output for Heathrow Airport in London:\n\n    {\n      \"name\": \"Heathrow Airport\",\n      \"city\": \"London\",\n      \"distance_km\": 24,\n      \"airport_coords\": { \"lat\": 51.4700, \"lon\": -0.4543 },\n      \"city_coords\": { \"lat\": 51.5074, \"lon\": -0.1278 },\n      \"transit_options\": [\n        {\n          \"mode\": \"Heathrow Express\",\n          \"transit_type\": \"train\",\n          \"travel_time_minutes\": 20,\n          \"frequency\": \"every 15 minutes\",\n          \"fare_euros\": 25\n        },\n        {\n          \"mode\": \"Taxi\",\n          \"transit_type\": \"taxi\",\n          \"travel_time_minutes\": 50,\n          \"frequency\": \"on-demand\",\n          \"fare_euros\": 65\n        },\n        {\n          \"mode\": \"Bus\",\n          \"transit_type\": \"bus\",\n          \"travel_time_minutes\": 55,\n          \"frequency\": \"every 30 minutes\",\n          \"fare_euros\": 8\n        }\n      ]\n    },"
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html#building-the-transit-trek-app",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html#building-the-transit-trek-app",
    "title": "Transit Trek",
    "section": "Building the Transit Trek App",
    "text": "Building the Transit Trek App\nOnce I had the data, I asked the o3-mini-high model to create a Streamlit app to display it interactively. I think it did a decent job; the app is now live and available for anyone to explore at transit-trek.streamlit.app.\n\n\n\nThis is what the landing page of the Transit Trek app looks like\n\n\nThe app features three tabs:\n\nExploratory Tab:\nA customizable scatterplot lets users choose which metrics to compare (for example, distance versus travel time). A Folium map with clickable markers‚Äîeach displaying a neat, formatted HTML table‚Äîprovides a geographical perspective on transit options.\nAnalysis Tab:\nThis section dives into the insights. One chart highlights the difference between the cheapest and most expensive fares per airport, while another bar chart compares taxi prices per kilometer, revealing, for instance, that Geneva‚Äôs taxis are among the priciest while Istanbul offers more affordable rides.\nData Tab:\nHere, users can inspect the raw and flattened datasets, ensuring transparency and inviting further exploration.\n\n\n\n\n\n\n\n\n\n\nExploratory tab: folium map\n\n\n\n\n\n\n\nAnalysis Tab: costly trips\n\n\n\n\n\n\n\nData Tab: raw data"
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html#insights-opportunities",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html#insights-opportunities",
    "title": "Transit Trek",
    "section": "Insights & Opportunities",
    "text": "Insights & Opportunities\nThe app immediately uncovered some interesting patterns:\n\nTransit Variation:\nUnsurprisingly, when looking at travel time and holding distance constant, we see that the fastest transit options are trains and metros (dark blue and pink), while taxis are next fastest (light blue), followed by busses (in red).\n\n\n\nFare Differentials:\nBusses and metros lead the pack in affordability, with trains and taxis costing more. The scatterplot reveals that some airports‚Äîlike Geneva and Zurich‚Äîhave a wide range of fares, while others‚Äîlike Istanbul and Madrid‚Äîoffer more consistent pricing.\n\n\nI think that the current app is a very good start, especially given that it took me a couple of hours to refine based on the output from the o3 model.\nLooking ahead, there are plenty of opportunities to improve the app:\n\nPopulation Metrics:\nIncorporating data on city populations might explain why airports in mega-cities like Paris and London have longer distances and varied transit options.\nData Quality:\nA more robust quality assurance process could update outdated figures‚Äîsuch as revised taxi fares‚Äîto ensure the app remains current. I picked up some old data for the RER B train from CDG to Paris, for example, which now costs ‚Ç¨12, rather than the ‚Ç¨10 I have in the dataset."
  },
  {
    "objectID": "posts/2025-02-23-transit-trek/transit-trek.html#conclusion",
    "href": "posts/2025-02-23-transit-trek/transit-trek.html#conclusion",
    "title": "Transit Trek",
    "section": "Conclusion",
    "text": "Conclusion\nTransit Trek is just a dashboard. With the OpenAI o3-mini-high model, I could automate a process that once required a great deal of manual effort. I think this was a useful benchmark for me to understand the capabilities of modern LLM systems and how they can support our work as idea havers."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "",
    "text": "One of the claims of generative AI is that it will free humans from labourious tasks which are currently tedious to do manually and challenging to automate. In this article, we‚Äôll explore how we can use LangChain to automate the process of extracting information from PDF documents.\nThe National Treasury in South Africa produces a fantastically informative budget review each year. It includes headline figures like GDP growth, inflation, revenue and expenditure, as well as more detailed information about the budget for each government department. In addition, the budget speech is delivered by the Minister of Finance, and contains a wealth of information about the budget, including the government‚Äôs priorities for the year, and the reasoning behind the budget allocations.\nIf we want to get specific information from these documents, we can read through them and extract the information manually, searching for specific terms with the control + F functionality. This is a time consuming process, and it‚Äôs easy to make mistakes. What if we could ask a computer to do it for us?\n\n\n\n\n\n\nNote\n\n\n\nThere are, in addition to the PDF documents a series of excel files that National Treasury releases too. These files do contain the information that we want, but are not in an easily machine readable format.1 Useful tools like EconData from Codera Analytics make it much easier to access this data programatically."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-challenge",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-challenge",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "",
    "text": "One of the claims of generative AI is that it will free humans from labourious tasks which are currently tedious to do manually and challenging to automate. In this article, we‚Äôll explore how we can use LangChain to automate the process of extracting information from PDF documents.\nThe National Treasury in South Africa produces a fantastically informative budget review each year. It includes headline figures like GDP growth, inflation, revenue and expenditure, as well as more detailed information about the budget for each government department. In addition, the budget speech is delivered by the Minister of Finance, and contains a wealth of information about the budget, including the government‚Äôs priorities for the year, and the reasoning behind the budget allocations.\nIf we want to get specific information from these documents, we can read through them and extract the information manually, searching for specific terms with the control + F functionality. This is a time consuming process, and it‚Äôs easy to make mistakes. What if we could ask a computer to do it for us?\n\n\n\n\n\n\nNote\n\n\n\nThere are, in addition to the PDF documents a series of excel files that National Treasury releases too. These files do contain the information that we want, but are not in an easily machine readable format.1 Useful tools like EconData from Codera Analytics make it much easier to access this data programatically."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-solution-a-vector-database-and-a-large-language-model",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-solution-a-vector-database-and-a-large-language-model",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "The solution: a vector database and a Large Language Model",
    "text": "The solution: a vector database and a Large Language Model\nLangChain is a framework that allows us to connect a large language model to our own data. In this case, it allows to transform the South African National Budget Reviews and Budget Speeches into an interactive, queryable knowledge base. The final output in this case is a table of summary statistics that would have taken a human hours to compile, but which we can now generate in seconds.\nIn essence, to transform budget documents into a queryable database, we read in the PDFs, segment them into chunks, and store the text in a database. To query this database, we use a simple similarity calculation to find the most relevant chunks of text, which are then fed to a large language model to answer our questions.\nHere is a diagram of the process:\n\n\n\n\n\n\ngraph LR\n    A((Budget Documents PDF)) --&gt;|Chunking&lt;br&gt;Vectorization| B{Chroma DB}\n    C[User Query] --&gt;|Vectorization| D{Chroma DB}\n    D --&gt; E[Top-n Similar Chunks]\n    E --&gt; F{Large Language &lt;br&gt;Model Context Window}\n    F --&gt; G((Answer))\n    style A fill:#ece7f2,stroke:#333,stroke-width:2px\n    style B fill:#a6bddb,stroke:#333,stroke-width:2px\n    style C fill:#ece7f2,stroke:#333,stroke-width:2px\n    style D fill:#a6bddb,stroke:#333,stroke-width:2px\n    style E fill:#2b8cbe,stroke:#333,stroke-width:2px\n    style F fill:#a6bddb,stroke:#333,stroke-width:2px\n    style G fill:#2b8cbe,stroke:#333,stroke-width:2px"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#an-example-what-was-consumer-price-inflation-in-2022",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#an-example-what-was-consumer-price-inflation-in-2022",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "An example: what was Consumer Price Inflation in 2022?",
    "text": "An example: what was Consumer Price Inflation in 2022?\nSay for instance that we want to know what Consumer Price Inflation was in 2022 in South Africa. We can ask the question in a natural language query, and the system will return the answer. Here is the query and the answer, along with the source documents that the system used to answer the question:\n\n\n\n\n\n\n\n\nQuery\nResult\nSource documents\n\n\n\n\nWhat was Consumer Price Index inflation in 2022?\n6.9%\nDocument(page_content=to moderate in 2023, before rising gradually over the medium term . inflation peaked in the third quarter and averaged 6.9 per cent during 2022 . is projected is estimated to ease to 5.3 per cent in 2023. Global crude oil and food prices remain sources of inflationary pressure. Electricity prices have been up by 4.6 per cent over the medium term compared to the 2022 MTBPS and are‚Äô, metadata={‚Äòsource‚Äô: ‚Äòc:\\Users\\User\\Documents\\Recon\\langchain-budget-speech-demo\\budget-documents\\budget-review-2023.pdf‚Äô, ‚Äòpage‚Äô: 39}),\n\n\n\n\nDocument(page_content=‚ÄòCHAPTER 1 NAVIGATING AN UNEVEN ECONOMIC RECOVERY Real percentage growth 2022 2023 2024 2025 Estimate Forecast Household consumption 2.8 1.0 1.5 1.8 Gross fixed-capital formation 4.2 1.3 3.8 3.5 Exports 8.8 1.0 2.2 2.9 Imports 14.0 1.1 2.3 2.9 Real GDP growth 2.5 0.9 1.5 1.8 Consumer price index (CPI) inflation 6.9 5.3 4.9 4.7 Current account balance (% of GDP) -0.4 -1.8 -2.0 -2.1‚Äô, metadata={‚Äòsource‚Äô: ‚Äòc:\\Users\\User\\Documents\\Recon\\langchain-budget-speech-demo\\budget-documents\\budget-review-2023.pdf‚Äô, ‚Äòpage‚Äô: 27}),\n\n\n\nIn the table above, we can see two of the chunks of text that are most similar in terms of content to our query, from the ChromaDB database that contains the 2023 Budget Review and Budget Speech broken into 2000 token chunks. The first chunk in our table is from page 39 of the budget review, and the second chunk is from page 27. Both of these chunks contain the information that we are looking for, and the system has correctly identified them as the most relevant chunks.\nThese chunks are then passed to the OpenAI GPT-3.5 large language model, which is able to take in the question and the context, and then return to us the answer. In this case, the answer is 6.9%, which is correct, as we can see in the table from the budget review below (which is the same as the second source document in the markdown table above).\n\n\n\nScreenshot of table from 2023 Budget Review"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-is-going-on-in-the-background",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-is-going-on-in-the-background",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "What is going on in the background?",
    "text": "What is going on in the background?\nYou can have a look at my GitHub repository for the full code, but I‚Äôll walk through the main steps here.\n\nSet Up: The first step is to set up LangChain using an OpenAI key. This example uses OpenAI for both the language model and the embeddings.\nLoad Documents: LangChain can load multiple documents at once. In this example, PDF files containing the 2023 budget review and 2023 budget speech are loaded.\nSplit Data: The data is then split into chunks. Each chunk corresponds to a portion of information from a specific page of the budget documents.\nCreate a Database: A vector store database is created to store and index the document embeddings, which are vector representations of the text chunks. The embeddings are generated using OpenAI and stored in a database using a tool named ChromaDB.\nRetrieve Relevant Documents: A retriever is then set up to query the vector database and return relevant documents. The number of documents returned can be adjusted according to the user‚Äôs needs. In this case, I used fewer chunks with larger number of tokens per chunk compared to the default as it worked best.\nLanguage Model Chain: After setting up the retriever, a retrieval-question-answering chain is created using OpenAI. This chain is capable of answering queries based on the contents of the chunks that are passed to it, depending on the words in the query.\nFormat Output: A function is created to present the results and source documents neatly. This is what we show in the markdown table above.\nQuerying: The system is now ready for queries. A query is input and the system returns the most relevant response based on the indexed documents. In this case, we worked through each years‚Äô budget documents from 2010 to 2022 to find values for seven data series.\n\nHave a look at the output in the table below.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSelected series from Budget Reviews and Budget Speeches (2010-2022)\n\n\nKey\n2010\n2011\n2012\n2013\n2014\n2015\n2016\n2017\n2018\n2019\n2020\n2021\n2022\n\n\n\n\nBudget balance (% of GDP)\n-5.30\n-4.80\n5.2\n-4.00\n-3.9\n-3.9\n-3.40\n-4.30\n-3.70\n-6.30\n-14.00\n-5.70\n-4.2\n\n\nGDP (current ZAR[billions])\n2.67\n2.94\n\n\n3.39\n3.8\n\n\n4.32\n\n\n4.96\n5.09\n4.93\n6.17\n6.6\n\n\nGDP growth (%)\n3.40\n3.10\n2.5\n1.80\n1.4\n1.3\n0.50\n0.01\n0.70\n0.30\n-7.20\n4.80\n2.5\n\n\nGeneral government debt (% of GDP)\n25.70\n33.50\n41.8\n34.90\n40.8\n44.8\n45.70\n53.30\n55.60\n61.60\n80.30\n69.50\n71.1\n\n\nGeneral government expenditure (% of GDP)\n30.80\n32.60\n32.9\n33.20\n32.0\n33.9\n32.80\n33.20\n32.90\n29.40\n41.70\n33.20\n32.6\n\n\nGeneral government revenue (% of GDP)\n28.30\n27.70\n27.7\n29.20\n28.1\n30.0\n29.40\n28.80\n28.80\n29.40\n27.70\n27.50\n28.5\n\n\nInflation (CPI) (%)\n4.30\n5.00\n5.4\n5.70\n5.3\n4.6\n6.40\n5.30\n4.70\n4.10\n3.30\n4.80\n6.9\n\n\n\nSource: National Treasury of South Africa"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#visualising-the-data",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#visualising-the-data",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "Visualising the data",
    "text": "Visualising the data\nWe can also make a plot of our selected series. Mouse over the points to see the exact values."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-do-we-learn",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-do-we-learn",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "What do we learn?",
    "text": "What do we learn?\nThere are some missing values, particularly in the series GDP (current ZAR[billions]) and Budget balance (% of GDP).\nIn addition, in 2019, the government expenditure figure is incorectly plucked from the PDF document as 29.40% of GDP, when in reality, this was 35.70% of GDP. This is a significant error, and it is not clear why this happened. It is possible that the PDF document was not correctly parsed, or that the OpenAI API made an error extracting this value from the text chunk.\nIn order to overcome these kinds of errors, experimenting with multiple pipelines and chunk sizes is probably the best way to go. This was just a demonstration, and there is certainly a possiblity to make the pipline more robust and less error prone."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#conclusion",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#conclusion",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "Conclusion",
    "text": "Conclusion\nThough we have used budget data from South Africa in this case, the same approach can be used for any kind of data. LangChain has document loaders for text, PDF, and HTML, and can be used to extract information from many kinds of document."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#footnotes",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#footnotes",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are ideosyncracies between each year that mean ingesting this data into a tidy format is time consuming‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "",
    "text": "I recently visited my cousin Robyn in the United Kingdom. In her house there was, in addition to a standard tap in the kitchen sink, a tap that dispensed boiling water instantly. It was convenient, removing almost entirely the delay between the thought of tea and the first sip. But is it economically sound compared to a simple electric kettle?\nTLDR: I calculated that it would take  years for the instant hot water tap to be more cost-effective than a kettle. This is because the instant hot water tap is much more expensive than a kettle up-front, and the energy savings are minimal even under an improbable use case of  cups of tea per day.\nRead on to see how I came to this conclusion, and play with the assumptions in order to calculate how long it would take for the Quooker to be more cost-effective than a kettle for your family.1"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-is-an-instant-hot-water-tap",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-is-an-instant-hot-water-tap",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "What is an instant hot water tap?",
    "text": "What is an instant hot water tap?\nThe instant hot water tap system consists of a small, pressurised reservoir that is heated to 110 degrees Celsius and a tap to dispense the water. The system can be installed below a kitchen sink to minimise the distance and potential heat loss between the tap and the reservoir. Because the reservoir is pressurised, the water is unable to boil, even at 110 degrees Celsius. When the tap is opened, the pressure is released, and the water boils instantly.\nFor the remainder of the blog, I will refer to this system as a Quooker, a dutch brand which produces these taps, about which I could find the most information on the internet.\n\n\n\nOverview of instant hot water tap installation"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-does-it-compare-to-a-kettle",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-does-it-compare-to-a-kettle",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "How does it compare to a kettle?",
    "text": "How does it compare to a kettle?\nAlong with convenience, the Quooker also claims to be more energy efficient than a kettle by providing the exact amount of boiling water required for a cup of tea. This is in contrast to a kettle, which is often overfilled by the user, wasting energy and water.\nMy goal was thus to calculate for how long these energy savings would have to accrue before the Quooker became more cost-effective than a kettle. To do this, I ask GPT-4 to help me calculate the energy consumption of both the Quooker and a standard electric kettle."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#calculating-the-energy-consumption-of-the-quooker-and-a-kettle",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#calculating-the-energy-consumption-of-the-quooker-and-a-kettle",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Calculating the energy consumption of the Quooker and a kettle",
    "text": "Calculating the energy consumption of the Quooker and a kettle\n\nAssumptions\nFirst we pin down the scenarios under which the two systems are compared. My assumptions are:\n\n\n\n\n\n\n\n\n\nKettle Assumptions\nQuooker Assumptions\nOther Assumptions\n\n\n\n\nMy plastic kettle is rather old, and I have no idea what its efficiency is. I assume that it is % efficient.\nThe Quooker is % efficient.\nI drink  cups of tea per day.\n\n\nI overfill my kettle by  ml.\nThe Quooker is always on, maintaining the water at 110 degrees Celsius and using 10 Watts to do so.\nThe water in the kettle cools completely between each boil.\n\n\nThe kettle costs ‚Ç¨ .\nThe Quooker costs ‚Ç¨ .\nElectricity costs ‚Ç¨  per kWh.\n\n\n\n\n\n\nCalculations\nWe can then calculate the energy consumption of each system. The energy consumption of the kettle is calculated as follows:\n\\(q = mc\\Delta T\\) where:\n\nq is the heat energy required (in Joules),\nm is the mass of the water (in kg),\nc is the specific heat capacity of water (4200 J/kg¬∞C), and\nŒîT is the change in temperature (in ¬∞C).\n\nWe then convert the energy from Joules to kWh, and multiply by the electricity price to get the cost of boiling the kettle.\nEnergy (kWh) = Energy (Joules) / 3.6e6\nCost = Energy (kWh) * Cost per kWh\nPlugging in the numbers, we get the following calculation for the energy consumption of the kettle:\nBecause of overfilling the kettle, we must raise the temperature of g of water by ¬∞C each time we boil the kettle, even if we only use g of water in our tea. The kettle requires  kilojoules of energy, or  kWh each time we boil it. This is equivalent to ‚Ç¨  per day at  cups of tea per day.\nFor the Quooker, we get the following energy consumption:\nBecause the Quooker uses 10 Watts to maintain the water at 110 degrees Celsius, it uses 0.24 kWh per day in standby. This is equivalent to ‚Ç¨  per day. In addition, we have to heat g of water to 110 degrees Celsius, which requires  kilojoules of energy, or  kWh for each cup of tea. This is equivalent to ‚Ç¨  per day at  cups of tea per day.\nSo, the kettle uses ‚Ç¨  per year, and the Quooker uses ‚Ç¨  per year under our assumptions.\nGiven then price of the Quooker at ‚Ç¨  and the price of the kettle at ‚Ç¨ , the Quooker will pay for itself in  years."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#find-out-for-yourself",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#find-out-for-yourself",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Find out for yourself",
    "text": "Find out for yourself\nYou can adjust any of these assumptions in the inputs below to see how they affect the result. The entire article is dynamic, so adjust the inputs and read it again from the top to see how the calculations change and what the payoff period is in your case!2\n\nimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\nviewof kettle_efficiency= Inputs.range(\n  [50, 100], \n  {value: 70, step: 5, label: \"Kettle efficiency:\"}\n)\n\nviewof Quooker_efficiency= Inputs.range(\n  [50, 100], \n  {value: 90, step: 5, label: \"Quooker efficiency:\"}\n)\n\nviewof cups_per_day= Inputs.range(\n  [0, 40], \n  {value: 8, step: 1, label: \"Cups of tea per day:\"}\n)\n\nviewof kettle_overfill= Inputs.range(\n  [0, 1000], \n  {value: 250, step: 50, label: \"ml by which kettle is overfilled:\"}\n)\n\nviewof Quooker_price= Inputs.range(\n  [50, 2000], \n  {value: 1000, step: 50, label: \"Price of Quooker (Euro):\"}\n)\n\nviewof kettle_price= Inputs.range(\n  [50, 2000], \n  {value: 50, step: 50, label: \"Price of kettle (Euro):\"}\n)\n\nviewof electricity_price= Inputs.range(\n  [0, 1], \n  {value: .3, step: 0.05, label: \"Price of electricity per kWh (Euro):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nc = 4.184  // Specific heat capacity of water in J/g¬∞C\nT_change = 80  // Change in temperature for kettle (100¬∞C - 20¬∞C)\nT_change_Quooker = 90  // Change in temperature for Quooker (110¬∞C - 20¬∞C)\nwater_density = 1  // Density of water in g/mL\nmass_tea = 250 * water_density\nmass_overfill = kettle_overfill * water_density\nmass_kettle = mass_tea + mass_overfill\nQ_kettle = (mass_kettle * c * T_change) / (kettle_efficiency / 100)  // Energy in joules\nQ_kettle_kWh = Q_kettle / (3.6 * Math.pow(10, 6))  // Convert energy to kWh\ntotal_kettle_energy = Q_kettle_kWh * cups_per_day // Total energy for kettle per day\nQ_Quooker = (mass_tea * c * T_change_Quooker) / (Quooker_efficiency / 100)  // Energy in joules\nQ_Quooker_kWh = Q_Quooker / (3.6 * Math.pow(10, 6))  // Convert energy to kWh\ntotal_Quooker_energy = (Q_Quooker_kWh * cups_per_day) + 0.24  // Total energy for Quooker per day\nkettle_cost_daily = total_kettle_energy * electricity_price\nQuooker_cost_daily = total_Quooker_energy * electricity_price\nkettle_cost_yearly = kettle_cost_daily * 365\nQuooker_cost_yearly = Quooker_cost_daily * 365\n\nkettle_cost_yearly_round = Math.round(kettle_cost_yearly * 100) / 100\nQuooker_cost_yearly_round = Math.round(Quooker_cost_yearly * 100) / 100\n\nQ_kettle_wasted = (mass_overfill * c * T_change) / (kettle_efficiency / 100)\nQ_kettle_wasted_kWh = Q_kettle_wasted / (3.6 * Math.pow(10, 6))\ntotal_kettle_energy_wasted = Q_kettle_wasted_kWh * cups_per_day\ntotal_kettle_energy_wasted_round = Math.round(total_kettle_energy_wasted * 100) / 100\ntotal_kettle_energy_wasted_cost_round = Math.round(total_kettle_energy_wasted * electricity_price * 365)\npercentage_kettle_energy_wasted = Math.round(total_kettle_energy_wasted / total_kettle_energy * 100)\n\nQ_Quooker_wasted = 0.24\ntotal_Quooker_energy_wasted_cost_round = Math.round(Q_Quooker_wasted * electricity_price * 365)\npercentage_Quooker_energy_wasted_round = Math.round(Q_Quooker_wasted / total_Quooker_energy * 100)\n\nyears = (Quooker_price - kettle_price) / (kettle_cost_yearly - Quooker_cost_yearly)\nyears_round = Math.round(years * 100) / 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndt = aq.table({\n  years: Array.from({length: 200}, (_, i) =&gt; i + 1),\n  kettle: Array.from({length: 200}, (_, i) =&gt; {\n    return (i + 1) * kettle_cost_yearly + kettle_price\n  }),\n  Quooker: Array.from({length: 200}, (_, i) =&gt; {\n    return (i + 1) * Quooker_cost_yearly + Quooker_price\n  })\n})\n\n// Prepare the data in a format suitable for Plot.line\ndata = dt.objects().flatMap(({years, kettle, Quooker}) =&gt; [\n  {years, value: kettle, key: 'Kettle'},\n  {years, value: Quooker, key: 'Quooker'}\n]);\n\nPlot.plot({\n  y: {\n    grid: true,\n    label: 'Cumulative cost (Euro)'\n  },\n  x: {\n    grid: true,\n    label: 'Time (years)'\n  },\n  color: {\n    domain: [\"Kettle\", \"Quooker\"], legend: true, type: \"categorical\", range: [\"#1f77b4\", \"#ff7f0e\"]\n  },\n  marks: [\n    Plot.line(data, {x: 'years', y: 'value', stroke: 'key'})\n  ]})"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-much-energy-is-lost-by-each-system",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-much-energy-is-lost-by-each-system",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "How much energy is lost by each system?",
    "text": "How much energy is lost by each system?\n\n\n\n\n\n\n\n\n\nWaste Calculation\nKettle\nQuooker\n\n\n\n\nReason for waste\nDue to overfilling\nDue to heat loss\n\n\nkWh wasted per day\n kWh\n kWh\n\n\nPercentage of total energy\n%\n%\n\n\nCost of waste per year\n‚Ç¨ \n‚Ç¨"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-else-do-we-learn",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-else-do-we-learn",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "What else do we learn?",
    "text": "What else do we learn?\nAs the number of cups of tea increases, the Quooker becomes more cost-effective than the kettle, as the wasted energy is a fixed amount, while the wasted energy of the kettle increases linearly with the number of cups of tea, assuming the overfilling remains constant.\nReducing the amount by which we overfill the kettle decreases the energy loss and the cost of operating the kettle."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#appendix",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#appendix",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Appendix",
    "text": "Appendix\nI have assumed that the water in the kettle cools completely between each boil. This would require 2 hours between each boil, assuming an ambient temperature of 20 degrees Celsius and a cooling constant of 0.02. The cooling constant would change based on the specific kettle/environment. See the code below for the simulation.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import odeint\n\n# Define the function to compute derivative\ndef model(T, t):\n    T_ambient = 20  # Ambient temperature in degree Celsius\n    k = 0.02  # Cooling constant. This would change based on the specific kettle/environment\n    dTdt = -k * (T - T_ambient)\n    return dTdt\n\nT0 = 100  # Starting temperature of the water in degree Celsius\n\nt = np.linspace(0, 120, num=500)  # Time points in minutes. Here we simulate for 2 hours\n\n# Solve the ODE\nT = odeint(model, T0, t)\n\n# Plot the results\nplt.plot(t, T)\nplt.xlabel('Time (minutes)')\nplt.ylabel('Temperature (¬∞C)')\nplt.title('Cooling of Boiled Water in a Kettle Over Time')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#footnotes",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#footnotes",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe article is dynamic, so adjust the inputs and read it again from the top to see what the payoff period is in your case!‚Ü©Ô∏é\nIf for instance, you drink 25 cups of tea per day, the Quooker will pay for itself in 12.5 years.‚Ü©Ô∏é"
  },
  {
    "objectID": "posts/2025-04-05-tariffs-where-to-invest/tariffs-where-to-invest.html",
    "href": "posts/2025-04-05-tariffs-where-to-invest/tariffs-where-to-invest.html",
    "title": "Why I would buy stocks in US postcard manufacturers",
    "section": "",
    "text": "To say that the landscape of global trade has undergone a dramatic upheaval this week is an understatement. It seems that the current US administration believes in the aggressive use of tariffs to significantly alter the cost calculus for imports with the hope of forcing a major realignment of supply chains towards US shores. This analysis explores which publicly traded US companies might be relatively well-positioned to navigate this disruption, potentially benefiting from a pivot towards the domestic market."
  },
  {
    "objectID": "posts/2025-04-05-tariffs-where-to-invest/tariffs-where-to-invest.html#conclusion-navigating-uncertainty",
    "href": "posts/2025-04-05-tariffs-where-to-invest/tariffs-where-to-invest.html#conclusion-navigating-uncertainty",
    "title": "Why I would buy stocks in US postcard manufacturers",
    "section": "Conclusion: Navigating Uncertainty",
    "text": "Conclusion: Navigating Uncertainty\nThe current high-tariff environment presents a potential inflection point for US manufacturing. While significant opportunities could arise for domestic companies capable of capturing market share from expensive imports, the path is complex and uncertain. Success likely favors firms with strong North American operations, relevant product lines, and the agility to adapt production within a challenging multi-year timeframe, all while navigating retaliatory measures and domestic operational hurdles.\nThe persistence of these tariffs, the precise treatment of USMCA partners, and the practical ability of individual companies to execute a domestic pivot remain critical unknowns. Therefore, while this analysis identifies potential areas of interest, it underscores the need for deep, company-specific research and caution. This is just an economic framework for considering the potential industrial consequences of a dramatically altered trade policy."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html",
    "href": "posts/2023-03-10/chatgpt-as-translator.html",
    "title": "ChatGPT as translator and text wizard",
    "section": "",
    "text": "Data is the lifeblood of modern economic history. It courses through our models and charts the path of history. Yet data is often born unruly, its words jumbled and disorganized, its meaning buried under mountains of text. For the economic historian, this is an all too familiar challenge. We need to extract structure from chaos, to transform raw text into organized data that we can analyze.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts a handshake between a robot arm and a human arm\n\n\n\n\nBut doing so by hand is a laborious and time-consuming task, one that demands hours of tedious work and a heightened attention to detail. Luckily, we live in an age of powerful AI tools, such as ChatGPT, that can help us wrangle our data and make sense of the world."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#introduction",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#introduction",
    "title": "ChatGPT as translator and text wizard",
    "section": "",
    "text": "Data is the lifeblood of modern economic history. It courses through our models and charts the path of history. Yet data is often born unruly, its words jumbled and disorganized, its meaning buried under mountains of text. For the economic historian, this is an all too familiar challenge. We need to extract structure from chaos, to transform raw text into organized data that we can analyze.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts a handshake between a robot arm and a human arm\n\n\n\n\nBut doing so by hand is a laborious and time-consuming task, one that demands hours of tedious work and a heightened attention to detail. Luckily, we live in an age of powerful AI tools, such as ChatGPT, that can help us wrangle our data and make sense of the world."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#a-typical-problem",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#a-typical-problem",
    "title": "ChatGPT as translator and text wizard",
    "section": "A typical problem",
    "text": "A typical problem\nI am interested in a source of Swedish biographical data called Vem √§r Vem? (or Who is Who?), part of Projekt Runeberg, a volunteer effort to create free electronic editions of classic Scandinavian literature.\nIn particular I am interested in the occupational trajectories of the individuals - what job titles do they hold through the course of their lives. By extracting information on the career paths of notable Swedes, we can gain insights into the labour market dynamics of Sweden and how they have evolved over time. With this structured data, economic historians can explore questions such as the influence of education and social networks on career trajectories, the impact of technological change on the labour market, and much more.\nCareful work by two librarians in Uppsala has resulted in a large selection of the source being scanned and digitized. Though the OCR engine is not perfect, the resulting text is largely legible. A typical example of the source is shown below.\nThe city librarian in Uppsala Paul Harnesk was the initiator and chief editor of the work Who is Who? which was published in two editions, each comprising five parts. He died in 1965 and the work was completed by the first librarian at Uppsala University Library, associate professor √Öke Davidsson.\n\n\n\nAn example of the source, note the highlighted abbreviations wherein ‚Äúf.‚Äù means ‚Äúf√∂dd‚Äù and ‚ÄúMed. kand.‚Äù means ‚Äúmedicine kandidat‚Äù.**"
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#planning",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#planning",
    "title": "ChatGPT as translator and text wizard",
    "section": "Planning",
    "text": "Planning\nExtracting occupational trajectories from the Swedish biographical source ‚ÄúVem √§r Vem?‚Äù comprises a two-part process. The first part involves translating the Swedish text, which is full of abbreviations and contractions, into English using the natural language processing capabilities of ChatGPT. This step is crucial for making the data accessible and understandable to researchers who may not be familiar with Swedish (like myself!).\nOnce the text has been translated, the second part of the process demands structuring the data into year and occupation pairs. This involves identifying mentions of job titles and dates in the text and extracting them in a structured format."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-translator",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-translator",
    "title": "ChatGPT as translator and text wizard",
    "section": "ChatGPT as translator",
    "text": "ChatGPT as translator\nI simply ask ChatGPT:\nCan you convert the following text from Swedish into English? There are many abbreviations, for instance, ‚Äúf.‚Äù means ‚Äúf√∂dd‚Äù or born, ‚ÄúMed. kand.‚Äù means ‚Äúmedicine kandidat‚Äù.\nIt is a biography about a man:\nAnd I paste in the OCR output:\nAbramson, Ernst Leopold, professor, Djursholm, f.¬†i Grava, V√§rml. L, 12/3 /96 av gods√§g. Hugo A. o. Anna Sal√©n. Med. kand. Sthlm 18, med. lic. o. dr 27, doc. i fysiol. 27, labor. i fysiol. 30-38, prof. o. f√∂rest, v. statrs inst. f.¬†folkh√§lsan fr. 38. Batalj :l√§k. i f alt-l√§k rk√•r. res. 27, l√§r. i fysiol, o. r√∂rel-sefysiol. v. gymn. centr anst. 22-30, led. av arm√©f√∂rv :s sjukv rstyr. vetensk. r√•d 36, medl. av riksidr rf√∂rb. √∂v rstyr. 24-36, skolidr :f√∂rb. l√§k. 28, led. av direkt, √∂v. gymn. centr :inst. 33, ordf. d√§r 36, led. o. sekr. i n√§ringsr√•d. 36, ordf. i 36 √•rs livsmedelslagstiftn :-sakk. Utg. ett 50-tal arb. s√§rsk. r√∂r. n√§ringsfysiol., fysiol, mek., muskel-fysiol. RNO.\n\n\n\nScreenshot of input\n\n\nChatGPT returns to me the translated text, free from abbreviations and contractions:\n\nAbramson, Ernst Leopold, professor, Djursholm, born in Grava, V√§rmland, on March 12, 1896, to estate owner Hugo A. and Anna Sal√©n. He obtained a medicine kandidat degree in Stockholm in 1918, followed by a medical licentiate and doctorate in 1927. He was a docent in physiology in 1927 and worked as a laboratory assistant in physiology from 1930 to 1938. From 1938, he was a professor and director at the State Institute for Public Health. He served as a battalion doctor and reserve officer from 1927 and was a teacher in physiology and movement physiology at the Gymnastics Central Institute from 1922 to 1930. He was a member of the Army Medical Corps and Reserve Force from 1924 to 1936, a member of the Scientific Council of the Army Medical Service from 1936, and a member of the School Management Association of Doctors from 1928. He was also a member of the Directorate of the Gymnastics Central Institute from 1933, its chairman from 1936, and a member and secretary of the Nutrition Council from 1936. In addition, he was the chairman of the 1936 Food Legislation Committee. He authored around 50 works, primarily related to nutrition physiology, physiology, mechanics, and muscle physiology. He was awarded the Royal Order of the North Star (RNO).\n\nIt surprises me that ChatGPT is able to extrapolate correctly from just ‚ÄúRNO‚Äù (an abbreviation of the Swedish Nordstj√§rneorden) in the final part of the description to:\n\nHe was awarded the Royal Order of the North Star (RNO).\n\nFantastic!\n\n\n\nScreenshot of text output\n\n\nNext I ask ChatGPT to return to us the occupational information in a structured format."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-interpreter",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-interpreter",
    "title": "ChatGPT as translator and text wizard",
    "section": "ChatGPT as interpreter",
    "text": "ChatGPT as interpreter\nStrucutred data is our goal. Again I use a simple instruction to ask:\nGreat, thank you, can you extract the occupations he held and the dates for which he held them, and return the output in an R tibble with a column for start year, a column for end year (if any), a column for the Swedish description of the occupation, and a column for the English translation.\nIt returns the following R code:\n\nlibrary(tidyverse)\n\noccupations &lt;- tibble(\n  start_year = c(1918, 1927, 1927, 1930, 1938, 1927, 1922, 1924, 1936, 1928, 1933, 1936),\n  end_year = c(NA, NA, NA, 1938, NA, 1936, 1930, 1936, NA, NA, NA, NA),\n  swedish_occupation = c(\n    \"Med. kand.\", \"Med. lic. o. dr\", \"Docent i fysiol.\", \"Labor. i fysiol.\",\n    \"Prof. o. f√∂rest, v. stats inst. f. folkh√§lsan\", \"Batalj :l√§k. i f alt-l√§k rk√•r.\",\n    \"L√§r. i fysiol, o. r√∂rel-sefysiol. v. gymn. centr anst.\", \"Led. av arm√©f√∂rv :s sjukv rstyr.\",\n    \"Medl. av riksidr rf√∂rb.\", \"Skolidr :f√∂rb. l√§k.\", \"Led. av direkt, √∂v. gymn. centr :inst.\",\n    \"Ordf. i 36 √•rs livsmedelslagstiftn :-sakk.\"\n  ),\n  english_occupation = c(\n    \"Medicine kandidat\", \"Medical licentiate and doctorate\", \"Docent in physiology\",\n    \"Laboratory assistant in physiology\", \"Professor and director at the State Institute for Public Health\",\n    \"Battalion doctor and reserve officer\", \"Teacher in physiology and movement physiology at the Gymnastics Central Institute\",\n    \"Member of the Army Medical Corps and Reserve Force\", \"Member of the Scientific Council of the Army Medical Service\",\n    \"Member of the School Management Association of Doctors\", \"Member of the Directorate of the Gymnastics Central Institute\",\n    \"Chairman of the 1936 Food Legislation Committee.\"\n  )\n)"
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#displaying-the-output",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#displaying-the-output",
    "title": "ChatGPT as translator and text wizard",
    "section": "Displaying the output",
    "text": "Displaying the output\n\nTable\nI can easily take this code and turn the output into a table with a hyperlink to the source:\n\n\nCode\nlibrary(gt)\n\noccupations %&gt;%\n  arrange(start_year) %&gt;%\n  gt() %&gt;%\n  tab_header(title = md(\"**Ernst Leopold Abramson's career trajectory**\")) %&gt;%\n  cols_label(\n    start_year = md(\"Start\"),\n    end_year = md(\"End\"),\n    swedish_occupation = md(\"Swedish\"),\n    english_occupation = md(\"English\")\n  ) %&gt;%\n  sub_missing(columns = end_year, missing_text = \"\") %&gt;%\n  tab_spanner(md(\"**Duration**\"), columns = c(start_year, end_year)) %&gt;%\n  tab_spanner(md(\"**Occupation**\"), columns = c(swedish_occupation, english_occupation)) %&gt;%\n  tab_source_note(md(\"Data source: [Projekt Runeberg](http://runeberg.org/vemarvem/sthlm45/0018.html)\"))\n\n\n\n\n\n  \n    \n      Ernst Leopold Abramson's career trajectory\n    \n    \n  \n  \n    \n      \n        Duration\n      \n      \n        Occupation\n      \n    \n    \n      Start\n      End\n      Swedish\n      English\n    \n  \n  \n    1918\n\nMed. kand.\nMedicine kandidat\n    1922\n1930\nL√§r. i fysiol, o. r√∂rel-sefysiol. v. gymn. centr anst.\nTeacher in physiology and movement physiology at the Gymnastics Central Institute\n    1924\n1936\nLed. av arm√©f√∂rv :s sjukv rstyr.\nMember of the Army Medical Corps and Reserve Force\n    1927\n\nMed. lic. o. dr\nMedical licentiate and doctorate\n    1927\n\nDocent i fysiol.\nDocent in physiology\n    1927\n1936\nBatalj :l√§k. i f alt-l√§k rk√•r.\nBattalion doctor and reserve officer\n    1928\n\nSkolidr :f√∂rb. l√§k.\nMember of the School Management Association of Doctors\n    1930\n1938\nLabor. i fysiol.\nLaboratory assistant in physiology\n    1933\n\nLed. av direkt, √∂v. gymn. centr :inst.\nMember of the Directorate of the Gymnastics Central Institute\n    1936\n\nMedl. av riksidr rf√∂rb.\nMember of the Scientific Council of the Army Medical Service\n    1936\n\nOrdf. i 36 √•rs livsmedelslagstiftn :-sakk.\nChairman of the 1936 Food Legislation Committee.\n    1938\n\nProf. o. f√∂rest, v. stats inst. f. folkh√§lsan\nProfessor and director at the State Institute for Public Health\n  \n  \n    \n      Data source: Projekt Runeberg\n    \n  \n  \n\n\n\n\n\n\nVisualization\nAnd we can take the output and make it into a nice figure that visualizes Ernst‚Äôs career trajectory.\n\n\nCode\noccupations %&gt;%\n  arrange(start_year) %&gt;%\n  mutate(end_year_imputed = case_when(\n    is.na(end_year) ~ lead(start_year),\n    is.na(lead(start_year)) ~ start_year,\n    TRUE ~ end_year\n  )) %&gt;%\n  mutate(\n    english_occupation = str_squish(str_remove_all(english_occupation, \"[:punct:]\")),\n    english_occupation = fct_reorder(english_occupation, start_year, .desc = TRUE)\n  ) %&gt;%\n  # pivot_longer(c(start_year, end_year_imputed)) %&gt;%\n  # ggplot(aes(value, english_occupation, colour = name)) +\n  ggplot(aes(xmin = start_year, xmax = end_year_imputed, y = english_occupation)) +\n  geom_linerange(colour = \"gray50\", size = 2) +\n  geom_point(aes(x = end_year_imputed), colour = \"#be5d2b\", size = 4) +\n  geom_point(aes(x = start_year), colour = \"#2B8CBE\", size = 4) +\n  labs(\n    title = \"Ernst Leopold Abramson's career trajectory\",\n    subtitle = \"Showing &lt;span style = 'color:#2B8CBE;'&gt;start dates&lt;/span&gt; and &lt;span style = 'color:#be5d2b;'&gt;end dates&lt;/span&gt; for his various roles\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.subtitle = element_markdown(),\n    plot.title.position = \"plot\",\n    text = element_text(family = \"ibm\", size = 28)\n  )"
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#conclusion",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#conclusion",
    "title": "ChatGPT as translator and text wizard",
    "section": "Conclusion",
    "text": "Conclusion\nThank you for reading along. I have demonstrated how we can use unstructured text data to reconstruct career trajectories for notable Swedes, translating and structuring the biographical text data through the ChatGPT web interface.\nNext week I‚Äôll share a post on how to take this process and scale it by leveraging OpenAI‚Äôs API.\nI hope that it might prove useful in your own research!"
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "title": "Our World In Data choropleth",
    "section": "",
    "text": "I really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, ‚Äúmake progress against the world‚Äôs largest problems‚Äù.\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser last year, included below.\n\n\n\n\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#purpose",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#purpose",
    "title": "Our World In Data choropleth",
    "section": "",
    "text": "I really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, ‚Äúmake progress against the world‚Äôs largest problems‚Äù.\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser last year, included below.\n\n\n\n\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#finished-product",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#finished-product",
    "title": "Our World In Data choropleth",
    "section": "Finished product",
    "text": "Finished product\nHere is the finished product that we will create in the tutorial"
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "title": "Our World In Data choropleth",
    "section": "Context",
    "text": "Context\nThere is a lot of information about the OWID grapher tool. You can have a look at their github repo and an older reddit AMA if you are interested. It‚Äôs a custom system that allows for systematic changes across their website, drawing on data from a central database."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "title": "Our World In Data choropleth",
    "section": "Components",
    "text": "Components\nWhat are the parts I want to recreate? The map has:\n\na base map, where the colour fill of each country indicates it‚Äôs position in a specific measure in a particular year.\na simple line chart that appears when you hover over a country, showing how the measure has changed within a country over time.\na clear legend\na note specifying the source of the data\n\nI walk through creating each of these below.\n\nThe world map\nThe base map is sourced from the maps package. I add a three letter country code from the english name of the country using the countrycode package and filter out Antarctica, Greenland and the French Southern and Antarctic Lands.\n\n\nShow the code\n# preamble\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n# load map\nmap &lt;- st_as_sf(maps::map(database=\"world\", plot = FALSE, fill = TRUE))\n\n# create code to match coutnry to data with\nlibrary(countrycode)\nmap &lt;- map %&gt;% \n  mutate(code = countrycode(ID, \"country.name\", \"iso3c\"))\n\n# remove clutter from map\ncountry_to_remove &lt;- c(\n  'Antarctica','Greenland', 'French Southern and Antarctic Lands'\n)\n\nmap &lt;- map %&gt;% \n  filter(!ID %in% country_to_remove)\n\n\nThe base map is projected with the Web Mercator or WGS 84 projection, the same one Google Maps uses.\n\n\nShow the code\nmap %&gt;% \n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\n\n\n\n\nData\nI source the data from the Our World in Data website (the WHO collects the data and the World Bank distributes it). We read in the data as a CSV file, and tidy up the column names so that they are in snake case with the clean_names() command from the very helpful janitor package.\n\n\nShow the code\ndf &lt;- read.csv(\"data/share-of-adults-who-smoke.csv\")\n\ndf &lt;- df %&gt;% \n  as_tibble() %&gt;% \n  dplyr::rename(value = Prevalence.of.current.tobacco.use....of.adults.) %&gt;% \n  janitor::clean_names()\n\n\nNext we remove the summary groups which we cannot show on the map, including the World Bank country income groupings.\n\n\nShow the code\ndf %&gt;% \n  filter(!code %in% map$code) %&gt;% \n  distinct(entity)\n\n\n# A tibble: 16 √ó 1\n   entity                      \n   &lt;chr&gt;                       \n 1 East Asia and Pacific       \n 2 Europe and Central Asia     \n 3 European Union              \n 4 High income                 \n 5 Latin America and Caribbean \n 6 Low and middle income       \n 7 Low income                  \n 8 Lower middle income         \n 9 Middle East and North Africa\n10 Middle income               \n11 North America               \n12 South Asia                  \n13 Sub-Saharan Africa          \n14 Tuvalu                      \n15 Upper middle income         \n16 World                       \n\n\nShow the code\ndf &lt;- df %&gt;% \n  filter(code %in% map$code)\n\n\n\n\nCreate a colour palette\nWhat we want to do is use the scale_color_viridis_c() palette. We have to map it to the min and max of the values in our dataset for smoking so that we get a nice mapping across the colour palette.\n\n\nShow the code\ndf %&gt;%\n  summarise(\n    min = min(value),\n    max = max(value)\n  )\n\n\n# A tibble: 1 √ó 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   3.5  68.5\n\n\nShow the code\nlibrary(viridisLite) \n\nvir_10 &lt;- viridis(n = 10)\n\nsmoking_hex &lt;- scales::gradient_n_pal(\n  colours = vir_10,\n  values = seq(0, 50, by = 5)\n)\n\nscale_smoking &lt;- function() {\n  scale_color_gradientn(\n    colours = vir_10,\n    values = seq(0, 50, by = 5) / 50,\n    limits = c(0, 68.5),\n    name = \"value\"\n  )\n}\n\n\n\n\nHow to plot the line graph?\nThe line graph that appears when you hover over OWID map is very simple. It has just the starting value on the y-axis, and the first and last years on the x-axis, and a line that changes colour in accordance with the scale of the choropleth. The hover window which contains the graph also shows the country name, and the value of the measure in the most recent year.\nTo recreate it, we need store these four values, and draw the coloured line.\n\n\n\nA function for plotting the graph\nFirst we write a function to plot the line that is a very minimal ggplot, removing lots of superfluous elements with the theme() command.\n\n\nShow the code\n# Here the function to plot the line takes only one argument, `cd` the country code \nplot_line &lt;- function(cd) {\n  # get axis marks\n  label_y &lt;- df %&gt;%\n    filter(code == cd) %&gt;%\n    mutate(\n      min_year = min(year),\n      max_year = max(year)\n    ) %&gt;%\n    filter(year == min(year))\n  \n  # plot the line\n  df %&gt;%\n    filter(code == cd) %&gt;%\n    ggplot(aes(year, value)) +\n    geom_point(cex = 3) +\n    # mapping the colour of the line segment to the value is done here\n    geom_line(aes(colour = value), cex = 2, alpha = .7) +\n    # this scale is created above, with bounds appropriate to this data\n    scale_smoking() +\n    scale_y_continuous(\n      # specifying the break on the y-axis creates the axis text\n      breaks = c(label_y$value),\n      labels = scales::percent_format(scale = 1, accuracy = .1),\n      # the limits argument here ensures the y-axis starts at zero\n      limits = c(0, NA)\n    ) +\n    scale_x_continuous(\n      # x-axis needs only two years, the first and last\n      breaks = c(label_y$min_year, label_y$max_year)) +\n    theme(\n      # removing the axis ticks and lines clears the graph of clutter\n      axis.ticks.y = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor.x = element_blank(),\n      legend.position = \"none\",\n      text = element_text(size = 20)\n    ) +\n    labs(\n      x = NULL,\n      y = NULL\n    )\n}\n\n# Test the function with India.\nplot_line(\"IND\")\n\n\n\n\n\n\n\n\n\n\n\nNext to make the table that displays the most recent value\nWe write a function to make a gt() table that contains just two cells. In the left cell of the table is our line graph from above that shows the evolution of smoking rates in each country from 2000 onwards. In the right cell is the most recent value for the metric.\n\n\nShow the code\nlibrary(gt)\nlibrary(gtExtras)\n\nmake_table &lt;- function(cd) {\n  message(\"Making table for \", cd)\n  vars &lt;- df %&gt;%\n    filter(code == cd) %&gt;%\n    filter(year == max(year))\n\n  plot &lt;- plot_line(cd)\n\n  # the `ggplot_image` command outputs an image that can easily be put into a gt table when formatted as markdown\n  plot &lt;- gt::ggplot_image(plot, height = px(250), aspect_ratio = 1.6)\n\n  tbl &lt;- tibble(plot = plot, value = vars$value, context = glue::glue(\"in {vars$year}\"))\n\n  gt(tbl) %&gt;%\n    fmt_markdown(columns = c(plot)) %&gt;%\n    fmt_percent(value, scale_values = F, decimals = 1) %&gt;% \n    # the `merge_stack` command joins the value and the year in one cell\n    # the `smoking_hex` function we created above makes the text coloured appropriately\n    gt_merge_stack(col1 = value, col2 = context, palette = c(smoking_hex(tbl$value), \"grey\")) %&gt;%\n    tab_style(\n      style = cell_text(size = \"xx-large\"),\n      locations = cells_body(\n        columns = c(value)\n      )\n    ) %&gt;%\n    tab_header(\n      # title table with coutnry name\n      title = md(glue::glue(\"**{vars$entity}**\"))) %&gt;%\n    tab_options(column_labels.hidden = TRUE) %&gt;% \n    as_raw_html(inline_css = F)\n}\n\n# Test on South Africa\nmake_table(\"ZAF\")\n\n\n\n  \n  \n\n\n\n\n\n\n\nSouth Africa\n\n\n\n\n\n\n\n\n\n20.3%\n\n\nin 2020\n\n\n\n\n\n\n\n\n\n\nCreating the plots for each country\nHere we use the purrr::map command to make the table in raw HTML for each country and save it inside a tibble. The output shows an HTML list in the column called gt.\n\n\nShow the code\ngt_tables &lt;- df %&gt;% \n  distinct(code) %&gt;%\n  mutate(gt = purrr::map(code, make_table))\n\ngt_tables\n\n\nWe then create a tibble called df_map that selects the most recent year for each country from the dataset and joins it to the map by the country code variable we created above. Finally we join this to the tibble of tables called gt_tables.\n\n\nShow the code\ndf_map &lt;- df %&gt;% \n  group_by(entity) %&gt;% \n  filter(year == max(year)) %&gt;% \n  ungroup() %&gt;% \n  left_join(map, by = c(\"code\"))\n\ndf_map &lt;- df_map %&gt;% \n  inner_join(gt_tables)\n\n\n\n\nCreating the interactive figure\nNow we are ready to create the interactive figure!\nWe begin by drawing a static map in grey, with data from the original map. Next we overlay the interactive choropleth. The grey static map will show through all the countries we don‚Äôt have data on in the dataset.\n\n\nShow the code\ng &lt;- df_map %&gt;%\n  ggplot(aes(geometry = geom)) +\n  geom_sf(data = map, fill = \"grey80\") +\n  geom_sf_interactive(aes(fill = value, tooltip = gt)) +\n  scale_fill_binned(type = \"viridis\", labels = scales::percent_format(scale = 1)) +\n  cowplot::theme_minimal_grid() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = .5, title.position = \"top\", label = TRUE)) +\n  labs(\n    fill = \"Share of adults who smoke, 2020\",\n    caption = \"Source: World Health Organization (via World Bank)\"\n  ) +\n  theme(\n    plot.background = element_rect(fill = \"white\", color = NA),\n    plot.title = element_text(hjust = 0.5, family = \"marker\", size = 50),\n    plot.subtitle = element_markdown(size = 20, family = \"open\", lineheight = 0.5),\n    plot.caption = element_markdown(size = 12, family = \"open\"),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid.major = element_line(color = \"grey80\", size = 0.1),\n    legend.title.align = .5\n  )\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow off the interactive figure\nWow! Have a look at that! Pretty neat, and similar to the OWID explorer. I might make this more extendable in the future that you can upload your own data, but that‚Äôs for another day.\n\n\nShow the code\n# customizing the CSS makes the hover box easier to read.\ntooltip_css &lt;- \"background-color:gray;color:white;padding:10px;border-radius:5px;text-align:center;\"\n\nggiraph(\n  ggobj = g,\n  options = list(\n    opts_tooltip(css = tooltip_css),\n    opts_sizing(width = 1)\n  )\n)\n\n\n\n\n\n\n\n\nWhich African country has seen the largest reduction in smoking?\nFinally, we can plot the evolution of smoking behaviour among adults in Africa, highlighting my home nation of South Africa.\n\n\nShow the code\nget_ranking &lt;- function(continent_in) {\n  message(\"Getting ranking for \", continent_in)\n\n  continent_out &lt;- df %&gt;%\n    mutate(continent = countrycode(code, \"iso3c\", \"continent\")) %&gt;%\n    filter(continent == continent_in)\n\n  df_rank &lt;- continent_out %&gt;%\n    select(year, country_name = entity, value) %&gt;%\n    distinct()\n\n  df_rank &lt;- df_rank %&gt;%\n    group_by(year) %&gt;%\n    mutate(rank = rank(desc(value), ties.method = \"first\")) %&gt;%\n    ungroup()\n\n  labels_left &lt;- df_rank %&gt;%\n    filter(year == min(year)) %&gt;%\n    mutate(\n      left_rank = rank,\n      left_value = value\n    ) %&gt;%\n    select(country_name, left_rank, left_value)\n\n  labels_right &lt;- df_rank %&gt;%\n    filter(year == max(year)) %&gt;%\n    mutate(\n      right_rank = rank,\n      right_value = value\n    ) %&gt;%\n    select(country_name, right_rank, right_value)\n\n  df_rank &lt;- df_rank %&gt;%\n    inner_join(labels_left) %&gt;%\n    inner_join(labels_right)\n  \n  return(df_rank)\n}\n\nafrica_rank &lt;- get_ranking(\"Africa\")\n\n\n\n\nShow the code\nplot_levels &lt;- function(continent_in, highlight_country_in){\n  \n  tbl &lt;- get_ranking(continent_in)\n  \n  midpoint_in &lt;- tbl %&gt;% \n    summarise(mean(value)) %&gt;% \n    pull()\n  \n  tbl %&gt;%\n  mutate(across(contains(\"value\"), ~ round(.x, digits = 1))) %&gt;%\n  ggplot(aes(year, value, colour = right_value, group = country_name)) +\n  geom_line(size = 2.8, aes(year, value, group = country_name), colour = \"black\") +\n  geom_line(size = 2) +\n  geom_line(size = 3, colour = \"black\", data = tbl %&gt;% filter(country_name == highlight_country_in)) +\n  geom_text(aes(\n    x = 2000,\n    y = left_value,\n    label = paste0(country_name, \" \", left_value, \" %\")\n  ),\n  check_overlap = T,\n  colour = \"grey20\",\n  hjust = 1.1,\n  cex = 3\n  ) +\n  geom_text(aes(\n    x = 2020,\n    y = right_value,\n    label = paste0(country_name, \" \", right_value, \" %\")\n  ),\n  check_overlap = T,\n  colour = \"grey20\",\n  hjust = 0,\n  cex = 3\n  ) +\n  # scale_y_reverse() +\n  scale_color_gradient2(\n    low = \"#3C5488\",\n    high = \"#00A087\",\n    mid = \"#4DBBD5\",\n    midpoint = midpoint_in\n  ) +\n  scale_x_continuous(breaks = c(2000, 2005, 2010, 2015, 2018, 2019, 2020)) +\n  coord_cartesian(xlim = c(1995, 2025)) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percentage of adults who smoke\",\n    title = glue::glue(\"Evolution of percentage of adults who smoke in {continent_in}\"),\n    subtitle = glue::glue(\"{highlight_country_in} is highlighted in black\"),\n    caption = \"Data: WHO via World Bank\\nGraphic: @JonathanJayes\"\n  )\n  \n}\n\n\n# jpeg(\n#   filename = \"posts/2022-10-17-our-world-in-data-choropleth/images/Africa_levels.jpeg\",\n#   height = 10,\n#   width = 8,\n#   units = \"in\",\n#   res = 1000\n# )\n\nplot_levels(continent_in = \"Africa\",\n            highlight_country_in = \"South Africa\")\n\n\n\n\n\n\n\n\n\nShow the code\n# dev.off()\n\n\nHmm, South Africa has seen a drop in smoking since 2000, but the drop is relatively small in comparison to the progress that other African nations have made in the last two decades.\nWe can also plot the evolution of the ranking of who smokes the most in Africa.\n\n\nShow the code\nplot_ranking &lt;- function(continent_in, highlight_country_in){\n  \n  tbl &lt;- get_ranking(continent_in)\n  \n  midpoint_in &lt;- tbl %&gt;% \n    distinct(country_name) %&gt;% \n    count() %&gt;% \n    pull() / 2\n  \n  tbl %&gt;%\n    mutate(across(contains(\"value\"), ~ scales::percent(.x, scale = 1, accuracy = 1))) %&gt;%\n    ggplot(aes(year, rank, colour = right_rank, group = country_name)) +\n    geom_line(size = 2.8, aes(year, rank, group = country_name), colour = \"black\") +\n    geom_line(size = 2) +\n    geom_line(size = 3, colour = \"black\", data = tbl %&gt;% filter(country_name == highlight_country_in)) +\n    geom_text(aes(\n      x = 2000,\n      y = left_rank,\n      label = paste0(left_rank, \". \", country_name, \" \", left_value)\n    ),\n    colour = \"grey20\",\n    hjust = 1.1,\n    cex = 3\n    ) +\n    geom_text(aes(\n      x = 2020,\n      y = right_rank,\n      label = paste0(right_rank, \". \", country_name, \" \", right_value)\n    ),\n    colour = \"grey20\",\n    hjust = 0,\n    cex = 3\n    ) +\n    scale_y_reverse() +\n    scale_color_gradient2(\n      low = \"blue\",\n      high = \"red\",\n      mid = \"pink\",\n      midpoint = midpoint_in\n    ) +\n    scale_x_continuous(breaks = c(2000, 2005, 2010, 2015, 2018, 2019, 2020)) +\n    coord_cartesian(xlim = c(1995, 2025)) +\n    theme(\n      legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      axis.text.y = element_blank()\n    ) +\n    labs(\n      x = NULL,\n      y = \"Ranking of adults who smoke\",\n      title = glue::glue(\"Evolution of ranking of adults who smoke in {continent_in}\"),\n      subtitle = glue::glue(\"{highlight_country_in} is highlighted in black\"),\n      caption = \"Data: WHO via World Bank\\nGraphic: @JonathanJayes\"\n    )\n}\n\n# jpeg(\n#   filename = \"posts/2022-10-17-our-world-in-data-choropleth/images/Africa_ranking.jpeg\",\n#   height = 10,\n#   width = 8,\n#   units = \"in\",\n#   res = 1000\n# )\n\nplot_ranking(\"Africa\", \"South Africa\")\n\n\n\n\n\n\n\n\n\nShow the code\n# dev.off()\n\n\nNow we can see that South Africa has climbed the rankings from 18th place in 2000 up to 7th in 2020. Interesting!\n\n\nWhat about the other continents?\nHave a look at each below, and then give my Shiny app a gander (TODO).\n\n\nShow the code\nmake_tab &lt;- function(continent_in, highlight_country_in) { # function to make the tabs\n  cat(\"##\", continent_in) # Label tab\n  cat(\"\\n\") # Close tab\n  p &lt;- plot_levels(\n    continent_in,\n    highlight_country_in\n  ) # Create plot in levels\n  print(p) # Display plot\n  cat(\"\\n\") # Space\n  q &lt;- plot_ranking(\n    continent_in,\n    highlight_country_in\n  ) # Create plot in levels\n  print(q)\n  cat(\"\\n\") # Close tab\n}\n\n\n\nAmericasAsiaEuropeOceania\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmake_tab(\"Asia\", \"China\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmake_tab(\"Europe\", \"Sweden\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nShow the code\nmake_tab(\"Oceania\", \"New Zealand\")"
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html",
    "title": "How Rare-Earth Element Dependency Threatens the Global Wind Rollout",
    "section": "",
    "text": "A couple of weeks back, I read an article in the FT about √òrsted, the big Danish offshore wind developer. Its stock was getting hammered. Part of the story involved project hiccups off the coast of the United States, but another significant factor was the escalating trade tension, particularly involving US tariffs under the Trump administration. I initially dismissed the tariffs as ‚Äúpolitical noise,‚Äù figuring the underlying green transition trend was solid enough to weather it. This looked like a potential buy-the-dip opportunity. So my colleague Jonas and I each bought one share each, just to keep an eye on it.\nIt wasn‚Äôt our finest investment idea. After some short-lived gains, the stock slumped further. It is down 22 percent year to date.\nOur mistake wasn‚Äôt just misjudging the company‚Äôs immediate prospects. It was more fundamental. I‚Äôd failed to appreciate how deeply intertwined the green energy revolution is with complex global supply chains and how vulnerable those chains are to geopolitical actions. What I‚Äôd dismissed as ‚Äúpolitical noise‚Äù wasn‚Äôt just noise ‚Äì it was rapidly escalating into direct actions targeting the very materials essential for green technology. Building wind turbines isn‚Äôt just about steel and fiberglass; it‚Äôs about a sprawling supply chain with precarious dependencies, especially on rare earth metals. My √òrsted bet foundered partly because I didn‚Äôt grasp the machine itself, where its most critical parts really come from, and how directly exposed they are to the current trade conflict."
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html#the-giants-we-see-vs.-the-materials-we-dont",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html#the-giants-we-see-vs.-the-materials-we-dont",
    "title": "How Rare-Earth Element Dependency Threatens the Global Wind Rollout",
    "section": "The Giants We See vs.¬†The Materials We Don‚Äôt",
    "text": "The Giants We See vs.¬†The Materials We Don‚Äôt\nWhen you think of a wind farm, you picture the obvious things: the enormous towers reaching for the sky, the massive blades sweeping through the air. These are engineering marvels, and their sheer size dictates a lot about how they‚Äôre made and sourced.\n\n\n\n\n\nA ship loaded with components for an offshore wind turbine installation\n\n\nPhoto by Rob Webbon on Unsplash\nTowers are typically huge steel tubes, sometimes concrete, assembled on site1,2. Blades are complex composite structures, often using fiberglass or carbon fiber, resins, and lightweight core materials like balsa wood or synthetic foams3‚Äì5. Because these components are so gigantic and unwieldy, shipping them long distances is incredibly expensive and logistically grisly6,7,7. Consequently, manufacturing for towers and blades tends to be regionalized. You see factories popping up relatively close to the major wind markets in the US, Europe, China, India, and Brazil8‚Äì11.\n\n\n\nWind Turbine Components, showing the gearbox style turbine\n\n\nThe raw materials themselves seem somewhat diversified, at first glance. Steel is a global commodity, though regional suppliers are often preferred for towers12,13. Fiberglass and resins come from the global chemical industry14. While China is a major consumer of balsa wood for blades, the primary source for balsa is Ecuador15, and alternatives like PET foam are increasingly used5,16.\nSo, looking just at these colossal, visible parts, the supply chain seems somewhat distributed. Logistics, not geopolitics, appears to be the main driver. The direct dependency on any single country, including China, looks manageable for these specific components. But this surface-level view is deceptive. The real story, and the real, active vulnerability being exposed by current events, lies hidden inside the box atop the tower."
  },
  {
    "objectID": "posts/2025-04-17/limits-of-multimodal-llms.html",
    "href": "posts/2025-04-17/limits-of-multimodal-llms.html",
    "title": "Untitled",
    "section": "",
    "text": "The letter 'r' appears 3 times in the word 'strawberry'."
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html#deep-dive-the-magnetic-heart-chinas-dominance",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html#deep-dive-the-magnetic-heart-chinas-dominance",
    "title": "The Rare Earth Grip: My Miscalculation and Wind Power‚Äôs Chokepoint",
    "section": "Deep Dive: The Magnetic Heart & China‚Äôs Dominance",
    "text": "Deep Dive: The Magnetic Heart & China‚Äôs Dominance\nSo, the big blades and towers look relatively straightforward from a geopolitical sourcing perspective. But the real action, and the real vulnerability, is inside the nacelle ‚Äì the house-sized box sitting atop the tower that contains the turbine‚Äôs power-generating guts (Goldwind Americas 2025; Goldwind 2025).\nA key component here, especially for many efficient modern designs like direct-drive turbines (which ditch the heavy gearbox), is the Permanent Magnet Synchronous Generator (PMSG) [MarketsandMarkets (2025); UserPromptInfo]. These generators rely on incredibly powerful permanent magnets, specifically Neodymium-Iron-Boron (NdFeB) magnets, to convert rotational energy into electricity efficiently [Polytechnique Insights (2025); UserPromptInfo].\nThese aren‚Äôt your average fridge magnets. They require specific Rare Earth Elements (REEs) ‚Äì primarily Neodymium (Nd) and Praseodymium (Pr), often boosted with smaller amounts of Dysprosium (Dy) and Terbium (Tb) for high-temperature performance [Polytechnique Insights (2025); UserPromptInfo]. And here‚Äôs the crux: while REEs are mined in various places (including the US and Australia), the complex, dirty, and technologically challenging task of separating and processing these elements into usable oxides and metals is overwhelmingly dominated by one country: China. China controls over 85% of global REE processing capacity [Polytechnique Insights (2025); Polytechnique Insights (2025); UserPromptInfo].\nBut it doesn‚Äôt stop there. China also dominates the next crucial step ‚Äì manufacturing the finished high-performance NdFeB magnets themselves, accounting for around 90-92% of global production [Polytechnique Insights (2025); Polytechnique Insights (2025); UserPromptInfo]. This near-monopoly on both processing and magnet manufacturing creates a massive strategic vulnerability for the entire global wind industry (and other sectors like EVs and electronics).\n\n\n\nREE Supply Chain Infographic\n\n\nWhy is this dependency so hard to break? Building new REE processing plants and magnet factories outside China requires enormous long-term investment, navigating environmental hurdles, and developing specialized technical expertise [Polytechnique Insights (2025); UserPromptInfo]. Efforts are underway, including reviving processing in the US and developing recycling technologies, but scaling these alternatives to meet the massive demand from the energy transition is a slow, expensive, and uncertain process [Polytechnique Insights (2025); NORTHEAST - NEWS CHANNEL NEBRASKA (2025); UserPromptInfo]. Adding another layer of risk, China has already implemented export restrictions on REE extraction and separation technologies, and signals suggest it could potentially restrict magnet exports too, giving it significant geopolitical leverage [Discovery Alert (2025); CSIS (2025); UserPromptInfo].\nThis REE choke point is the single most critical dependency in the wind turbine supply chain. While other components present challenges, none combine this level of near-total reliance on a single country with such difficulty in finding or building large-scale alternatives.\nHere‚Äôs a quick breakdown of the dependency levels for key components:\n\nChina Dependency Analysis for Wind Turbine Components (Source: User Provided Info & Cited Docs)\n\n\n\n\n\n\n\n\nComponent\nKey Materials/Processes Involved\nChina‚Äôs Role & Dependency Level\nDifficulty Sourcing Alternatives at Scale\n\n\n\n\nBlades\nFiberglass, Carbon Fiber, Resins, Core (Balsa, PET/PVC Foam) (Santie Oil Company 2025)\nModerate: Major manufacturing hub (incl.¬†for Western OEM exports (Windmills Tech 2025)); Major consumer of raw materials (e.g., balsa (Energy Follower 2025)).\nLow-Moderate: Balsa sourced from Ecuador (U.S. Geological Survey 2025a); PET foam alternative global (Energy Follower 2025); Fibers/resins global (Santie Oil Company 2025); Regional mfg capacity exists.\n\n\nTowers\nSteel (predominantly), Concrete (Department of Energy 2025b)\nLow: Major steel producer & tower manufacturer for domestic market. (Crosby Airpes 2025)\nLow: Steel is global commodity; Tower mfg highly regionalized (Crosby Airpes 2025); Capacity exists in key markets (Crosby Airpes 2025).\n\n\nNacelle Assembly\nIntegration of internal components (4C Offshore 2025)\nModerate: Major global assembly hub, including for Western OEM exports. (Windmills Tech 2025)\nLow (for assembly): Assembly capacity exists globally (Department of Energy 2025a). High dependency relates to specific internal components (see below).\n\n\nGearbox\nSteel alloys, Gears, Bearings (Rabobank 2025)\nModerate-High: Major manufacturing location; Key domestic suppliers (e.g., NGC). (Wood Mackenzie 2024)\nModerate: Concentrated market, but key suppliers also exist in Europe (ZF/Winergy, Moventas etc.). (Wood Mackenzie 2024)\n\n\nGenerator (PMSG)\nCopper, Steel, REE Magnets (Nd, Pr, Dy, Tb) (Department of Energy 2025b)\nVery High (for Magnets): Dominates REE processing (&gt;85%) and magnet manufacturing (~90-92%) (BloombergNEF 2025); Export controls (GWEC 2024).\nVery High (for Magnets): Alternatives (non-China processing, recycling) limited/developing slowly. (Mercom India 2025)\n\n\nGenerator (Other)\nCopper, Steel, Insulation (Department of Energy 2025b)\nLow-Moderate: Major manufacturing location, but global suppliers and capacity exist. (Renewables.Digital 2024b)\nLow: Global commodity materials; Established global suppliers (ABB, Siemens Energy etc.). (Renewables.Digital 2024b)\n\n\nBearings\nSteel alloys\nModerate-High (Assumed): Major industrial component hub; Potential US bottleneck. (Enel Green Power 2025)\nModerate: Specialized; Global mfrs exist but scaling alternatives for large turbines might be challenging. (Enel Green Power 2025)\n\n\nPower Electronics\nSemiconductors, Copper, Aluminum, Plastics (Department of Energy 2025b)\nHigh (for Components): Major hub for electronics manufacturing; Reliance on Asian semiconductor supply chain.\nModerate-High (for Components): Diversifying semiconductor/electronics supply chains is complex and long-term. (Renewables.Digital 2024b)"
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html#beyond-magnets-other-pressure-points",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html#beyond-magnets-other-pressure-points",
    "title": "How Rare-Earth Element Dependency Threatens the Global Wind Rollout",
    "section": "Beyond Magnets: Other Pressure Points",
    "text": "Beyond Magnets: Other Pressure Points\nOkay, so the reliance on China for rare earth magnets is the elephant in the room ‚Äì and recent export controls confirm it‚Äôs an elephant that can be prodded for geopolitical leverage. It‚Äôs the most stark example of a critical dependency. But it‚Äôs not the only potential pressure point hidden within the wind turbine‚Äôs supply chain, even if others are less severe.\nThink about gearboxes, essential for many turbine designs that aren‚Äôt direct-drive25. These are complex, high-precision machines. While major European suppliers like ZF/Winergy, Moventas, and Renk exist, China is also a key manufacturing hub and home to major players like China High Speed Transmission (NGC)33,39. The market for these specialized components is relatively concentrated globally, meaning diversifying away from existing players, including Chinese ones, isn‚Äôt necessarily easy32.\nThen there are the large, specialized bearings needed for the main shaft rotation, blade pitch adjustment, and nacelle yaw movement25,40. Specific sourcing data can be scarce, but these have been flagged as potential bottlenecks, particularly as turbines get bigger and for the growing US offshore market23,38. While global bearing manufacturers exist, scaling up alternative production quickly for these massive, custom-designed parts could be challenging38.\nFinally, consider the power electronics and control systems ‚Äì the turbine‚Äôs brain and nervous system28. These rely heavily on the global semiconductor and electronics supply chain, which, as we know, is heavily concentrated in Asia, including China28,41. Even if the final control units are assembled or integrated regionally, the essential chips and other electronic components often start their journey in East Asia. Rapidly shifting this underlying dependency is a complex, long-term proposition.\nSo, while the rare earth magnet situation represents an extreme case ‚Äì a near-monopoly on a critical material with few easy substitutes ‚Äì these other dependencies on gearboxes, bearings, and electronics add further layers of complexity and moderate risk to the wind turbine supply chain. They underscore that even beyond the REE bottleneck, building resilience requires looking deep into the sourcing of multiple specialized components. Still, the magnet issue remains in a class of its own due to that unique combination of extreme market concentration and the sheer difficulty of developing alternatives at scale."
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html#conclusion-recalibrating-the-green-dream",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html#conclusion-recalibrating-the-green-dream",
    "title": "How Rare-Earth Element Dependency Threatens the Global Wind Rollout",
    "section": "Conclusion: Recalibrating the Green Dream",
    "text": "Conclusion: Recalibrating the Green Dream\nSo, back to my ill-fated √òrsted investment. My initial thinking was simplistic: tariffs are noise, the green trend is real, buy the dip. What I missed was the plumbing behind the green trend, but how exposed that plumbing is to geopolitical friction. The ‚Äúnoise‚Äù of tariffs directly provoked retaliatory Chinese export controls on the very rare earths and magnets essential for wind turbines. The fragility isn‚Äôt theoretical; it‚Äôs being actively demonstrated.\nThe wind industry starkly illustrates this. The critical vulnerability lies in the permanent magnets, powered by REEs processed almost exclusively in China, particularly the heavy REEs now subject to export licensing. This isn‚Äôt just a supply chain concentration; it‚Äôs a geopolitical chokepoint being actively tightened. Diversifying away, as the US is attempting with DoD backing, is a monumental, multi-year task facing vast scale and technological hurdles, dwarfed by China‚Äôs current capacity.\nThe fact that these materials are dual-use ‚Äì critical for both green energy (wind turbines, EVs) and advanced defense systems ‚Äì raises the stakes immensely. The competition and potential restrictions impact national security and climate goals simultaneously.\nBuilding independent European or Western capacity is proving incredibly difficult, as the recent Northvolt bankruptcy highlights42. If major industrial projects struggle even for batteries, the challenge of creating an independent mine-to-magnet supply chain for REEs outside of China ‚Äì overcoming technical gaps and massive scale differences ‚Äì seems even more daunting. Failure risks ceding control over the pace and cost of the energy transition, leaving climate ambitions hostage to the supply of essential components from a single, dominant source that has shown willingness to restrict access.\nAchieving our climate goals requires more than targets and subsidies. It demands a hard-nosed focus on industrial strategy and supply chain security, recognizing that the materials underpinning the transition are now squarely in the geopolitical crosshairs. We need serious, sustained investment in diversifying REE processing and magnet production, fostering alternatives43, and acknowledging that building resilience against these active pressures will be slow, expensive, and absolutely critical. Ignoring the reality of where essential parts come from, and the geopolitical levers attached to them, is a gamble the green transition cannot afford."
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html#the-magnetic-heart-chinas-dominance",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html#the-magnetic-heart-chinas-dominance",
    "title": "How Rare-Earth Element Dependency Threatens the Global Wind Rollout",
    "section": "The Magnetic Heart & China‚Äôs Dominance",
    "text": "The Magnetic Heart & China‚Äôs Dominance\nSo, the big blades and towers look relatively straightforward from a geopolitical sourcing perspective. But the real action, and the real vulnerability, is inside the nacelle ‚Äì the house-sized box sitting atop the tower that contains the turbine‚Äôs power-generating guts17,18.\nA key component here, especially for many efficient modern designs like direct-drive turbines (which ditch the heavy gearbox), is the Permanent Magnet Synchronous Generator (PMSG)19. These generators rely on incredibly powerful permanent magnets, specifically Neodymium-Iron-Boron (NdFeB) magnets, to convert rotational energy into electricity20.\nThese aren‚Äôt your average fridge magnets. They require specific Rare Earth Elements (REEs) ‚Äì primarily Neodymium (Nd) and Praseodymium (Pr), often boosted with smaller amounts of Dysprosium (Dy) and Terbium (Tb) for high-temperature performance20.\nAnd here‚Äôs the crux: while REEs are mined in various places (including the US and Australia), the complex, dirty, and technologically challenging task of separating and processing these elements into usable oxides and metals is overwhelmingly dominated by one country: China. China controls over 85% of global REE processing capacity20. Crucially, this dominance is even starker for the heavy REEs like Dysprosium and Terbium ‚Äì essential for high-performance magnets ‚Äì where China accounted for effectively 100% of global processing until very recently.\nBut it doesn‚Äôt stop there. China also dominates the next crucial step ‚Äì manufacturing the finished high-performance NdFeB magnets themselves, accounting for around 90-92% of global production20. This dual chokehold isn‚Äôt just a passive risk; it‚Äôs now being actively leveraged.\nIn direct response to recent US tariff hikes under President Trump‚Äôs second term, China imposed new export restrictions in April 2025. These specifically target seven medium and heavy REEs (including the critical magnet inputs Dysprosium and Terbium) and the magnets themselves, requiring special export licenses. This isn‚Äôt a hypothetical threat; it‚Äôs the weaponization of a known dependency, directly impacting inputs vital not only for defense technologies but fundamentally for the magnets powering the green transition‚Äôs wind turbines and electric vehicles.\nIt‚Äôs crucial to understand, however, that while US actions triggered this response, the consequences ripple far beyond the United States. The export licensing requirement itself applies globally, affecting buyers in Europe, Japan, South Korea, and other nations dependent on these materials. Reports even indicate China is directly warning companies in allied nations like South Korea against supplying products containing these REEs to US defense firms. So, while framed within the US-China conflict, these controls create uncertainty and potential disruption for the entire global high-tech manufacturing base reliant on Chinese-processed rare earths, further complicating energy transition plans worldwide.\n\n\n\n\n\n\nflowchart TD\n\n    subgraph Stage1[\"Stage 1: REE Mining ü™®\"]\n        mining[\"Extracting Rare Earth Element ores from the ground&lt;br&gt;&lt;br&gt;Key Miners: üá®üá≥ üá∫üá∏ üá¶üá∫ üá≤üá≤&lt;br&gt;&lt;br&gt;China's Share: ~60-70%\"]\n        style mining fill:#e6f2ff,stroke:#0066cc,color:#003366\n    end\n    style Stage1 fill:#b3d9ff,stroke:#0066cc,color:#003366\n\n    subgraph Stage2[\"Stage 2: Complex Processing & Separation ‚öóÔ∏è\"]\n        refining[\"Chemically separating raw ores into usable oxides and metals&lt;br&gt;&lt;br&gt;&lt;b style='color:red'&gt;China's Global Share: &gt;85%&lt;/b&gt;&lt;br&gt;&lt;span style='font-size:12px'&gt;Rest of World: &lt;15%&lt;/span&gt;\"]\n        style refining fill:#ffe6e6,stroke:#cc0000,color:#660000\n    end\n    style Stage2 fill:#ffcccc,stroke:#cc0000,color:#660000\n\n    subgraph Stage3[\"Stage 3: NdFeB Magnet Manufacturing üß≤\"]\n        magnet_making[\"Producing high-performance Neodymium-Iron-Boron permanent magnets&lt;br&gt;&lt;br&gt;&lt;b style='color:red'&gt;China's Global Share: ~90-92%&lt;/b&gt;&lt;br&gt;&lt;span style='font-size:12px'&gt;Rest of World: &lt;10%&lt;/span&gt;\"]\n        style magnet_making fill:#ffebcc,stroke:#cc7000,color:#664400\n    end\n    style Stage3 fill:#ffd699,stroke:#cc7000,color:#664400\n\n    subgraph Stage4[\"Stage 4: Used in Wind Turbines üå¨Ô∏è\"]\n        turbine_use[\"Essential for Permanent Magnet Synchronous Generators (PMSGs) in modern turbines\"]\n        style turbine_use fill:#e6ffe6,stroke:#009900,color:#004d00\n    end\n    style Stage4 fill:#ccffcc,stroke:#009900,color:#004d00\n\n    Stage1 --&gt; Stage2\n    Stage2 --&gt; Stage3\n    Stage3 --&gt; Stage4\n\n\n\n\n\n\n\n\nWhy is this dependency so hard to break? Building new REE processing plants and magnet factories outside China requires enormous long-term investment, navigating environmental hurdles, and developing specialized technical expertise20. Efforts are underway, including reviving processing in the US and developing recycling technologies, but scaling these alternatives to meet the massive demand from the energy transition is a slow, expensive, and uncertain process20,21. Adding another layer of risk, China has already implemented export restrictions on REE extraction and separation technologies in December 2023, and signals suggest it could potentially restrict magnet exports too, giving it significant geopolitical leverage22,23.\nEfforts are underway ‚Äì the US Department of Defense, recognizing the strategic risk (evidenced by the massive amounts of REEs in military hardware), is funding projects like MP Materials and Lynas USA to build domestic processing and magnet capacity. However, the scale highlights the challenge: even planned US NdFeB magnet production by late 2025 (e.g., MP Materials aiming for 1,000 tons) is dwarfed by China‚Äôs estimated 300,000 tons produced in 2024. The US is years away from self-sufficiency, even for its defense needs, let alone the vast quantities required for the energy transition. International partners like Australia are also developing resources, but currently lack refining capacity independent of China.\nThis REE bottleneck, now actively being constricted through export controls triggered by trade disputes, is the single most critical and immediate geopolitical vulnerability in the wind turbine supply chain. While other components face challenges, none combine near-total reliance on a single nation actively using that reliance as leverage, with such immense difficulty in building alternatives at the necessary scale and speed.\nHere‚Äôs a quick breakdown of the dependency levels for key components:\n\n\n\n\n\n\n\n\n\n\nComponent\nKey Materials/Processes Involved\nChina‚Äôs Role & Dependency Level\nDifficulty Sourcing Alternatives at Scale\n\n\n\n\nBlades\nFiberglass, Carbon Fiber, Resins, Core (Balsa, PET/PVC Foam)24\nModerate: Major manufacturing hub (incl.¬†for Western OEM exports25); Major consumer of raw materials (e.g., balsa26).\nLow-Moderate: Balsa sourced from Ecuador27; PET foam alternative global26; Fibers/resins global24; Regional mfg capacity exists.\n\n\nTowers\nSteel (predominantly), Concrete28\nLow: Major steel producer & tower manufacturer for domestic market.29\nLow: Steel is global commodity; Tower mfg highly regionalized29; Capacity exists in key markets29.\n\n\nNacelle Assembly\nIntegration of internal components30\nModerate: Major global assembly hub, including for Western OEM exports.25\nLow (for assembly): Assembly capacity exists globally31. High dependency relates to specific internal components (see below).\n\n\nGearbox\nSteel alloys, Gears, Bearings32\nModerate-High: Major manufacturing location; Key domestic suppliers (e.g., NGC).33\nModerate: Concentrated market, but key suppliers also exist in Europe (ZF/Winergy, Moventas etc.).33\n\n\nGenerator (PMSG)\nCopper, Steel, REE Magnets (Nd, Pr, Dy, Tb)28\nVery High (for Magnets): Dominates REE processing (&gt;85%) and magnet manufacturing (~90-92%)34; Subject to active export controls imposed April 2025.35.\nVery High (for Magnets): Alternatives (non-China processing, recycling) limited/developing slowly.36\n\n\nGenerator (Other)\nCopper, Steel, Insulation28\nLow-Moderate: Major manufacturing location, but global suppliers and capacity exist.37\nLow: Global commodity materials; Established global suppliers (ABB, Siemens Energy etc.).37\n\n\nBearings\nSteel alloys\nModerate-High (Assumed): Major industrial component hub; Potential US bottleneck.38\nModerate: Specialized; Global mfrs exist but scaling alternatives for large turbines might be challenging.38\n\n\nPower Electronics\nSemiconductors, Copper, Aluminum, Plastics28\nHigh (for Components): Major hub for electronics manufacturing; Reliance on Asian semiconductor supply chain.\nModerate-High (for Components): Diversifying semiconductor/electronics supply chains is complex and long-term.37\n\n\n\n\nTable: China Dependency Analysis for Wind Turbine Components"
  },
  {
    "objectID": "posts/2025-04-23-windturbines/wind-turbines.html#post-script",
    "href": "posts/2025-04-23-windturbines/wind-turbines.html#post-script",
    "title": "How Rare-Earth Element Dependency Threatens the Global Wind Rollout",
    "section": "Post script",
    "text": "Post script\nThe information that I draw on in this article comes from Gemini 2.5‚Äôs deep reasearch function. I asked for information on concetrantion in the wind turbine industry, as well as a deep dive on manufacturing in the industry. The results were impressive, and I have included the two PDFs below.\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file.\n\nHere is the deep dive on manufacturing in the wind turbine industry.\n\n    It appears you don't have a PDF plugin for this browser.\n    No biggie... you can click here to\n    download the PDF file.\n\n\n\n\n\n\nquadrantChart\n    title Wind Turbine Component: China Dependency vs. Sourcing Difficulty\n    x-axis Low China Dependency --&gt; High China Dependency\n    y-axis Many Sourcing Alternatives --&gt; Few Sourcing Alternatives\n    quadrant-1 Strategic Vulnerability\n    quadrant-2 Specialized Tech\n    quadrant-3 Resilient\n    quadrant-4 Concentrated Mfg\n    Blades: [0.6, 0.4]\n    Towers: [0.2, 0.2]\n    Nacelle Assembly: [0.61, 0.2]\n    Gearbox: [0.7, 0.6]\n    PMSG: [0.95, 0.95]\n    Generator: [0.4, 0.2]\n    Bearings: [0.72, 0.65]\n    Power Electronics: [0.85, 0.75]"
  }
]