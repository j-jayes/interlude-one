[
  {
    "objectID": "posts/welcome/welcome.html",
    "href": "posts/welcome/welcome.html",
    "title": "Welcome to Interlude One",
    "section": "",
    "text": "This is a line from Interlude One off of Alt-J’s first album, An Awesome Wave. I like this lyric a great deal!\nI aspire to balance in my own life. This site is a space to share what I find interesting, and encourage me to keep learning and sharing. It includes exploratory data analyses, R tutorials, and answers to some data specific questions. I hope that you enjoy your look around."
  },
  {
    "objectID": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "href": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html",
    "title": "ChatGPT fills in for me as a blog post author",
    "section": "",
    "text": "ChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\nToday I asked ChatGPT to to the same.\nIt took 30 seconds.\nHere are the results!\n\n\n\n\n\nPenguins created with DALL·E with the prompt “A diagram of three penguins in the style of Paul Cézanne, hyper realistic, beautiful art”\n\n\n‘Prompt engineering’ describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\nHere, I asked ChatGPT to:\n\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\n\n\n\n\nI recieved text and code that ran almost perfectly:\nIt began\n\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we’ll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\nTo start, let’s load the necessary packages :\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\n\nNext, we’ll load the Palmer Penguins dataset and take a look at the variables we have to work with:\n\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we’ll focus on the relationship between bill_length_mm and body_mass_g.\nTo understand the coefficient plot, we’ll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We’ll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\n\n\n\n\n\nNow we’ll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\n\n\nreg_results &lt;- lm(body_mass_g ~ bill_length_mm, data = penguins)\nreg_tidy &lt;- broom::tidy(reg_results, conf.int = TRUE)\n\n\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\nTo add the error bars, we’ll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\n\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\nAnd that’s it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\n\n\n\n\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nWow! Amazing. I learned a bunch about ggplot and R from reading the code which accompanied David Robinson’s screencasts in 2019 and 2020. I am excited for the generation of data analysts beginning their journey now - being able to ask for tailored instructions for a task is a real boon!"
  },
  {
    "objectID": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html#purpose",
    "href": "posts/2022-12-20-gpt-chat-blog-post-author/gpt-chat-blog-post-author.html#purpose",
    "title": "ChatGPT fills in for me as a blog post author",
    "section": "",
    "text": "ChatGPT is all the rage at the moment. On Friday last week I spent an hour writing a short blog post to explain how to draw coefficient plots for a colleague.\nToday I asked ChatGPT to to the same.\nIt took 30 seconds.\nHere are the results!\n\n\n\n\n\nPenguins created with DALL·E with the prompt “A diagram of three penguins in the style of Paul Cézanne, hyper realistic, beautiful art”\n\n\n‘Prompt engineering’ describes the art of illiciting useful responses from Large Language Models (LLMs). Yet, the latest iteration of ChatGPT often gives me what I ask for on the first try.\nHere, I asked ChatGPT to:\n\nWrite a blog post for a website using R tidyverse syntax that shows how to draw a coefficient plot with error bars in ggplot. Use the Palmer Penguins dataset and show the relationship between bill_length_mm and body_mass_g. Use the lm() command to run a regression and the broom::tidy(conf.int = TRUE) command on the regression output to draw error bars with the geom_errorbarh() geometry.\n\n\n\n\nI recieved text and code that ran almost perfectly:\nIt began\n\nAre you trying to visualize the relationship between two variables in your dataset using ggplot and the tidyverse syntax in R? One way to do this is by creating a coefficient plot, which is a type of scatterplot that includes error bars for the data points. In this blog post, we’ll walk through the steps for creating a coefficient plot with error bars using the Palmer Penguins dataset, which includes measurements of bill length and body mass for several species of penguins.\nTo start, let’s load the necessary packages :\n\n\nlibrary(tidyverse)\nlibrary(broom)\n\n\nNext, we’ll load the Palmer Penguins dataset and take a look at the variables we have to work with:\n\n\nlibrary(palmerpenguins)\ndata(\"penguins\")\n\nhead(penguins)\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_l…¹ body_…² sex    year\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;       &lt;int&gt;   &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7         181    3750 male   2007\n2 Adelie  Torgersen           39.5          17.4         186    3800 fema…  2007\n3 Adelie  Torgersen           40.3          18           195    3250 fema…  2007\n4 Adelie  Torgersen           NA            NA            NA      NA &lt;NA&gt;   2007\n5 Adelie  Torgersen           36.7          19.3         193    3450 fema…  2007\n6 Adelie  Torgersen           39.3          20.6         190    3650 male   2007\n# … with abbreviated variable names ¹​flipper_length_mm, ²​body_mass_g\n\n\n\nWe can see that the dataset includes variables such as bill_length_mm, body_mass_g, and species. For our coefficient plot, we’ll focus on the relationship between bill_length_mm and body_mass_g.\nTo understand the coefficient plot, we’ll start by using the ggplot function to specify the data and the aesthetics for a scatter plot. We’ll set the x variable to be bill_length_mm and the y variable to be body_mass_g.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g))\n\n\n\n\n\nNow we’ll add the geom_point layer to plot the data points and the geom_smooth layer to add a smooth line of best fit.\n\n\nggplot(data = penguins, aes(x = bill_length_mm, y = body_mass_g)) +\n  geom_point() +\n  geom_smooth()\n\n\n\n\n\nThis gives us a scatter plot with a line of best fit. We can see that body mass increases as does bill length.\nNext, we can use the lm() command to run a regression analysis on the data, using bill length as the predictor variable and body mass as the outcome variable. We can then use the broom::tidy() function with the conf.int = TRUE argument to extract the regression results, including the confidence intervals for the coefficients:\n\n\nreg_results &lt;- lm(body_mass_g ~ bill_length_mm, data = penguins)\nreg_tidy &lt;- broom::tidy(reg_results, conf.int = TRUE)\n\n\nNow that we have the regression results, we can use ggplot to create our coefficient plot.\nTo add the error bars, we’ll use the geom_errorbarh() geometry and specify the xmin and xmax aesthetics as the lower and upper bounds of the confidence intervals, respectively:\n\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high))\n\n\n\n\n\nAnd that’s it! We now have a coefficient plot with error bars showing the relationship between bill length and body mass in the Palmer Penguins dataset. This plot can help us understand how the two variables are related, and how certain we can be about the strength of that relationship based on the size of the error bars.\nI hope this tutorial was helpful in showing you how to create a coefficient plot with error bars using the tidyverse syntax in R. If you have any questions or comments, please feel free to leave them below. Happy plotting!\n\n\n\n\nThis was a fantastic tutorial and written up in a way that is relatively easy to understand too. I enjoyed the use of succinct variable names and how to the point the write up was.\nThe only things I added in my own post on Friday were a dotted line at zero, a title, and some axis labels. This is shown below.\n\nggplot(reg_tidy, aes(x = estimate, y = term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nWow! Amazing. I learned a bunch about ggplot and R from reading the code which accompanied David Robinson’s screencasts in 2019 and 2020. I am excited for the generation of data analysts beginning their journey now - being able to ask for tailored instructions for a task is a real boon!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "",
    "text": "Thank you to Adam A. Kemberling for providing some useful feedback on how to explain the panelset syntax and suggesting the use of dplyr::walk() instead of a for loop. His suggestion is here.\nThis is a poor solution. In the case where there are only a small number of possible categories, it works. However, if there are many potential tab names and many are empty, having to click through and find content is not pleasing at all!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#purpose",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#purpose",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Purpose",
    "text": "Purpose\nI want to create a parametrized report in Rmarkdown. The report is populated after querying a database. In short, the Rmarkdown document must have a tab for each category of a categorical variable, and draw a ggplot in the tab. The number of categories depends on the result of the query.\nWe can simulate the problem by removing one of the islands from the palmerpenguins dataset:\nThere are three islands:\n\n\n\n\n\n\n\n\nIsland name\nNumber of penguins\n\n\n\n\nBiscoe\n168\n\n\nDream\n124\n\n\nTorgersen\n52\n\n\n\n\n\nWe will subset the data to choose only two islands:\n\ndf &lt;- palmerpenguins::penguins %&gt;% \n    filter(island != \"Biscoe\")\n\ndf %&gt;%\n  distinct(island) %&gt;%\n  knitr::kable(col.names = c(\"Remaining islands\"))\n\n\n\n\nRemaining islands\n\n\n\n\nTorgersen\n\n\nDream"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#problem",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#problem",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Problem",
    "text": "Problem\nIf I hardcode the number of tabs one of the plots is drawn but has no data!"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Biscoe",
    "text": "Biscoe\n\nplot_penguins(\"Biscoe\")"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe-1",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#biscoe-1",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Biscoe",
    "text": "Biscoe\n\nplot_penguins(\"Biscoe\")"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#better-solution",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#better-solution",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Better solution",
    "text": "Better solution\nA better solution is to generate the tabs programmatically, with code that writes itself depending on the contents of the returned dataset.\nI am creating the tabs with Garrick Aden-Buie’s xaringanExtra package. Have a look at the specifics in the presentation below.\n\nknitr::include_url(\"https://pkg.garrickadenbuie.com/xaringanExtra/panelset/?panelset=r-code#2\")\n\n\n\nOther useful resources incude this Science Loft post and this Stack Overflow question.\n\nTools\nIn vanilla Rmarkdown you can create the tabs with a simple heading structure.\n\n## Here are the penguins {.panelset} \n\n### Biscoe\nplot_penguins(\"Biscoe\")\n### Dream\nplot_penguins(\"Dream\")\n\nI opt to use Pandoc’s fenced div structure instead, to avoid inadvertently creating additional tabs with a third level heading. It looks like this:\n\n::::: {.panelset}\n\n::: {.panel}\n\n[Biscoe]{.panel-name}\n\nplot_penguins(\"Biscoe\")\n:::\n    \n::: {.panel}\n\n[Dream]{.panel-name}\n\nplot_penguins(\"Dream\")\n\n:::\n\n:::::\n\nIt is worth highlighting that the tabset\n\nbegins with ::::: {.panelset} and,\nends with five colons :::::\n\nEach individual tab needs to\n\nopen with ::: {.panel},\ninclude a [Panel name specified here]{.panel-name}, and\nclose with three colons :::"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#how-does-it-work",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#how-does-it-work",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "How does it work?",
    "text": "How does it work?\nNow I can programmatically create my tabs and plots within the fences. I start with a function that makes the tabs. Then I open the tabset, get a list of the islands present, and walk across each one applying my make_tab function before closing the tabset.\n\nmake_tab &lt;- function(isl) {         # function to make the tabs\n  cat(\"::: {.panel}\\n\")             # Open tab\n  cat(\"##\", isl, \"{.panel-name}\\n\") # Label tab\n  p &lt;- plot_penguins(isl)           # Create plot\n  print(p)                          # Display plot\n  cat(\"\\n\")                         # Space\n  cat(\":::\\n\")                      # Close tab\n}\n\n::::: {.panelset}\n\nisl_list &lt;- unique(df$island) %&gt;%   # Get a list of islands present in the dataset\n    as.character()\n\ndplyr::walk(isl_list, make_tab)     # make the tabs for each island that is there\n\n:::::\n\n\nGood to remember\n\nWrap code you want executed in cat(), and print() your ggplot objects.\nSet code options to results = 'asis' for the code chunk that makes your tabs. According to the Rmarkdown Cookbook:\n\n\nThis option tells knitr not to wrap your text output in verbatim code blocks, but treat it “as is.” This can be particularly useful when you want to generate content dynamically from R code.\n\n\nPut in lots of line breaks to separate Rmarkdown content, e.g. frequent uses of cat(\"\\n\") between elements."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#does-it-work",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#does-it-work",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Does it work?",
    "text": "Does it work?\nYes! Look at that, just two tabs and no empty plots!\n\nisl_list &lt;- unique(df$island) %&gt;% as.character()\n\nwalk(isl_list, make_tab)\n\nTorgersen\n\n\n\n\n\nDream\n\n\n\n\n\nI hope that this was useful. Please create an issue on Github if you have any suggestions."
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#torgersen-2",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#torgersen-2",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Torgersen",
    "text": "Torgersen"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#dream-2",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#dream-2",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Dream",
    "text": "Dream"
  },
  {
    "objectID": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#postscript",
    "href": "posts/2022-03-04-programmatically-generate-tabs-in-rmarkdown/programmatically-generate-tabs-in-rmarkdown.html#postscript",
    "title": "Programmatically generate tabs in Rmarkdown",
    "section": "Postscript",
    "text": "Postscript\n\nThis is the function that plots the figures:\n\nplot_penguins &lt;- function(isl) {\n  df %&gt;%\n    filter(island == isl) %&gt;%\n    ggplot(aes(bill_length_mm, \n               bill_depth_mm, \n               colour = body_mass_g, \n               shape = species)) +\n    geom_point(cex = 2) +\n    geom_smooth(method = \"lm\", se = F) +\n    scale_color_viridis_c(labels = scales::number_format()) +\n    labs(\n      x = \"Bill length (mm)\",\n      y = \"Bill depth (mm)\",\n      colour = \"Body mass (g)\",\n      shape = \"Penguin species\"\n    )\n}"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "Welcome to the first in a series of data screencasts where I attempt to show you how great the R language is.\nThis post follows along with the data screencast and includes the code methodically.\nIt begins by reading in the data, then tidying it up, analysing it, making some visualizations and then performing some predictive modelling.\n\n\nI’m signed up to a fantastic newsletter called “Data is Plural” curated by journalist Jeremy Singer-Vine. Truly, it is a treasure trove of interesting public datasets from all over the internet. You can sign up to the newsletter here if this sounds up your alley.\nThis week it included a link to a fun selection of chocolate bar reviews, which Jeremy described as:\n\nChocolate bar reviews. The Manhattan Chocolate Society’s Brady Brelinski has reviewed 2,500+ bars of craft chocolate since 2006, and compiles his findings into a copy-paste-able table that lists each bar’s manufacturer, bean origin, percent cocoa, ingredients, review notes, and numerical rating.\n\nA live link to the database is shown below.\n\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")\n\n\n\n\n\n\nI’ve copied and pasted the data into an Excel spreadsheet, accessible on my Github if you want to download it an analyse it yourself.\nThe underlying data is served in a Javascript container on the website rather than vanilla HTML. This makes it a little bit more difficult to scrape with a package like rvest, for example. Hence the Excel spreadsheet.\nTo begin the analysis, we’ll read in the Excel file using the readxl package, and the here package that helps us with file paths.1\nWe don’t need to load the packages via the library(readxl) command because we’re only going to use them once or twice. Instead we can call the name of the package followed by two colons and the command, as shown below.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# read in the data\ndf &lt;- readxl::read_excel(here::here(\"posts\", \n                                    \"2022-01-17-flavours-of-cocoa\", \n                                    \"data\", \n                                    \"chocolate-bar-ratings.xlsx\"))\n\n# display the first six rows of tibble\nhead(df)\n\n# A tibble: 6 × 10\n    REF Company…¹ Compa…² Revie…³ Count…⁴ Speci…⁵ Cocoa…⁶ Ingre…⁷ Most …⁸ Rating\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1  2454 5150      U.S.A.     2019 Tanzan… Kokoa …    0.76 3- B,S… rich c…   3.25\n2  2458 5150      U.S.A.     2019 Domini… Zorzal…    0.76 3- B,S… cocoa,…   3.5 \n3  2454 5150      U.S.A.     2019 Madaga… Bejofo…    0.76 3- B,S… cocoa,…   3.75\n4  2542 5150      U.S.A.     2021 Fiji    Matasa…    0.68 3- B,S… chewy,…   3   \n5  2546 5150      U.S.A.     2021 Venezu… Sur de…    0.72 3- B,S… fatty,…   3   \n6  2546 5150      U.S.A.     2021 Uganda  Semuli…    0.8  3- B,S… mildly…   3.25\n# … with abbreviated variable names ¹​`Company (Manufacturer)`,\n#   ²​`Company Location`, ³​`Review Date`, ⁴​`Country of Bean Origin`,\n#   ⁵​`Specific Bean Origin or Bar Name`, ⁶​`Cocoa Percent`, ⁷​Ingredients,\n#   ⁸​`Most Memorable Characteristics`\n\n\nThis gives us a tibble (similar to a dataframe) with 10 columns (4 numeric and 6 character) and 2,530 individual reviews.\nThe column names are a big ugly though:\n\n# show column names\ndf %&gt;% colnames()\n\n [1] \"REF\"                              \"Company (Manufacturer)\"          \n [3] \"Company Location\"                 \"Review Date\"                     \n [5] \"Country of Bean Origin\"           \"Specific Bean Origin or Bar Name\"\n [7] \"Cocoa Percent\"                    \"Ingredients\"                     \n [9] \"Most Memorable Characteristics\"   \"Rating\"                          \n\n\nWe can use the janitor package to make the column names snake case (lower case with words separated by an underscore).\n\n# clean names\ndf &lt;- df %&gt;% \n    janitor::clean_names()\n\n# show names again\ndf %&gt;% colnames()\n\n [1] \"ref\"                              \"company_manufacturer\"            \n [3] \"company_location\"                 \"review_date\"                     \n [5] \"country_of_bean_origin\"           \"specific_bean_origin_or_bar_name\"\n [7] \"cocoa_percent\"                    \"ingredients\"                     \n [9] \"most_memorable_characteristics\"   \"rating\"                          \n\n\nNow that we have a nice tibble with clean names, we can ask what the data itself looks like. There are many ways to get summary statistics of a dataset. I love the skim function from the skimr package.\n\n# skim the dataset\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nref\n0\n1\n1429.80\n757.65\n5.00\n802.0\n1454.00\n2079.00\n2712\n▆▇▇▇▇\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006.00\n2012.0\n2015.00\n2018.00\n2021\n▃▅▇▆▅\n\n\ncocoa_percent\n0\n1\n0.72\n0.06\n0.42\n0.7\n0.70\n0.74\n1\n▁▁▇▁▁\n\n\nrating\n0\n1\n3.20\n0.45\n1.00\n3.0\n3.25\n3.50\n4\n▁▁▅▇▇\n\n\n\n\n\nGreat! Our reviews are almost all complete.\n\nOnly 3 percent are missing information on the ingredients.\nThe reviews begin in 2006, the mean review is from 2014, and the latest is from 2021.\nThe percent of the bar comprising of cocoa ranges from 42 to 100, with a mean of 72.\nWe have 62 unique countries of origin for the beans, and 67 countries of manufacture.\nThere are 21 unique combinations of ingredients, comprising of seven elements in total.\n\n\n\n\nLet’s have a look at that ingredients column.\n\n# count elements of ingredients column\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 × 2\n   ingredients      n\n   &lt;chr&gt;        &lt;int&gt;\n 1 3- B,S,C       999\n 2 2- B,S         718\n 3 4- B,S,C,L     286\n 4 5- B,S,C,V,L   184\n 5 4- B,S,C,V     141\n 6 &lt;NA&gt;            87\n 7 2- B,S*         31\n 8 4- B,S*,C,Sa    20\n 9 3- B,S*,C       12\n10 3- B,S,L         8\n# … with 12 more rows\n\n\nSo we have a number of ingredients, a dash, and then a key for what the ingredients are. Consulting the website reveals that there are seven possible ingredients:\n\n\n\n\n\nkey\nvalue\n\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\nThese key and value combinations are very sensible - if we have a lot of data we can save space by using the keys instead of the whole string. However, I would prefer to have them written out, because we’re going to split them into their own columns a little bit later.\nWe can use the str_replace_all function from the stringr package to replace items in the list of ingredients with names.\n\ndf &lt;- df %&gt;% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nLet’s look again at our ingredients column:\n\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 × 2\n   ingredients                                        n\n   &lt;chr&gt;                                          &lt;int&gt;\n 1 3- beans,sugar,cocoa_butter                      999\n 2 2- beans,sugar                                   718\n 3 4- beans,sugar,cocoa_butter,lecithin             286\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\n 5 4- beans,sugar,cocoa_butter,vanilla              141\n 6 &lt;NA&gt;                                              87\n 7 2- beans,non_sugar_sweetener                      31\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\n10 3- beans,sugar,lecithin                            8\n# … with 12 more rows\n\n\nFantastic! Now we have the number of ingredients, a dash, and then each ingredient by name in one column. Let’s separate this information into two columns so that we can use the number of ingredients as a feature.\nThe separate function from the tidyr package is made just for this purpose. It takes three arguments:\n\nthe name of the column to separate.\nnew column names corresponding to the number of elements.\nthe separator between elements.\n\n\ndf &lt;- df %&gt;% \n  separate(col = ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"),\n           sep = \"-\") %&gt;% \n    # parse_number looks for a number inside a character column and discards the rest\n  mutate(n_ingredients = parse_number(n_ingredients),\n         # str_squish removes whitespace around the elements in the ingredients column\n         ingredients = str_squish(ingredients))\n\ndf %&gt;% \n  select(n_ingredients, ingredients)\n\n# A tibble: 2,530 × 2\n   n_ingredients ingredients                      \n           &lt;dbl&gt; &lt;chr&gt;                            \n 1             3 beans,sugar,cocoa_butter         \n 2             3 beans,sugar,cocoa_butter         \n 3             3 beans,sugar,cocoa_butter         \n 4             3 beans,sugar,cocoa_butter         \n 5             3 beans,sugar,cocoa_butter         \n 6             3 beans,sugar,cocoa_butter         \n 7             3 beans,sugar,cocoa_butter         \n 8             4 beans,sugar,cocoa_butter,lecithin\n 9             4 beans,sugar,cocoa_butter,lecithin\n10             4 beans,sugar,cocoa_butter,lecithin\n# … with 2,520 more rows\n\n\nNow we have a numeric column with the number of ingredints and a column called ingredients with each element separated by a comma.\nFinally, let’s break the ingredients from a comma separated list into a binary variable for each ingredient. We can use the recipes package that is part of the tidymodels metapackage - a framework for doing statistical modelling in a tidy manner.\nFirst we break our ingredients into 6 columns. The problem we run into is that for bars that contain different ingredients, the order of the ingredients split into the columns is not constant.\n\n# df &lt;- df %&gt;% \n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\n#            sep = \",\") \n\n# df %&gt;% \n#     select(company_manufacturer , starts_with(\"ingredient_\")) %&gt;%\n#     slice(c(1L, 51L, 54L))\n\nPerhaps there is a better way to do this? Separate rows and pivot wider?\n\ndf %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    count(ingredients)\n\n# A tibble: 8 × 2\n  ingredients             n\n  &lt;chr&gt;               &lt;int&gt;\n1 beans                2443\n2 cocoa_butter         1668\n3 lecithin              493\n4 non_sugar_sweetener    76\n5 salt                   37\n6 sugar                2360\n7 vanilla               353\n8 &lt;NA&gt;                   87\n\ndf &lt;- df %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    filter(!is.na(ingredients)) %&gt;% \n    pivot_wider(names_from = ingredients, values_from = ingredients) %&gt;% \n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nTidymodels\n\n# library(tidymodels)\n# dummy_multi_choice_rec &lt;- recipe(~ ., data = df) %&gt;%\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %&gt;%\n#   prep()\n# \n# df &lt;- bake(dummy_multi_choice_rec, new_data = NULL)\n\n\n\n\n\n\n\ndf %&gt;% \n  count(rating, sort = T)\n\n# A tibble: 12 × 2\n   rating     n\n    &lt;dbl&gt; &lt;int&gt;\n 1   3.5    552\n 2   3      506\n 3   3.25   453\n 4   2.75   320\n 5   3.75   295\n 6   2.5    156\n 7   4      111\n 8   2       29\n 9   2.25    14\n10   1.5      5\n11   1        1\n12   1.75     1\n\n\nScore range between 1 and 4, and the modal value is 3.5.\n\n# histogram\ndf %&gt;% \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\n    labs(x = \"Chocolate bar rating\",\n         y = \"Number of bars\")\n\n\n\n\n\n\n\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    knitr::kable(digits = 2)\n\n\n\n\nreview_date\nmean_rating\n\n\n\n\n2006\n3.06\n\n\n2007\n3.17\n\n\n2008\n3.04\n\n\n2009\n3.09\n\n\n2010\n3.19\n\n\n2011\n3.27\n\n\n2012\n3.21\n\n\n2013\n3.21\n\n\n2014\n3.20\n\n\n2015\n3.24\n\n\n2016\n3.24\n\n\n2017\n3.36\n\n\n2018\n3.19\n\n\n2019\n3.13\n\n\n2020\n3.26\n\n\n2021\n3.32\n\n\n\n\n\nIt certainly seems like the mean rating is increasing over time. What is driving this?\nWe can make a plot of the figures above to see the increasing trend.\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    ggplot(aes(review_date, mean_rating)) +\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\n    labs(x = \"Date of review\",\n         y = \"Mean rating\")\n\n\n\n\nLet’s make a boxplot to see how the spread of scores has changed over time.\n\ndf %&gt;%\n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_jitter(alpha = .2) +\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\n\n\n\n\nIt seems as if the share of bars with very low scores has decreased over time, while the median value has remained relatively stable over time, shown by the bar in the centre of the boxplots.\nWhat about making a joy plot or ridgeline plot with the ggridges package? This allows us to see how the spread of values has changed over time.\n\nlibrary(ggridges)\n\ndf %&gt;%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\nThis confirms what we saw in the boxplots above: fewer low scores in more recent years mean that the mean has increased, while the top of the distributions remain largely the same.\n\n\n\n\ndf %&gt;% \n  mutate(cocoa_percent = round(cocoa_percent, 1)) %&gt;% \n  count(cocoa_percent, n_ingredients) %&gt;% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\n\n\n\n\ndf %&gt;% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 × 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                246\n 2 Peru                     231\n 3 Dominican Republic       220\n 4 Ecuador                  201\n 5 Madagascar               171\n 6 Blend                    144\n 7 Nicaragua                100\n 8 Bolivia                   79\n 9 Colombia                  78\n10 Tanzania                  78\n# … with 52 more rows\n\ndf %&gt;% \n  add_count(country_of_bean_origin) %&gt;%\n  # only include countries with more than 60 bars\n  filter(n &gt; 60) %&gt;% \n  group_by(country_of_bean_origin) %&gt;% \n  summarise(mean_rating = mean(rating)) %&gt;% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %&gt;% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\n\n\n\n\nlibrary(tmap)\ndata(\"World\")\n\nworld &lt;- World %&gt;% as_tibble()\n\nTo join our data on chocolate to this map, we need to get coutnry codes, using the countrycode package.\n\nlibrary(countrycode)\n\ndf &lt;- df %&gt;% \n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\n\n\nlibrary(sf)\n\ndf_map &lt;- df %&gt;% \n    group_by(iso_a3) %&gt;%\n    add_count() %&gt;% \n    summarise(mean_rating = mean(rating),\n              n = n) %&gt;% \n    ungroup() %&gt;% \n    distinct() %&gt;% \n    left_join(world, by = \"iso_a3\")\n\n\ndf_map %&gt;% \n    filter(n &gt; 3) %&gt;% \n    st_as_sf() %&gt;% ggplot() +\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n    geom_sf(aes(fill = mean_rating)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    labs(fill = \"Mean country rating\")\n\n\n\n\n\n\n\n\n\ndf_characteristics &lt;- df %&gt;% \n  select(c(most_memorable_characteristics, rating)) %&gt;% \n  separate_rows(most_memorable_characteristics, sep = \",\") %&gt;% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\n\ndf_characteristics %&gt;% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 × 2\n   most_memorable_characteristics     n\n   &lt;chr&gt;                          &lt;int&gt;\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# … with 938 more rows\n\n\nWe can start with a naive analysis that looks only at average score per word. These are the highest scoring words.\n\n# df_characteristics %&gt;% \n#   group_by(most_memorable_characteristics) %&gt;% \n#   add_count() %&gt;% \n#   mutate(avg_rating = mean(rating)) %&gt;% \n#   ungroup() %&gt;% \n#   slice_max(avg_rating, n = 12, with_ties = F)\n\ndf_characteristics %&gt;% \n  group_by(most_memorable_characteristics) %&gt;% \n  add_count() %&gt;% \n  filter(n &gt; 3) %&gt;% \n  mutate(avg_rating = mean(rating)) %&gt;% \n  ungroup() %&gt;% \n  distinct(most_memorable_characteristics, avg_rating) %&gt;% \n  slice_max(avg_rating, n = 12, with_ties = F) %&gt;% \n    mutate(avg_rating = round(avg_rating, 2)) %&gt;% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds &lt;- vfold_cv(df_characteristics)\n\nglmnet_recipe &lt;- \n  recipe(formula = rating ~ ., data = df_characteristics) %&gt;% \n  step_tokenize(most_memorable_characteristics) %&gt;% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %&gt;% \n  step_tf(most_memorable_characteristics) %&gt;% \n  step_normalize(all_predictors(), -all_nominal())\n\n\nglmnet_recipe %&gt;% prep() %&gt;% juice()\n\n# A tibble: 6,839 × 101\n   rating tf_most_memo…¹ tf_mo…² tf_mo…³ tf_mo…⁴ tf_mo…⁵ tf_mo…⁶ tf_mo…⁷ tf_mo…⁸\n    &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 2   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 3   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 4   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 5   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 6   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 7   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 8   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 9   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n10   3           -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n# … with 6,829 more rows, 92 more variables:\n#   tf_most_memorable_characteristics_bitter &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_black &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bland &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bold &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bright &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_brownie &lt;dbl&gt;, …\n\n\n\nglmnet_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") \n\nglmnet_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(glmnet_recipe) %&gt;% \n  add_model(glmnet_spec) \n\nglmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune &lt;- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\n\nglmnet_tune %&gt;% \n  autoplot()\n\n\n\n\n\nglmnet_model_final &lt;- finalize_workflow(glmnet_workflow, glmnet_tune %&gt;% \n  select_best())\n\nfinal_fit &lt;- glmnet_model_final %&gt;% \n  fit(df_characteristics)\n\nWhat does the fit look like? These are the terms that have the greatest effect on bar rating.\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %&gt;%\n  mutate(sign = estimate &gt; 0) %&gt;%\n  group_by(sign) %&gt;%\n  mutate(estimate = abs(estimate)) %&gt;% \n  slice_max(estimate, n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\nWow! have a look at the terms up top - “creamy”, “complex” and “rich” are good chocolate words. On the other side, “bitter”, “off” and “chemical” are terms that lower the score of the bar."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#beans",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#beans",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "I’m signed up to a fantastic newsletter called “Data is Plural” curated by journalist Jeremy Singer-Vine. Truly, it is a treasure trove of interesting public datasets from all over the internet. You can sign up to the newsletter here if this sounds up your alley.\nThis week it included a link to a fun selection of chocolate bar reviews, which Jeremy described as:\n\nChocolate bar reviews. The Manhattan Chocolate Society’s Brady Brelinski has reviewed 2,500+ bars of craft chocolate since 2006, and compiles his findings into a copy-paste-able table that lists each bar’s manufacturer, bean origin, percent cocoa, ingredients, review notes, and numerical rating.\n\nA live link to the database is shown below.\n\nknitr::include_url(\"http://flavorsofcacao.com/chocolate_database.html\")"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#ingest-the-data",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#ingest-the-data",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "I’ve copied and pasted the data into an Excel spreadsheet, accessible on my Github if you want to download it an analyse it yourself.\nThe underlying data is served in a Javascript container on the website rather than vanilla HTML. This makes it a little bit more difficult to scrape with a package like rvest, for example. Hence the Excel spreadsheet.\nTo begin the analysis, we’ll read in the Excel file using the readxl package, and the here package that helps us with file paths.1\nWe don’t need to load the packages via the library(readxl) command because we’re only going to use them once or twice. Instead we can call the name of the package followed by two colons and the command, as shown below.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n# read in the data\ndf &lt;- readxl::read_excel(here::here(\"posts\", \n                                    \"2022-01-17-flavours-of-cocoa\", \n                                    \"data\", \n                                    \"chocolate-bar-ratings.xlsx\"))\n\n# display the first six rows of tibble\nhead(df)\n\n# A tibble: 6 × 10\n    REF Company…¹ Compa…² Revie…³ Count…⁴ Speci…⁵ Cocoa…⁶ Ingre…⁷ Most …⁸ Rating\n  &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;     &lt;dbl&gt; &lt;chr&gt;   &lt;chr&gt;    &lt;dbl&gt;\n1  2454 5150      U.S.A.     2019 Tanzan… Kokoa …    0.76 3- B,S… rich c…   3.25\n2  2458 5150      U.S.A.     2019 Domini… Zorzal…    0.76 3- B,S… cocoa,…   3.5 \n3  2454 5150      U.S.A.     2019 Madaga… Bejofo…    0.76 3- B,S… cocoa,…   3.75\n4  2542 5150      U.S.A.     2021 Fiji    Matasa…    0.68 3- B,S… chewy,…   3   \n5  2546 5150      U.S.A.     2021 Venezu… Sur de…    0.72 3- B,S… fatty,…   3   \n6  2546 5150      U.S.A.     2021 Uganda  Semuli…    0.8  3- B,S… mildly…   3.25\n# … with abbreviated variable names ¹​`Company (Manufacturer)`,\n#   ²​`Company Location`, ³​`Review Date`, ⁴​`Country of Bean Origin`,\n#   ⁵​`Specific Bean Origin or Bar Name`, ⁶​`Cocoa Percent`, ⁷​Ingredients,\n#   ⁸​`Most Memorable Characteristics`\n\n\nThis gives us a tibble (similar to a dataframe) with 10 columns (4 numeric and 6 character) and 2,530 individual reviews.\nThe column names are a big ugly though:\n\n# show column names\ndf %&gt;% colnames()\n\n [1] \"REF\"                              \"Company (Manufacturer)\"          \n [3] \"Company Location\"                 \"Review Date\"                     \n [5] \"Country of Bean Origin\"           \"Specific Bean Origin or Bar Name\"\n [7] \"Cocoa Percent\"                    \"Ingredients\"                     \n [9] \"Most Memorable Characteristics\"   \"Rating\"                          \n\n\nWe can use the janitor package to make the column names snake case (lower case with words separated by an underscore).\n\n# clean names\ndf &lt;- df %&gt;% \n    janitor::clean_names()\n\n# show names again\ndf %&gt;% colnames()\n\n [1] \"ref\"                              \"company_manufacturer\"            \n [3] \"company_location\"                 \"review_date\"                     \n [5] \"country_of_bean_origin\"           \"specific_bean_origin_or_bar_name\"\n [7] \"cocoa_percent\"                    \"ingredients\"                     \n [9] \"most_memorable_characteristics\"   \"rating\"                          \n\n\nNow that we have a nice tibble with clean names, we can ask what the data itself looks like. There are many ways to get summary statistics of a dataset. I love the skim function from the skimr package.\n\n# skim the dataset\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n2530\n\n\nNumber of columns\n10\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n6\n\n\nnumeric\n4\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\ncompany_manufacturer\n0\n1.00\n2\n39\n0\n580\n0\n\n\ncompany_location\n0\n1.00\n4\n21\n0\n67\n0\n\n\ncountry_of_bean_origin\n0\n1.00\n4\n21\n0\n62\n0\n\n\nspecific_bean_origin_or_bar_name\n0\n1.00\n3\n51\n0\n1605\n0\n\n\ningredients\n87\n0.97\n4\n14\n0\n21\n0\n\n\nmost_memorable_characteristics\n0\n1.00\n3\n37\n0\n2487\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\nref\n0\n1\n1429.80\n757.65\n5.00\n802.0\n1454.00\n2079.00\n2712\n▆▇▇▇▇\n\n\nreview_date\n0\n1\n2014.37\n3.97\n2006.00\n2012.0\n2015.00\n2018.00\n2021\n▃▅▇▆▅\n\n\ncocoa_percent\n0\n1\n0.72\n0.06\n0.42\n0.7\n0.70\n0.74\n1\n▁▁▇▁▁\n\n\nrating\n0\n1\n3.20\n0.45\n1.00\n3.0\n3.25\n3.50\n4\n▁▁▅▇▇\n\n\n\n\n\nGreat! Our reviews are almost all complete.\n\nOnly 3 percent are missing information on the ingredients.\nThe reviews begin in 2006, the mean review is from 2014, and the latest is from 2021.\nThe percent of the bar comprising of cocoa ranges from 42 to 100, with a mean of 72.\nWe have 62 unique countries of origin for the beans, and 67 countries of manufacture.\nThere are 21 unique combinations of ingredients, comprising of seven elements in total."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#data-cleaning-and-feature-engineering",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#data-cleaning-and-feature-engineering",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "Let’s have a look at that ingredients column.\n\n# count elements of ingredients column\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 × 2\n   ingredients      n\n   &lt;chr&gt;        &lt;int&gt;\n 1 3- B,S,C       999\n 2 2- B,S         718\n 3 4- B,S,C,L     286\n 4 5- B,S,C,V,L   184\n 5 4- B,S,C,V     141\n 6 &lt;NA&gt;            87\n 7 2- B,S*         31\n 8 4- B,S*,C,Sa    20\n 9 3- B,S*,C       12\n10 3- B,S,L         8\n# … with 12 more rows\n\n\nSo we have a number of ingredients, a dash, and then a key for what the ingredients are. Consulting the website reveals that there are seven possible ingredients:\n\n\n\n\n\nkey\nvalue\n\n\n\n\nB\nBeans\n\n\nS\nSugar\n\n\nS*\nSweetener other than white cane or beet sugar\n\n\nC\nCocoa Butter\n\n\nV\nVanilla\n\n\nL\nLecithin\n\n\nSa\nSalt\n\n\n\n\n\nThese key and value combinations are very sensible - if we have a lot of data we can save space by using the keys instead of the whole string. However, I would prefer to have them written out, because we’re going to split them into their own columns a little bit later.\nWe can use the str_replace_all function from the stringr package to replace items in the list of ingredients with names.\n\ndf &lt;- df %&gt;% \n  mutate(ingredients = str_replace_all(ingredients, c(\"Sa\" = \"salt\",\n                                                      # the * is a special character \n                                                      # when writing Regex and so \n                                                      # we use the two backslashes to \n                                                      # \"escape\" the meaning\n                                                      \"S\\\\*\" = \"non_sugar_sweetener\",\n                                                      \"B\" = \"beans\",\n                                                      \"S\" =  \"sugar\",\n                                                      \"V\" = \"vanilla\",\n                                                      \"L\" = \"lecithin\",\n                                                      \"C\" = \"cocoa_butter\"\n                                                      )))\n\nLet’s look again at our ingredients column:\n\ndf %&gt;% \n    count(ingredients, sort = T)\n\n# A tibble: 22 × 2\n   ingredients                                        n\n   &lt;chr&gt;                                          &lt;int&gt;\n 1 3- beans,sugar,cocoa_butter                      999\n 2 2- beans,sugar                                   718\n 3 4- beans,sugar,cocoa_butter,lecithin             286\n 4 5- beans,sugar,cocoa_butter,vanilla,lecithin     184\n 5 4- beans,sugar,cocoa_butter,vanilla              141\n 6 &lt;NA&gt;                                              87\n 7 2- beans,non_sugar_sweetener                      31\n 8 4- beans,non_sugar_sweetener,cocoa_butter,salt    20\n 9 3- beans,non_sugar_sweetener,cocoa_butter         12\n10 3- beans,sugar,lecithin                            8\n# … with 12 more rows\n\n\nFantastic! Now we have the number of ingredients, a dash, and then each ingredient by name in one column. Let’s separate this information into two columns so that we can use the number of ingredients as a feature.\nThe separate function from the tidyr package is made just for this purpose. It takes three arguments:\n\nthe name of the column to separate.\nnew column names corresponding to the number of elements.\nthe separator between elements.\n\n\ndf &lt;- df %&gt;% \n  separate(col = ingredients, \n           into = c(\"n_ingredients\", \"ingredients\"),\n           sep = \"-\") %&gt;% \n    # parse_number looks for a number inside a character column and discards the rest\n  mutate(n_ingredients = parse_number(n_ingredients),\n         # str_squish removes whitespace around the elements in the ingredients column\n         ingredients = str_squish(ingredients))\n\ndf %&gt;% \n  select(n_ingredients, ingredients)\n\n# A tibble: 2,530 × 2\n   n_ingredients ingredients                      \n           &lt;dbl&gt; &lt;chr&gt;                            \n 1             3 beans,sugar,cocoa_butter         \n 2             3 beans,sugar,cocoa_butter         \n 3             3 beans,sugar,cocoa_butter         \n 4             3 beans,sugar,cocoa_butter         \n 5             3 beans,sugar,cocoa_butter         \n 6             3 beans,sugar,cocoa_butter         \n 7             3 beans,sugar,cocoa_butter         \n 8             4 beans,sugar,cocoa_butter,lecithin\n 9             4 beans,sugar,cocoa_butter,lecithin\n10             4 beans,sugar,cocoa_butter,lecithin\n# … with 2,520 more rows\n\n\nNow we have a numeric column with the number of ingredints and a column called ingredients with each element separated by a comma.\nFinally, let’s break the ingredients from a comma separated list into a binary variable for each ingredient. We can use the recipes package that is part of the tidymodels metapackage - a framework for doing statistical modelling in a tidy manner.\nFirst we break our ingredients into 6 columns. The problem we run into is that for bars that contain different ingredients, the order of the ingredients split into the columns is not constant.\n\n# df &lt;- df %&gt;% \n#   separate(ingredients, into = c(paste0(\"ingredient_\", rep(1:6))),\n#            sep = \",\") \n\n# df %&gt;% \n#     select(company_manufacturer , starts_with(\"ingredient_\")) %&gt;%\n#     slice(c(1L, 51L, 54L))\n\nPerhaps there is a better way to do this? Separate rows and pivot wider?\n\ndf %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    count(ingredients)\n\n# A tibble: 8 × 2\n  ingredients             n\n  &lt;chr&gt;               &lt;int&gt;\n1 beans                2443\n2 cocoa_butter         1668\n3 lecithin              493\n4 non_sugar_sweetener    76\n5 salt                   37\n6 sugar                2360\n7 vanilla               353\n8 &lt;NA&gt;                   87\n\ndf &lt;- df %&gt;% \n    separate_rows(ingredients, sep = \",\") %&gt;%\n    filter(!is.na(ingredients)) %&gt;% \n    pivot_wider(names_from = ingredients, values_from = ingredients) %&gt;% \n    mutate(across(beans:non_sugar_sweetener, ~ ifelse(is.na(.), 0, 1)))\n\nTidymodels\n\n# library(tidymodels)\n# dummy_multi_choice_rec &lt;- recipe(~ ., data = df) %&gt;%\n#   step_dummy_multi_choice(starts_with(\"ingredient_\")) %&gt;%\n#   prep()\n# \n# df &lt;- bake(dummy_multi_choice_rec, new_data = NULL)"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#analysing-the-data",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#analysing-the-data",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "df %&gt;% \n  count(rating, sort = T)\n\n# A tibble: 12 × 2\n   rating     n\n    &lt;dbl&gt; &lt;int&gt;\n 1   3.5    552\n 2   3      506\n 3   3.25   453\n 4   2.75   320\n 5   3.75   295\n 6   2.5    156\n 7   4      111\n 8   2       29\n 9   2.25    14\n10   1.5      5\n11   1        1\n12   1.75     1\n\n\nScore range between 1 and 4, and the modal value is 3.5.\n\n# histogram\ndf %&gt;% \n  ggplot(aes(rating)) +\n  geom_histogram(bins = 14, alpha = .7, fill = \"midnightblue\") +\n    labs(x = \"Chocolate bar rating\",\n         y = \"Number of bars\")\n\n\n\n\n\n\n\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    knitr::kable(digits = 2)\n\n\n\n\nreview_date\nmean_rating\n\n\n\n\n2006\n3.06\n\n\n2007\n3.17\n\n\n2008\n3.04\n\n\n2009\n3.09\n\n\n2010\n3.19\n\n\n2011\n3.27\n\n\n2012\n3.21\n\n\n2013\n3.21\n\n\n2014\n3.20\n\n\n2015\n3.24\n\n\n2016\n3.24\n\n\n2017\n3.36\n\n\n2018\n3.19\n\n\n2019\n3.13\n\n\n2020\n3.26\n\n\n2021\n3.32\n\n\n\n\n\nIt certainly seems like the mean rating is increasing over time. What is driving this?\nWe can make a plot of the figures above to see the increasing trend.\n\ndf %&gt;% \n    group_by(review_date) %&gt;% \n    summarise(mean_rating = mean(rating)) %&gt;% \n    ungroup() %&gt;% \n    ggplot(aes(review_date, mean_rating)) +\n    geom_point(colour = \"midnightblue\", alpha = .6, size = 5) +\n    geom_smooth(method = \"lm\", se = F, colour = \"grey20\") +\n    labs(x = \"Date of review\",\n         y = \"Mean rating\")\n\n\n\n\nLet’s make a boxplot to see how the spread of scores has changed over time.\n\ndf %&gt;%\n  ggplot(aes(review_date, rating, group = review_date)) +\n  geom_jitter(alpha = .2) +\n  geom_boxplot(varwidth = TRUE, fill = \"midnightblue\", alpha = .6)\n\n\n\n\nIt seems as if the share of bars with very low scores has decreased over time, while the median value has remained relatively stable over time, shown by the bar in the centre of the boxplots.\nWhat about making a joy plot or ridgeline plot with the ggridges package? This allows us to see how the spread of values has changed over time.\n\nlibrary(ggridges)\n\ndf %&gt;%\n  ggplot(aes(rating, y = factor(review_date), fill = review_date)) +\n  geom_density_ridges() +\n  scale_fill_viridis_c(option = \"magma\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(\n    title.position = \"bottom\",\n    barwidth = 25,\n    title.hjust = .5\n  )) +\n    labs(y = NULL,\n         x = \"Chocolate bar rating\",\n         fill = \"Date of review\")\n\n\n\n\nThis confirms what we saw in the boxplots above: fewer low scores in more recent years mean that the mean has increased, while the top of the distributions remain largely the same.\n\n\n\n\ndf %&gt;% \n  mutate(cocoa_percent = round(cocoa_percent, 1)) %&gt;% \n  count(cocoa_percent, n_ingredients) %&gt;% \n  ggplot(aes(cocoa_percent, n_ingredients, fill = n)) +\n  geom_tile() +\n  scale_fill_viridis_c() +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(x = \"Cocoa percent\",\n       y = \"Number of ingredients\",\n       fill = \"Number of bars reviewed\") +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colorbar(title.position = \"bottom\",\n                               barwidth = 25,\n                               title.hjust = .5))\n\n\n\n\n\n\n\n\ndf %&gt;% \n  count(country_of_bean_origin, sort = T)\n\n# A tibble: 62 × 2\n   country_of_bean_origin     n\n   &lt;chr&gt;                  &lt;int&gt;\n 1 Venezuela                246\n 2 Peru                     231\n 3 Dominican Republic       220\n 4 Ecuador                  201\n 5 Madagascar               171\n 6 Blend                    144\n 7 Nicaragua                100\n 8 Bolivia                   79\n 9 Colombia                  78\n10 Tanzania                  78\n# … with 52 more rows\n\ndf %&gt;% \n  add_count(country_of_bean_origin) %&gt;%\n  # only include countries with more than 60 bars\n  filter(n &gt; 60) %&gt;% \n  group_by(country_of_bean_origin) %&gt;% \n  summarise(mean_rating = mean(rating)) %&gt;% \n  mutate(country_of_bean_origin = fct_reorder(country_of_bean_origin, mean_rating)) %&gt;% \n  ggplot(aes(mean_rating, country_of_bean_origin)) +\n  geom_col(fill = \"midnightblue\", alpha = .8) +\n  # ensure that x-axis looks appropriate.\n  coord_cartesian(xlim = c(3,3.3)) +\n    labs(x = \"Average rating for countries of origin with more than 60 bars reviewed\",\n         y = NULL)\n\n\n\n\n\n\n\n\nlibrary(tmap)\ndata(\"World\")\n\nworld &lt;- World %&gt;% as_tibble()\n\nTo join our data on chocolate to this map, we need to get coutnry codes, using the countrycode package.\n\nlibrary(countrycode)\n\ndf &lt;- df %&gt;% \n    mutate(iso_a3 = countrycode(sourcevar = country_of_bean_origin, origin = \"country.name\", destination = \"iso3c\"))\n\n\nlibrary(sf)\n\ndf_map &lt;- df %&gt;% \n    group_by(iso_a3) %&gt;%\n    add_count() %&gt;% \n    summarise(mean_rating = mean(rating),\n              n = n) %&gt;% \n    ungroup() %&gt;% \n    distinct() %&gt;% \n    left_join(world, by = \"iso_a3\")\n\n\ndf_map %&gt;% \n    filter(n &gt; 3) %&gt;% \n    st_as_sf() %&gt;% ggplot() +\n    geom_sf(data = World, fill = \"grey80\", alpha = .5) +\n    geom_sf(aes(fill = mean_rating)) +\n    scale_fill_viridis_c(trans = \"sqrt\") +\n    labs(fill = \"Mean country rating\")"
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#word-model",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#word-model",
    "title": "Flavours of cocoa",
    "section": "",
    "text": "df_characteristics &lt;- df %&gt;% \n  select(c(most_memorable_characteristics, rating)) %&gt;% \n  separate_rows(most_memorable_characteristics, sep = \",\") %&gt;% \n  mutate(most_memorable_characteristics = str_squish(most_memorable_characteristics))\n\n\ndf_characteristics %&gt;% \n  count(most_memorable_characteristics, sort = T)\n\n# A tibble: 948 × 2\n   most_memorable_characteristics     n\n   &lt;chr&gt;                          &lt;int&gt;\n 1 sweet                            260\n 2 nutty                            256\n 3 cocoa                            242\n 4 roasty                           212\n 5 creamy                           187\n 6 earthy                           181\n 7 sandy                            164\n 8 fatty                            161\n 9 floral                           141\n10 intense                          139\n# … with 938 more rows\n\n\nWe can start with a naive analysis that looks only at average score per word. These are the highest scoring words.\n\n# df_characteristics %&gt;% \n#   group_by(most_memorable_characteristics) %&gt;% \n#   add_count() %&gt;% \n#   mutate(avg_rating = mean(rating)) %&gt;% \n#   ungroup() %&gt;% \n#   slice_max(avg_rating, n = 12, with_ties = F)\n\ndf_characteristics %&gt;% \n  group_by(most_memorable_characteristics) %&gt;% \n  add_count() %&gt;% \n  filter(n &gt; 3) %&gt;% \n  mutate(avg_rating = mean(rating)) %&gt;% \n  ungroup() %&gt;% \n  distinct(most_memorable_characteristics, avg_rating) %&gt;% \n  slice_max(avg_rating, n = 12, with_ties = F) %&gt;% \n    mutate(avg_rating = round(avg_rating, 2)) %&gt;% \n    knitr::kable(col.names = c(\"Most memorable characteristics\", \"Average rating\"))\n\n\n\n\nMost memorable characteristics\nAverage rating\n\n\n\n\npeanut\n3.75\n\n\nwine\n3.75\n\n\nbalanced\n3.73\n\n\nraspberry\n3.70\n\n\nmild tart\n3.69\n\n\nrobust\n3.69\n\n\nrich choco\n3.69\n\n\nlong lasting\n3.62\n\n\nblackberry\n3.61\n\n\ndark berry\n3.61\n\n\nsubtle\n3.61\n\n\ndelicate\n3.60\n\n\n\n\n\n\nlibrary(tidymodels)\nlibrary(textrecipes)\n\ndf_characteristics_folds &lt;- vfold_cv(df_characteristics)\n\nglmnet_recipe &lt;- \n  recipe(formula = rating ~ ., data = df_characteristics) %&gt;% \n  step_tokenize(most_memorable_characteristics) %&gt;% \n  step_tokenfilter(most_memorable_characteristics, max_tokens = 100) %&gt;% \n  step_tf(most_memorable_characteristics) %&gt;% \n  step_normalize(all_predictors(), -all_nominal())\n\n\nglmnet_recipe %&gt;% prep() %&gt;% juice()\n\n# A tibble: 6,839 × 101\n   rating tf_most_memo…¹ tf_mo…² tf_mo…³ tf_mo…⁴ tf_mo…⁵ tf_mo…⁶ tf_mo…⁷ tf_mo…⁸\n    &lt;dbl&gt;          &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 2   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 3   3.25        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 4   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 5   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 6   3.5         -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 7   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 8   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n 9   3.75        -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n10   3           -0.0767 -0.0630 -0.0805 -0.0528 -0.0949 -0.0528 -0.0696  -0.107\n# … with 6,829 more rows, 92 more variables:\n#   tf_most_memorable_characteristics_bitter &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_black &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bland &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bold &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_bright &lt;dbl&gt;,\n#   tf_most_memorable_characteristics_brownie &lt;dbl&gt;, …\n\n\n\nglmnet_spec &lt;- \n  linear_reg(penalty = tune(), mixture = 1) %&gt;% \n  set_mode(\"regression\") %&gt;% \n  set_engine(\"glmnet\") \n\nglmnet_workflow &lt;- \n  workflow() %&gt;% \n  add_recipe(glmnet_recipe) %&gt;% \n  add_model(glmnet_spec) \n\nglmnet_grid &lt;- tidyr::crossing(penalty = 10^seq(-6, -1, length.out = 20)) \n\nglmnet_tune &lt;- \n  tune_grid(glmnet_workflow, df_characteristics_folds, grid = glmnet_grid)\n\n\nglmnet_tune %&gt;% \n  autoplot()\n\n\n\n\n\nglmnet_model_final &lt;- finalize_workflow(glmnet_workflow, glmnet_tune %&gt;% \n  select_best())\n\nfinal_fit &lt;- glmnet_model_final %&gt;% \n  fit(df_characteristics)\n\nWhat does the fit look like? These are the terms that have the greatest effect on bar rating.\n\nfinal_fit %&gt;%\n  extract_fit_parsnip() %&gt;%\n  tidy() %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  mutate(term = str_remove(term, \"tf_most_memorable_characteristics_\")) %&gt;%\n  mutate(sign = estimate &gt; 0) %&gt;%\n  group_by(sign) %&gt;%\n  mutate(estimate = abs(estimate)) %&gt;% \n  slice_max(estimate, n = 12) %&gt;%\n  ungroup() %&gt;%\n  mutate(estimate = ifelse(sign == TRUE, estimate, -estimate)) %&gt;% \n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term, fill = sign)) +\n  geom_col(show.legend = F) +\n  geom_vline(xintercept = 0, lty = 2) +\n  scale_fill_brewer(palette = \"Paired\") +\n  labs(x = \"Effect of term on chocolate bar score\",\n       y = \"Memorable characteristic\")\n\n\n\n\nWow! have a look at the terms up top - “creamy”, “complex” and “rich” are good chocolate words. On the other side, “bitter”, “off” and “chemical” are terms that lower the score of the bar."
  },
  {
    "objectID": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#footnotes",
    "href": "posts/2022-01-17-flavours-of-cocoa/flavours-of-cocoa.html#footnotes",
    "title": "Flavours of cocoa",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe here package allows us to abstract from the specific file path on our local computer and use a generic path that will work on any computer that we download our R project to. For example, instead of specifying the path “C:/Users/Jonathan/Documents/R-work/jjayes_distill/posts/2022-01-17-flavours-of-cocoa/data/chocolate-bar-ratings.xlsx” we can just call the here function from the package with the same name - here(“posts”, “2022-01-17-flavours-of-cocoa”, “data”, “chocolate-bar-ratings.xlsx”). This is wonderful for switching between, for example, a Windows and a Mac, where the slashes are in opposite directions and can cause some frustration!↩︎"
  },
  {
    "objectID": "posts/2021-05-06-swedenmap/swedenmap.html",
    "href": "posts/2021-05-06-swedenmap/swedenmap.html",
    "title": "Mapping Sweden’s Regional GDP",
    "section": "",
    "text": "The scatter plot below shows the relationship between population in the Swedish counties in 1571 (x-axis) and regional GDP share (y-axis).\nHovering over a point shows the name of the county, and highlights its location on the map of Sweden to the right (shown with their current borders).\nClicking on a county on the map will take you to it’s page on Wikipedia.\n\n\n\n\n\n\nSource: Enflo and Missiaia (2018), Data accessible here.\nFigure inspiration and code from Claus Wilke\n\n\n\nIn the scatter plot above we can see that the two northernmost counties, though large in land are are very sparsely populated. The line of best fit shows that Västmanland County has a high relative share of GDP compared to it’s population, and that Kalmar Län has a high population relative to it’s share of GDP in 1571."
  },
  {
    "objectID": "posts/2021-05-06-swedenmap/swedenmap.html#what-was-the-relationship-between-population-and-regional-gdp-in-sweden-in-1571",
    "href": "posts/2021-05-06-swedenmap/swedenmap.html#what-was-the-relationship-between-population-and-regional-gdp-in-sweden-in-1571",
    "title": "Mapping Sweden’s Regional GDP",
    "section": "",
    "text": "The scatter plot below shows the relationship between population in the Swedish counties in 1571 (x-axis) and regional GDP share (y-axis).\nHovering over a point shows the name of the county, and highlights its location on the map of Sweden to the right (shown with their current borders).\nClicking on a county on the map will take you to it’s page on Wikipedia.\n\n\n\n\n\n\nSource: Enflo and Missiaia (2018), Data accessible here.\nFigure inspiration and code from Claus Wilke\n\n\n\nIn the scatter plot above we can see that the two northernmost counties, though large in land are are very sparsely populated. The line of best fit shows that Västmanland County has a high relative share of GDP compared to it’s population, and that Kalmar Län has a high population relative to it’s share of GDP in 1571."
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html",
    "href": "posts/2021-04-21-chips/chips.html",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "",
    "text": "In this document I walk through the process to recreate a data visualization from Our World in Data. I use the web scraping package rvest to get the data from Wikipedia and some neat tidyverse tools.\nThe graphic depicts the metal–oxide–silicon (MOS) transistor count on commercially available microprocessors over time. This progression is known as Moore’s law, which observed that the transistor count doubles approximately every two years.\n\n\n\n\nGraphic showing Moore’s Law - Our World In Data\n\n\n\nFirst, I show the wikipedia page that contains the information that I want to scrape (and contains the Our World in Data graphic).\n\n\n\n\nWikipedia article\n\n\n\nThe Wikipedia article on transistor counts includes a table containing 193 microprocessors released between 1971 and 2020, shown below. Crucially, it specifies for each chip the release year and transistor count.\n\n\n\n\nTable showing transistor count"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#purpose",
    "href": "posts/2021-04-21-chips/chips.html#purpose",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "",
    "text": "In this document I walk through the process to recreate a data visualization from Our World in Data. I use the web scraping package rvest to get the data from Wikipedia and some neat tidyverse tools.\nThe graphic depicts the metal–oxide–silicon (MOS) transistor count on commercially available microprocessors over time. This progression is known as Moore’s law, which observed that the transistor count doubles approximately every two years.\n\n\n\n\nGraphic showing Moore’s Law - Our World In Data\n\n\n\nFirst, I show the wikipedia page that contains the information that I want to scrape (and contains the Our World in Data graphic).\n\n\n\n\nWikipedia article\n\n\n\nThe Wikipedia article on transistor counts includes a table containing 193 microprocessors released between 1971 and 2020, shown below. Crucially, it specifies for each chip the release year and transistor count.\n\n\n\n\nTable showing transistor count"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#scraping",
    "href": "posts/2021-04-21-chips/chips.html#scraping",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "Scraping",
    "text": "Scraping\nThis chunk of code grabs the HTML of the Wikipedia page, selects all the tables on the page and saves the one we want to analyze as a tibble, a very handy dataframe alternative. Rvest’s html_table function returns all of the elements of a web table, neat!\n\nurl &lt;- \"https://en.wikipedia.org/wiki/Transistor_count\"\n\nhtml &lt;- read_html(url)\n\nhtml_tables &lt;- html %&gt;% \n  html_table()\n\ndf_transistors &lt;- html_tables[[4]]"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#data-processing",
    "href": "posts/2021-04-21-chips/chips.html#data-processing",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "Data processing",
    "text": "Data processing\nAs visible in the screen capture of the table above, a few cells also hyper-link to sources for the stats. These square brackets prevent R from reading the column as a set of numbers, instead the cell contents are returned as characters. In the chunk below I use readr’s parse_number function to extract the numbers from each cell and store them as a double.\n\ndf_transistors &lt;- df_transistors %&gt;% \n  janitor::clean_names() %&gt;% \n  mutate(processor = str_squish(str_remove(processor, \"\\\\(.*\"))) %&gt;% \n  mutate_at(c(\"transistor_count\", \"date_ofintroduction\", \"process_nm\", \"area_mm2\"), parse_number)"
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#data-viz-preparation",
    "href": "posts/2021-04-21-chips/chips.html#data-viz-preparation",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "Data viz preparation",
    "text": "Data viz preparation\nIn this chunk I lump together the chip designers into the five largest groups and an ‘other’ category with forcats’ fct_lump function. It returns these categories as factors. To make the graphic readable, I sample 50 of the processors from the table of 193 to annotate onto the plot I also store some graph labels.\n\ndf_transistors %&gt;% \n  count(designer, sort = T)\n\n# A tibble: 44 × 2\n   designer              n\n   &lt;chr&gt;             &lt;int&gt;\n 1 Intel                64\n 2 Apple                22\n 3 AMD                  21\n 4 IBM                  19\n 5 Fujitsu              10\n 6 Huawei                7\n 7 Motorola              7\n 8 Qualcomm              7\n 9 DEC                   5\n10 Texas Instruments     5\n# … with 34 more rows\n\ndf_transistors &lt;- df_transistors %&gt;% \n  mutate(designer = fct_lump(designer, 5))\n\nprocessor_labels &lt;- sample(df_transistors$processor, 50)\n\ntitle &lt;- \"Moore's Law\"\n\nsubtitle &lt;- \"The number of transistors on integrated circuit chips (1971-2020)\"\n\ncaption &lt;- \"Source: Wikipedia - Transistor Count: https://en.wikipedia.org/wiki/Transistor_count\""
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#plotting",
    "href": "posts/2021-04-21-chips/chips.html#plotting",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "Plotting",
    "text": "Plotting\nHere I draw the graphic using ggplot2.\n\n# jpeg(\n#     filename=\"C:/Users/User/Documents/Recon/jjayes_distill/_posts/2021-04-21-chips/figures/Transistors_colour.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_transistors %&gt;% \n  ggplot(aes(date_ofintroduction, transistor_count)) +\n  geom_smooth(group = 1, alpha = .1) +\n  geom_point(aes(colour = designer, shape = designer), cex = 3) +\n  geom_text_repel(data = df_transistors %&gt;% \n                    filter(processor %in% processor_labels), \n                  aes(date_ofintroduction, transistor_count, label = processor),\n                  cex = 3, colour = \"#505050\") +\n  scale_color_brewer(palette = \"Dark2\") +\n  scale_shape_manual(values = seq(15,20, by = 1)) +\n  scale_y_log10(labels = scales::comma_format()) +\n  labs(x = \"Year\",\n       y = \"Transistor Count (log scale)\",\n       shape = \"Chip Designer\",\n       colour = \"Chip Designer\",\n       caption = caption,\n       title = title,\n       subtitle = subtitle)\n\n\n\n# dev.off()\n\nI include a smoothed line of best fit (with geom_smooth) below the points to show how progression in transistor count has slowed since 2005, despite the number of chips designed each year increasing.\nI use geom_text_repel to prevent the labels overlapping each other, and put the y-axis on a log scale to draw attention to the approximate doubling of transistor count each year."
  },
  {
    "objectID": "posts/2021-04-21-chips/chips.html#conclusion",
    "href": "posts/2021-04-21-chips/chips.html#conclusion",
    "title": "Recreating Our World In Data’s Beautiful Transistor Count Graphic",
    "section": "Conclusion",
    "text": "Conclusion\nThanks for reading along! I hope you learned some neat tricks, and see how easy it is to grab data from the web to visualize."
  },
  {
    "objectID": "posts/2021-03-18-car-price-explorer/car-price-explorer.html",
    "href": "posts/2021-03-18-car-price-explorer/car-price-explorer.html",
    "title": "Car Price Comparison Tool",
    "section": "",
    "text": "Here’s a link to the web app, along with an image of what it looks like.\nThe app has three components. In the scatter plot you can see the price of each car advertised on Gumtree on the y-axis against its odometer reading on the x. Below this is a box and whisker plot of price to show the distribution of prices for the cars in the range. To the right is a table showing the details of each advert along with a link. You can choose the make, model, province, odometer range and year of manufacture in the panel on the left.\n\n\n\n\nWeb App\n\n\n\nI hope that you find it useful!"
  },
  {
    "objectID": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html",
    "href": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html",
    "title": "South African Car Colour Exploration",
    "section": "",
    "text": "While driving to Cape Town this past weekend in my beautiful white Hyundai i10, a German friend asked why every second car in South Africa was white in colour. While I didn’t have a compelling reason, I did have a some data to assess whether it was true that half of cars on the road are white.\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\nI show a useful trick for filling in missing data that creates gaps in the area chart using dplyr’s pivot_wider and pivot_longer commands.\nHere is the final interactive graphic:\n\ndf_plotly &lt;- read_rds(\"data/car_colour/df_plotly.rds\")\n\np_load(plotly)\n\ng &lt;- df_plotly %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\ndf &lt;- readRDS(\"data/car_colour/gumtree_ads_2020-10-01-11-33-am.rds\")\n\ndf &lt;- df %&gt;% filter(!is.na(make),\n                    !is.na(colour),\n                    !is.na(model)) %&gt;% \n             # Removing the duplicated province names   \n             mutate(province = str_remove(province, \"\\\\,.*\"))\n\n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n44644\n\n\nNumber of columns\n30\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n22\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nlink\n0\n1.00\n68\n86\n0\n2218\n0\n\n\nad_url\n0\n1.00\n78\n193\n0\n38392\n0\n\n\nsite\n0\n1.00\n7\n7\n0\n1\n0\n\n\nseller_type\n0\n1.00\n19\n30\n0\n2\n0\n\n\ntitle\n0\n1.00\n4\n100\n0\n26140\n0\n\n\ntext\n0\n1.00\n10\n3885\n0\n38343\n0\n\n\nad_date\n0\n1.00\n19\n19\n0\n42416\n0\n\n\nseller_name\n0\n1.00\n1\n50\n0\n8872\n0\n\n\nlocation\n0\n1.00\n24\n61\n0\n428\n0\n\n\nseller_age\n0\n1.00\n17\n22\n0\n79\n0\n\n\nn_all_time_ads\n0\n1.00\n1\n7\n0\n3481\n0\n\n\nscrape_time\n0\n1.00\n19\n19\n0\n35812\n0\n\n\nlocation_2\n0\n1.00\n11\n47\n0\n428\n0\n\n\nfor_sale_by\n226\n0.99\n5\n6\n0\n2\n0\n\n\nmake\n0\n1.00\n3\n13\n0\n37\n0\n\n\nmodel\n0\n1.00\n1\n20\n0\n459\n0\n\n\nbody_type\n226\n0.99\n3\n12\n0\n11\n0\n\n\ntransmission\n206\n1.00\n6\n9\n0\n2\n0\n\n\nfuel_type\n104\n1.00\n6\n8\n0\n4\n0\n\n\ncolour\n0\n1.00\n3\n8\n0\n16\n0\n\n\nna\n44644\n0.00\nNA\nNA\n0\n0\n0\n\n\nprovince\n0\n1.00\n7\n13\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npage\n0\n1.00\n14.51\n12.66\n1\n4.00\n11.0\n22.00\n50\n▇▃▂▁▁\n\n\nad_number\n0\n1.00\n23570.95\n14487.85\n1\n11471.75\n22889.5\n34258.25\n56454\n▇▇▇▆▂\n\n\nprice\n229\n0.99\n218706.68\n692268.63\n1\n109900.00\n165000.0\n259900.00\n140000000\n▇▁▁▁▁\n\n\nn_photos\n1099\n0.98\n8.63\n3.08\n1\n7.00\n9.0\n12.00\n12\n▁▂▃▃▇\n\n\nn_views\n0\n1.00\n588.85\n1281.69\n0\n91.00\n233.5\n580.00\n46956\n▇▁▁▁▁\n\n\nn_active_ads\n0\n1.00\n130.20\n225.27\n1\n5.50\n29.0\n100.00\n997\n▇▁▁▁▁\n\n\nyear\n100\n1.00\n2013.76\n6.66\n1900\n2011.00\n2016.0\n2018.00\n2021\n▁▁▁▁▇\n\n\nkilometers\n1189\n0.97\n92267.35\n157493.13\n1\n26890.50\n72000.0\n130000.00\n9999999\n▇▁▁▁▁\n\n\n\n\n\nThe dataset has some really nice information on the cars advertised, including their characteristics, as well as the location and name of the seller.\nFor this graphic, we only need the car’s year and colour.\n\n\n\nWe start by plotting the number of cars in each colour advertised on Gumtree. In this plot I show the 12 most common colours.\nIt is evident that white and silver cars are very common. In as far as our data reflects cars on the road, white cars make up nearly 1/3 of all cars advertised! A little less than the half quoted in the discussion above, but certainly the dominant colour.\n\ndf %&gt;%\n  mutate(colour = fct_lump(colour, 12)) %&gt;%\n  select(colour) %&gt;% \n  count(colour, sort = T) %&gt;% \n  mutate(colour = fct_reorder(colour, n)) %&gt;% \n  ggplot(aes(n, colour)) +\n  geom_segment(aes(xend = 0, yend = colour)) +\n  geom_point(size = 4, colour = \"orange\") +\n  geom_text(aes(label = scales::comma(n)), hjust = -.3) +\n  scale_x_continuous(labels = scales::comma_format(), limits = c(0, 16000)) +\n  labs(x = \"Number of vehicles\",\n       title = \"What are the most common vehicle colours advertised on Gumtree?\",\n       subtitle = \"Data from 45,000 adverts from across the country\",\n       y = \"\")\n\n\n\n\n\n\n\nNext we want to know if some colours have become more popular over time. Given that our data source is a snapshot of cars for sale at one point in time and on one online website, this is unlikely to be representative of the entire population of cars in South Africa. However, it will be interesting to see the trends that do exist in this data.\nWe start with a simple plot of the number of cars advertised in the none more common colours, by year.\n\ndf_colours &lt;- df %&gt;% \n  group_by(year, colour) %&gt;% \n  summarise(n = n()) %&gt;%\n  ungroup() %&gt;% \n  filter(between(year, 1990, 2020)) \n\nIt makes sense that the majority of adverts are for cars that are produced recently, and it is striking that white and silver cars see such a dramatic increase, even relative to the third most common car colour, grey.\n\ndf_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n  filter(colour != \"Other\") %&gt;% \n  mutate(colour = fct_reorder(colour, n, .fun = sum, .desc = T)) %&gt;% \n  ggplot(aes(year, n, fill = colour)) +\n  geom_area() +\n  facet_wrap(~ colour)\n\n\n\n\n\n\n\nIn this plot we show the evolution of car colour between 1990 and the present.\nThere are a few problems with this plot. First, there are too many colours, the least common of which have just a few entries. Second, the missing values for the less common colours create unappealing white gaps in the bottom left of the plot.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(colour = fct_reorder(colour, nn, .fun = sum, .desc = T)) %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\nTo solve this problem, we can fill in the missing values by making the dataset wider and then long gain.\nUsing dplyr’s pivot_wider and the values_fill = 0 option, we create a wide dataset with one row for each year, shown below.\nWe go from a long dataset like this:\n\nhead(df_colours)\n\n# A tibble: 6 × 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      3\n\n\nTo a wide one, like this:\n\ndf_colours &lt;- df_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n    group_by(year, colour) %&gt;% \n    mutate(n = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  distinct() %&gt;% \n  pivot_wider(names_from = colour, values_from = n, values_fill = 0) \n\nhead(df_colours)\n\n# A tibble: 6 × 11\n   year Black  Blue  Gold Green  Grey Other   Red Silver White Burgundy\n  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n1  1990     2    14     3     3     5     4    10      3    10        0\n2  1991     3    11     2     4     4     3     9      3    12        1\n3  1992     1    16     2     4     5     7    11      3    14        1\n4  1993     0    13     2     1     4     1    13      7    18        1\n5  1994     2    15     0     8     7     1    13      6    41        6\n6  1995     6    36     0    17     6     3    32      7    34        7\n\n\nNow we have a ‘wide’ dataset with the missing values filled in with zeros\nWe can then make it long again, with the pivot_longer command, specifying that we want the original column names back.\n\ndf_colours &lt;- df_colours %&gt;% \n  pivot_longer(!year, names_to = \"colour\", values_to = \"n\")\n\nhead(df_colours)\n\n# A tibble: 6 × 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      4\n\n\n\n\n\nNow we have a plot with no white gaps, and an appropriate number of colours. However, it would be nice to have the colours on the plot match the colours of the cars, and have some labels to make it clear to the reader how to interpret the graphic.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\nWe use ggplot’s scale_fill_manual to specify the hex codes for the colours in the order that they appear in the plot.\nWe add in a title and axes labels, and make the plot interactive with ggplotly.\n\ng &lt;- df_colours %&gt;% \n  filter(year &gt; 1994) %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\n\n\nWow! Blue and Green cars really lost out to white and Silver ones over time. Of the cars advertised that were produced in 2020, nearly 42% are white!\nBlack cars seemed to have a peak in popularity around 2007, and then decline again, coinciding with the first round of load-shedding in South Africa.\nThanks for reading along, and I hope you enjoyed learning about ggplot as well as the cars on South Africa’s roads!\n\n\n\nHere is the static figure with accompanying caption.\n\n# jpeg(\n#     filename=\"figures/stacked_area_2.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_colours %&gt;%\n  filter(year &gt; 1994) %&gt;%\n  group_by(year) %&gt;%\n  add_tally(n) %&gt;%\n  ungroup() %&gt;%\n  mutate(share = n / nn) %&gt;%\n  mutate(colour = fct_relevel(colour, \"Silver\")) %&gt;%\n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#95A5A6\", \"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#EAECEE\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\n\n\n# dev.off()\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html#purpose",
    "href": "posts/2021-03-13-south-africa-car-colour-exploration/south-africa-car-colour-exploration.html#purpose",
    "title": "South African Car Colour Exploration",
    "section": "",
    "text": "While driving to Cape Town this past weekend in my beautiful white Hyundai i10, a German friend asked why every second car in South Africa was white in colour. While I didn’t have a compelling reason, I did have a some data to assess whether it was true that half of cars on the road are white.\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\nI show a useful trick for filling in missing data that creates gaps in the area chart using dplyr’s pivot_wider and pivot_longer commands.\nHere is the final interactive graphic:\n\ndf_plotly &lt;- read_rds(\"data/car_colour/df_plotly.rds\")\n\np_load(plotly)\n\ng &lt;- df_plotly %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\ndf &lt;- readRDS(\"data/car_colour/gumtree_ads_2020-10-01-11-33-am.rds\")\n\ndf &lt;- df %&gt;% filter(!is.na(make),\n                    !is.na(colour),\n                    !is.na(model)) %&gt;% \n             # Removing the duplicated province names   \n             mutate(province = str_remove(province, \"\\\\,.*\"))\n\n\n\n\nskimr::skim(df)\n\n\nData summary\n\n\nName\ndf\n\n\nNumber of rows\n44644\n\n\nNumber of columns\n30\n\n\n_______________________\n\n\n\nColumn type frequency:\n\n\n\ncharacter\n22\n\n\nnumeric\n8\n\n\n________________________\n\n\n\nGroup variables\nNone\n\n\n\nVariable type: character\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmin\nmax\nempty\nn_unique\nwhitespace\n\n\n\n\nlink\n0\n1.00\n68\n86\n0\n2218\n0\n\n\nad_url\n0\n1.00\n78\n193\n0\n38392\n0\n\n\nsite\n0\n1.00\n7\n7\n0\n1\n0\n\n\nseller_type\n0\n1.00\n19\n30\n0\n2\n0\n\n\ntitle\n0\n1.00\n4\n100\n0\n26140\n0\n\n\ntext\n0\n1.00\n10\n3885\n0\n38343\n0\n\n\nad_date\n0\n1.00\n19\n19\n0\n42416\n0\n\n\nseller_name\n0\n1.00\n1\n50\n0\n8872\n0\n\n\nlocation\n0\n1.00\n24\n61\n0\n428\n0\n\n\nseller_age\n0\n1.00\n17\n22\n0\n79\n0\n\n\nn_all_time_ads\n0\n1.00\n1\n7\n0\n3481\n0\n\n\nscrape_time\n0\n1.00\n19\n19\n0\n35812\n0\n\n\nlocation_2\n0\n1.00\n11\n47\n0\n428\n0\n\n\nfor_sale_by\n226\n0.99\n5\n6\n0\n2\n0\n\n\nmake\n0\n1.00\n3\n13\n0\n37\n0\n\n\nmodel\n0\n1.00\n1\n20\n0\n459\n0\n\n\nbody_type\n226\n0.99\n3\n12\n0\n11\n0\n\n\ntransmission\n206\n1.00\n6\n9\n0\n2\n0\n\n\nfuel_type\n104\n1.00\n6\n8\n0\n4\n0\n\n\ncolour\n0\n1.00\n3\n8\n0\n16\n0\n\n\nna\n44644\n0.00\nNA\nNA\n0\n0\n0\n\n\nprovince\n0\n1.00\n7\n13\n0\n9\n0\n\n\n\nVariable type: numeric\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nskim_variable\nn_missing\ncomplete_rate\nmean\nsd\np0\np25\np50\np75\np100\nhist\n\n\n\n\npage\n0\n1.00\n14.51\n12.66\n1\n4.00\n11.0\n22.00\n50\n▇▃▂▁▁\n\n\nad_number\n0\n1.00\n23570.95\n14487.85\n1\n11471.75\n22889.5\n34258.25\n56454\n▇▇▇▆▂\n\n\nprice\n229\n0.99\n218706.68\n692268.63\n1\n109900.00\n165000.0\n259900.00\n140000000\n▇▁▁▁▁\n\n\nn_photos\n1099\n0.98\n8.63\n3.08\n1\n7.00\n9.0\n12.00\n12\n▁▂▃▃▇\n\n\nn_views\n0\n1.00\n588.85\n1281.69\n0\n91.00\n233.5\n580.00\n46956\n▇▁▁▁▁\n\n\nn_active_ads\n0\n1.00\n130.20\n225.27\n1\n5.50\n29.0\n100.00\n997\n▇▁▁▁▁\n\n\nyear\n100\n1.00\n2013.76\n6.66\n1900\n2011.00\n2016.0\n2018.00\n2021\n▁▁▁▁▇\n\n\nkilometers\n1189\n0.97\n92267.35\n157493.13\n1\n26890.50\n72000.0\n130000.00\n9999999\n▇▁▁▁▁\n\n\n\n\n\nThe dataset has some really nice information on the cars advertised, including their characteristics, as well as the location and name of the seller.\nFor this graphic, we only need the car’s year and colour.\n\n\n\nWe start by plotting the number of cars in each colour advertised on Gumtree. In this plot I show the 12 most common colours.\nIt is evident that white and silver cars are very common. In as far as our data reflects cars on the road, white cars make up nearly 1/3 of all cars advertised! A little less than the half quoted in the discussion above, but certainly the dominant colour.\n\ndf %&gt;%\n  mutate(colour = fct_lump(colour, 12)) %&gt;%\n  select(colour) %&gt;% \n  count(colour, sort = T) %&gt;% \n  mutate(colour = fct_reorder(colour, n)) %&gt;% \n  ggplot(aes(n, colour)) +\n  geom_segment(aes(xend = 0, yend = colour)) +\n  geom_point(size = 4, colour = \"orange\") +\n  geom_text(aes(label = scales::comma(n)), hjust = -.3) +\n  scale_x_continuous(labels = scales::comma_format(), limits = c(0, 16000)) +\n  labs(x = \"Number of vehicles\",\n       title = \"What are the most common vehicle colours advertised on Gumtree?\",\n       subtitle = \"Data from 45,000 adverts from across the country\",\n       y = \"\")\n\n\n\n\n\n\n\nNext we want to know if some colours have become more popular over time. Given that our data source is a snapshot of cars for sale at one point in time and on one online website, this is unlikely to be representative of the entire population of cars in South Africa. However, it will be interesting to see the trends that do exist in this data.\nWe start with a simple plot of the number of cars advertised in the none more common colours, by year.\n\ndf_colours &lt;- df %&gt;% \n  group_by(year, colour) %&gt;% \n  summarise(n = n()) %&gt;%\n  ungroup() %&gt;% \n  filter(between(year, 1990, 2020)) \n\nIt makes sense that the majority of adverts are for cars that are produced recently, and it is striking that white and silver cars see such a dramatic increase, even relative to the third most common car colour, grey.\n\ndf_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n  filter(colour != \"Other\") %&gt;% \n  mutate(colour = fct_reorder(colour, n, .fun = sum, .desc = T)) %&gt;% \n  ggplot(aes(year, n, fill = colour)) +\n  geom_area() +\n  facet_wrap(~ colour)\n\n\n\n\n\n\n\nIn this plot we show the evolution of car colour between 1990 and the present.\nThere are a few problems with this plot. First, there are too many colours, the least common of which have just a few entries. Second, the missing values for the less common colours create unappealing white gaps in the bottom left of the plot.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(colour = fct_reorder(colour, nn, .fun = sum, .desc = T)) %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\nTo solve this problem, we can fill in the missing values by making the dataset wider and then long gain.\nUsing dplyr’s pivot_wider and the values_fill = 0 option, we create a wide dataset with one row for each year, shown below.\nWe go from a long dataset like this:\n\nhead(df_colours)\n\n# A tibble: 6 × 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      3\n\n\nTo a wide one, like this:\n\ndf_colours &lt;- df_colours %&gt;% \n  mutate(colour = fct_lump(colour, 10)) %&gt;%\n    group_by(year, colour) %&gt;% \n    mutate(n = sum(n)) %&gt;% \n  ungroup() %&gt;% \n  distinct() %&gt;% \n  pivot_wider(names_from = colour, values_from = n, values_fill = 0) \n\nhead(df_colours)\n\n# A tibble: 6 × 11\n   year Black  Blue  Gold Green  Grey Other   Red Silver White Burgundy\n  &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;  &lt;int&gt; &lt;int&gt;    &lt;int&gt;\n1  1990     2    14     3     3     5     4    10      3    10        0\n2  1991     3    11     2     4     4     3     9      3    12        1\n3  1992     1    16     2     4     5     7    11      3    14        1\n4  1993     0    13     2     1     4     1    13      7    18        1\n5  1994     2    15     0     8     7     1    13      6    41        6\n6  1995     6    36     0    17     6     3    32      7    34        7\n\n\nNow we have a ‘wide’ dataset with the missing values filled in with zeros\nWe can then make it long again, with the pivot_longer command, specifying that we want the original column names back.\n\ndf_colours &lt;- df_colours %&gt;% \n  pivot_longer(!year, names_to = \"colour\", values_to = \"n\")\n\nhead(df_colours)\n\n# A tibble: 6 × 3\n   year colour     n\n  &lt;dbl&gt; &lt;chr&gt;  &lt;int&gt;\n1  1990 Black      2\n2  1990 Blue      14\n3  1990 Gold       3\n4  1990 Green      3\n5  1990 Grey       5\n6  1990 Other      4\n\n\n\n\n\nNow we have a plot with no white gaps, and an appropriate number of colours. However, it would be nice to have the colours on the plot match the colours of the cars, and have some labels to make it clear to the reader how to interpret the graphic.\n\ndf_colours %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area()\n\n\n\n\n\n\n\nWe use ggplot’s scale_fill_manual to specify the hex codes for the colours in the order that they appear in the plot.\nWe add in a title and axes labels, and make the plot interactive with ggplotly.\n\ng &lt;- df_colours %&gt;% \n  filter(year &gt; 1994) %&gt;% \n  group_by(year) %&gt;% \n  add_tally(n) %&gt;% \n  ungroup() %&gt;% \n  mutate(share = n / nn) %&gt;% \n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#95A5A6\", \"#F8F9F9\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\nggplotly(g)\n\n\n\n\n\n\n\n\nWow! Blue and Green cars really lost out to white and Silver ones over time. Of the cars advertised that were produced in 2020, nearly 42% are white!\nBlack cars seemed to have a peak in popularity around 2007, and then decline again, coinciding with the first round of load-shedding in South Africa.\nThanks for reading along, and I hope you enjoyed learning about ggplot as well as the cars on South Africa’s roads!\n\n\n\nHere is the static figure with accompanying caption.\n\n# jpeg(\n#     filename=\"figures/stacked_area_2.jpeg\",\n#     width=6,\n#     height=4,\n#     units=\"in\",\n#     res=1000)\n\ndf_colours %&gt;%\n  filter(year &gt; 1994) %&gt;%\n  group_by(year) %&gt;%\n  add_tally(n) %&gt;%\n  ungroup() %&gt;%\n  mutate(share = n / nn) %&gt;%\n  mutate(colour = fct_relevel(colour, \"Silver\")) %&gt;%\n  ggplot(aes(year, share, fill = colour)) +\n  geom_area(colour = \"black\", size = .2, alpha = .8) +\n  scale_fill_manual(values = c(\"#95A5A6\", \"#17202A\", \"#2471A3\", \"#800020\", \"#F39C12\", \"#27AE60\", \"#BDC3C7\", \"midnightblue\", \"#C0392B\", \"#EAECEE\")) +\n  scale_y_continuous(labels = scales::percent_format()) +\n  theme(legend.position = \"bottom\",\n        panel.grid = element_blank()) +\n  labs(title = \"Evolution of car colour in South Africa: 1995-2020\",\n       subtitle = \"Based on Gumtree vehicle adverts\",\n       x = \"Year of production\",\n       y = \"Share of cars in each colour per year\",\n       fill = \"\",\n       caption = \"Source: Gumtree vehicle adverts scraped in October 2020\\nGraphic: Jonathan Jayes\")\n\n\n\n# dev.off()\n\nDistill is a publication format for scientific and technical writing, native to the web.\nLearn more about using Distill at https://rstudio.github.io/distill."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "",
    "text": "I recently visited my cousin Robyn in the United Kingdom. In her house there was, in addition to a standard tap in the kitchen sink, a tap that dispensed boiling water instantly. It was convenient, removing almost entirely the delay between the thought of tea and the first sip. But is it economically sound compared to a simple electric kettle?\nTLDR: I calculated that it would take  years for the instant hot water tap to be more cost-effective than a kettle. This is because the instant hot water tap is much more expensive than a kettle up-front, and the energy savings are minimal even under an improbable use case of  cups of tea per day.\nRead on to see how I came to this conclusion, and play with the assumptions in order to calculate how long it would take for the Quooker to be more cost-effective than a kettle for your family.1"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-is-an-instant-hot-water-tap",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-is-an-instant-hot-water-tap",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "What is an instant hot water tap?",
    "text": "What is an instant hot water tap?\nThe instant hot water tap system consists of a small, pressurised reservoir that is heated to 110 degrees Celsius and a tap to dispense the water. The system can be installed below a kitchen sink to minimise the distance and potential heat loss between the tap and the reservoir. Because the reservoir is pressurised, the water is unable to boil, even at 110 degrees Celsius. When the tap is opened, the pressure is released, and the water boils instantly.\nFor the remainder of the blog, I will refer to this system as a Quooker, a dutch brand which produces these taps, about which I could find the most information on the internet.\n\n\n\nOverview of instant hot water tap installation"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-does-it-compare-to-a-kettle",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-does-it-compare-to-a-kettle",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "How does it compare to a kettle?",
    "text": "How does it compare to a kettle?\nAlong with convenience, the Quooker also claims to be more energy efficient than a kettle by providing the exact amount of boiling water required for a cup of tea. This is in contrast to a kettle, which is often overfilled by the user, wasting energy and water.\nMy goal was thus to calculate for how long these energy savings would have to accrue before the Quooker became more cost-effective than a kettle. To do this, I ask GPT-4 to help me calculate the energy consumption of both the Quooker and a standard electric kettle."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#calculating-the-energy-consumption-of-the-quooker-and-a-kettle",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#calculating-the-energy-consumption-of-the-quooker-and-a-kettle",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Calculating the energy consumption of the Quooker and a kettle",
    "text": "Calculating the energy consumption of the Quooker and a kettle\n\nAssumptions\nFirst we pin down the scenarios under which the two systems are compared. My assumptions are:\n\n\n\n\n\n\n\n\n\nKettle Assumptions\nQuooker Assumptions\nOther Assumptions\n\n\n\n\nMy plastic kettle is rather old, and I have no idea what its efficiency is. I assume that it is % efficient.\nThe Quooker is % efficient.\nI drink  cups of tea per day.\n\n\nI overfill my kettle by  ml.\nThe Quooker is always on, maintaining the water at 110 degrees Celsius and using 10 Watts to do so.\nThe water in the kettle cools completely between each boil.\n\n\nThe kettle costs € .\nThe Quooker costs € .\nElectricity costs €  per kWh.\n\n\n\n\n\n\nCalculations\nWe can then calculate the energy consumption of each system. The energy consumption of the kettle is calculated as follows:\n\\(q = mc\\Delta T\\) where:\n\nq is the heat energy required (in Joules),\nm is the mass of the water (in kg),\nc is the specific heat capacity of water (4200 J/kg°C), and\nΔT is the change in temperature (in °C).\n\nWe then convert the energy from Joules to kWh, and multiply by the electricity price to get the cost of boiling the kettle.\nEnergy (kWh) = Energy (Joules) / 3.6e6\nCost = Energy (kWh) * Cost per kWh\nPlugging in the numbers, we get the following calculation for the energy consumption of the kettle:\nBecause of overfilling the kettle, we must raise the temperature of g of water by °C each time we boil the kettle, even if we only use g of water in our tea. The kettle requires  kilojoules of energy, or  kWh each time we boil it. This is equivalent to €  per day at  cups of tea per day.\nFor the Quooker, we get the following energy consumption:\nBecause the Quooker uses 10 Watts to maintain the water at 110 degrees Celsius, it uses 0.24 kWh per day in standby. This is equivalent to €  per day. In addition, we have to heat g of water to 110 degrees Celsius, which requires  kilojoules of energy, or  kWh for each cup of tea. This is equivalent to €  per day at  cups of tea per day.\nSo, the kettle uses €  per year, and the Quooker uses €  per year under our assumptions.\nGiven then price of the Quooker at €  and the price of the kettle at € , the Quooker will pay for itself in  years."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#find-out-for-yourself",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#find-out-for-yourself",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Find out for yourself",
    "text": "Find out for yourself\nYou can adjust any of these assumptions in the inputs below to see how they affect the result. The entire article is dynamic, so adjust the inputs and read it again from the top to see how the calculations change and what the payoff period is in your case!2\n\nimport { aq, op } from '@uwdata/arquero'\n\n\n\n\n\n\n\nviewof kettle_efficiency= Inputs.range(\n  [50, 100], \n  {value: 70, step: 5, label: \"Kettle efficiency:\"}\n)\n\nviewof Quooker_efficiency= Inputs.range(\n  [50, 100], \n  {value: 90, step: 5, label: \"Quooker efficiency:\"}\n)\n\nviewof cups_per_day= Inputs.range(\n  [0, 40], \n  {value: 8, step: 1, label: \"Cups of tea per day:\"}\n)\n\nviewof kettle_overfill= Inputs.range(\n  [0, 1000], \n  {value: 250, step: 50, label: \"ml by which kettle is overfilled:\"}\n)\n\nviewof Quooker_price= Inputs.range(\n  [50, 2000], \n  {value: 1000, step: 50, label: \"Price of Quooker (Euro):\"}\n)\n\nviewof kettle_price= Inputs.range(\n  [50, 2000], \n  {value: 50, step: 50, label: \"Price of kettle (Euro):\"}\n)\n\nviewof electricity_price= Inputs.range(\n  [0, 1], \n  {value: .3, step: 0.05, label: \"Price of electricity per kWh (Euro):\"}\n)\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nc = 4.184  // Specific heat capacity of water in J/g°C\nT_change = 80  // Change in temperature for kettle (100°C - 20°C)\nT_change_Quooker = 90  // Change in temperature for Quooker (110°C - 20°C)\nwater_density = 1  // Density of water in g/mL\nmass_tea = 250 * water_density\nmass_overfill = kettle_overfill * water_density\nmass_kettle = mass_tea + mass_overfill\nQ_kettle = (mass_kettle * c * T_change) / (kettle_efficiency / 100)  // Energy in joules\nQ_kettle_kWh = Q_kettle / (3.6 * Math.pow(10, 6))  // Convert energy to kWh\ntotal_kettle_energy = Q_kettle_kWh * cups_per_day // Total energy for kettle per day\nQ_Quooker = (mass_tea * c * T_change_Quooker) / (Quooker_efficiency / 100)  // Energy in joules\nQ_Quooker_kWh = Q_Quooker / (3.6 * Math.pow(10, 6))  // Convert energy to kWh\ntotal_Quooker_energy = (Q_Quooker_kWh * cups_per_day) + 0.24  // Total energy for Quooker per day\nkettle_cost_daily = total_kettle_energy * electricity_price\nQuooker_cost_daily = total_Quooker_energy * electricity_price\nkettle_cost_yearly = kettle_cost_daily * 365\nQuooker_cost_yearly = Quooker_cost_daily * 365\n\nkettle_cost_yearly_round = Math.round(kettle_cost_yearly * 100) / 100\nQuooker_cost_yearly_round = Math.round(Quooker_cost_yearly * 100) / 100\n\nQ_kettle_wasted = (mass_overfill * c * T_change) / (kettle_efficiency / 100)\nQ_kettle_wasted_kWh = Q_kettle_wasted / (3.6 * Math.pow(10, 6))\ntotal_kettle_energy_wasted = Q_kettle_wasted_kWh * cups_per_day\ntotal_kettle_energy_wasted_round = Math.round(total_kettle_energy_wasted * 100) / 100\ntotal_kettle_energy_wasted_cost_round = Math.round(total_kettle_energy_wasted * electricity_price * 365)\npercentage_kettle_energy_wasted = Math.round(total_kettle_energy_wasted / total_kettle_energy * 100)\n\nQ_Quooker_wasted = 0.24\ntotal_Quooker_energy_wasted_cost_round = Math.round(Q_Quooker_wasted * electricity_price * 365)\npercentage_Quooker_energy_wasted_round = Math.round(Q_Quooker_wasted / total_Quooker_energy * 100)\n\nyears = (Quooker_price - kettle_price) / (kettle_cost_yearly - Quooker_cost_yearly)\nyears_round = Math.round(years * 100) / 100\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ndt = aq.table({\n  years: Array.from({length: 200}, (_, i) =&gt; i + 1),\n  kettle: Array.from({length: 200}, (_, i) =&gt; {\n    return (i + 1) * kettle_cost_yearly + kettle_price\n  }),\n  Quooker: Array.from({length: 200}, (_, i) =&gt; {\n    return (i + 1) * Quooker_cost_yearly + Quooker_price\n  })\n})\n\n// Prepare the data in a format suitable for Plot.line\ndata = dt.objects().flatMap(({years, kettle, Quooker}) =&gt; [\n  {years, value: kettle, key: 'Kettle'},\n  {years, value: Quooker, key: 'Quooker'}\n]);\n\nPlot.plot({\n  y: {\n    grid: true,\n    label: 'Cumulative cost (Euro)'\n  },\n  x: {\n    grid: true,\n    label: 'Time (years)'\n  },\n  color: {\n    domain: [\"Kettle\", \"Quooker\"], legend: true, type: \"categorical\", range: [\"#1f77b4\", \"#ff7f0e\"]\n  },\n  marks: [\n    Plot.line(data, {x: 'years', y: 'value', stroke: 'key'})\n  ]})"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-much-energy-is-lost-by-each-system",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#how-much-energy-is-lost-by-each-system",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "How much energy is lost by each system?",
    "text": "How much energy is lost by each system?\n\n\n\n\n\n\n\n\n\nWaste Calculation\nKettle\nQuooker\n\n\n\n\nReason for waste\nDue to overfilling\nDue to heat loss\n\n\nkWh wasted per day\n kWh\n kWh\n\n\nPercentage of total energy\n%\n%\n\n\nCost of waste per year\n€ \n€"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-else-do-we-learn",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#what-else-do-we-learn",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "What else do we learn?",
    "text": "What else do we learn?\nAs the number of cups of tea increases, the Quooker becomes more cost-effective than the kettle, as the wasted energy is a fixed amount, while the wasted energy of the kettle increases linearly with the number of cups of tea, assuming the overfilling remains constant.\nReducing the amount by which we overfill the kettle decreases the energy loss and the cost of operating the kettle."
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#appendix",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#appendix",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Appendix",
    "text": "Appendix\nI have assumed that the water in the kettle cools completely between each boil. This would require 2 hours between each boil, assuming an ambient temperature of 20 degrees Celsius and a cooling constant of 0.02. The cooling constant would change based on the specific kettle/environment. See the code below for the simulation.\n\nimport matplotlib.pyplot as plt\nimport numpy as np\nfrom scipy.integrate import odeint\n\n# Define the function to compute derivative\ndef model(T, t):\n    T_ambient = 20  # Ambient temperature in degree Celsius\n    k = 0.02  # Cooling constant. This would change based on the specific kettle/environment\n    dTdt = -k * (T - T_ambient)\n    return dTdt\n\nT0 = 100  # Starting temperature of the water in degree Celsius\n\nt = np.linspace(0, 120, num=500)  # Time points in minutes. Here we simulate for 2 hours\n\n# Solve the ODE\nT = odeint(model, T0, t)\n\n# Plot the results\nplt.plot(t, T)\nplt.xlabel('Time (minutes)')\nplt.ylabel('Temperature (°C)')\nplt.title('Cooling of Boiled Water in a Kettle Over Time')\nplt.grid(True)\nplt.show()"
  },
  {
    "objectID": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#footnotes",
    "href": "posts/2023-07-04-hot-water-tap-efficiency/hot-water-tap-efficiency.html#footnotes",
    "title": "The Economics of Instant Hot Water: Quooker vs Kettle",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThe article is dynamic, so adjust the inputs and read it again from the top to see what the payoff period is in your case!↩︎\nIf for instance, you drink 25 cups of tea per day, the Quooker will pay for itself in 12.5 years.↩︎"
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "",
    "text": "In 2023, video content is king. The trouble is, videos are not always the most convenient format to search through for information, especially if you’re after a specific snippet of content. Scrubbing back and forth through a video isn’t heaps of fun.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts an automaton robot examining a spreadsheet\n\n\n\n\nThis week I wanted to condense the key points from an interesting video on AI and white collar jobs in Sweden. The podcast episode in which Magnus Lodefalk and Erik Engberg discuss the coming dissemination of AI through the economy, and its consequences for the labourt market, is super interesting and I highly reccomend it.\n\n\n\n\nWhat I wanted was to create a tool to grab the English auto translated transcript of the podcast and summarize it, highlighting the most important takeaways. With the help of OpenAI’s GPT-4, I was able to create the tool for the job in an afternoon.\n\n\nI’m aware that the afternoon spent building the tool certainly took longer than rewatching the video and taking notes to summarize myself. However, I enjoyed making the tool, and I can now use it for any similar task in the future. I also learned a lot about how to interact with OpenAI’s API, which is a valuable skill in itself.\nYou can have a look at my github repo here, and you are welcome to use the tool yourself if you have your own OpenAI API key. The instructions are in the readme. The remainder of this post describes the tool and some thoughts about what it means to make these kinds of tools in the age of such capable generative AI."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#introduction",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#introduction",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "",
    "text": "In 2023, video content is king. The trouble is, videos are not always the most convenient format to search through for information, especially if you’re after a specific snippet of content. Scrubbing back and forth through a video isn’t heaps of fun.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts an automaton robot examining a spreadsheet\n\n\n\n\nThis week I wanted to condense the key points from an interesting video on AI and white collar jobs in Sweden. The podcast episode in which Magnus Lodefalk and Erik Engberg discuss the coming dissemination of AI through the economy, and its consequences for the labourt market, is super interesting and I highly reccomend it.\n\n\n\n\nWhat I wanted was to create a tool to grab the English auto translated transcript of the podcast and summarize it, highlighting the most important takeaways. With the help of OpenAI’s GPT-4, I was able to create the tool for the job in an afternoon.\n\n\nI’m aware that the afternoon spent building the tool certainly took longer than rewatching the video and taking notes to summarize myself. However, I enjoyed making the tool, and I can now use it for any similar task in the future. I also learned a lot about how to interact with OpenAI’s API, which is a valuable skill in itself.\nYou can have a look at my github repo here, and you are welcome to use the tool yourself if you have your own OpenAI API key. The instructions are in the readme. The remainder of this post describes the tool and some thoughts about what it means to make these kinds of tools in the age of such capable generative AI."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#how-the-tool-works",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#how-the-tool-works",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "How the Tool Works",
    "text": "How the Tool Works\nThe YouTube Caption Summaries tool is a streamlined tool that condenses the key points from a Youtube video transcript. As an input, you provide a Youtube URL, and the tool returns a text file with the summary. It makes use of the youtube-transcript-api to grab the transcript, and the OpenAI API to summarize it, leveraging the GPT-3.5 turbo model.\nThis tool isn’t limited to English content. If you’re dealing with a non-English video, you can specify the original language, and the tool will automatically grab the auto generated English transcript before summarizing it.\n\n\n\n\ngraph TB\n  A[Input YouTube URL] --&gt; B[YouTube Data API: Get Transcript]\n  B --&gt; C[OpenAI API: Summarize Transcript]\n  C --&gt; D[Save Summary to Text File]\n\n\n\n\n\n\n\nAn Example\nTo illustrate, here’s an example of the tool in action. The video in question is a 10 minute talk by Timothée Parrique titled “Best of #BeyondGrowth 2023”. You can watch the video below.\n\n\nHere’s the summary produced by the tool:\nHere are the key points from Timothée Parrique’s speech:\n\nThe concept of “green growth” is deeply ingrained in environmental strategies like SDG 8, the Paris Agreement, and the European Green Deal.\nHowever, Parrique criticizes the concept, arguing that the notion of economic growth fully detached from nature is baseless and misleading.\nHe contends that truly sustainable economic growth requires absolute decoupling of production and consumption from all environmental pressures, done quickly and maintained over time.\nHe disputes the feasibility of green growth, using minor reductions in European countries’ greenhouse gas emissions as evidence.\nHe suggests that achieving a 55 percent reduction in emissions by 2030 necessitates significant GDP “de-growth”.\nHe warns that neglecting the ecological impact of consumption exacerbates the problem.\nHe introduces the idea of “de-growth” or a steady-state economy as a realistic strategy to reduce environmental stress and attain environmental targets, specifically in Europe.\nThe strategy aims at reducing production and consumption to prevent ecological overshoot and stabilize the economy within planetary limits without compromising living standards.\nHe advocates for high-income countries to decrease consumption to allow less affluent nations to sustainably grow and fulfill their needs.\nThis strategy’s goal is to guarantee everyone’s well-being within the boundaries of the planet.\nThe text criticizes the “coupling” narrative as dangerous, potentially inducing complacency by assuring people that everything is okay.\nParrique sees “green growth” as macroeconomic greenwashing, preventing essential radical changes.\nHe argues that precious time is being lost by making minor tweaks to the system instead of concentrating on the pressing need for transformation.\nParrique poses the question of whether the priority should be economic growth or nature preservation."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#performance-pros-and-cons",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#performance-pros-and-cons",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Performance: Pros and Cons",
    "text": "Performance: Pros and Cons\nThe summary above does a good job of capturing the main points that Parrique makes in his speech.\nOne issue is the quality of the data being fed into the summariser. The captioning process on Youtube is optional - with many uploaders foregoing any captions. The auto generated translations are not perfect either. There is a fantastic 99 % invisible epdisode explaining the history of closed captioning on Youtube, as well as the various improvements that have been made over the years. It certainly isn’t perfect, however, even if the transcription isn’t 100% accurate, it’s still a great starting point for a summary. When the transript is processed by the OpenAI API, the model is looking for the key points and can overcome transcription errors that are not crucial to the main message.\nOverall, I am pleased with the quality of the summaries, and in the future I might work on providing the title of the video and it’s description as additional context for the summarization model."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#journey-of-development",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#journey-of-development",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Journey of Development",
    "text": "Journey of Development\nThe development of this tool is a testament to the power of generative AI and a bit of curiosity. I did it in an afternoon, chatting with my virtual programming partner; GPT-4 offering up Python code based on my instructions.\nWhile I am not an expert python coder, I could have produced this tool from first principles a year ago (in the pre-generative pretrained model era). It could have required a couple days worth of work, familiarizing myself with the various APIs, putting together a python package structure.\nInstead, I could glue the different parts together in a couple of hours, and spend the rest of the time tweaking the code to get the best results."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#implications",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#implications",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Implications",
    "text": "Implications\nThis is the kind of project I would have used in a job interview to showcase my skills. Back in the day, being able to develop a tool like this would have been a testament to one’s coding skills. However, given that I didn’t code this from first principles, could I still do the same today? Is the product less impressive, knowing that a significant part of the coding was handled by an AI model?\nThe answer, I believe, is that the landscape of software tooling is changing. What’s important is not that the code was generated with AI assistance, but that the tool was developed effectively and solves a real problem. Coding from first principles will always have its place, but leveraging AI tools, just like any other tools, demonstrates practical problem-solving skills and adaptability. It’s a recognition that we are moving into an era where working in partnership with AI will be an essential part of many jobs, not just in the tech industry."
  },
  {
    "objectID": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#conclusion",
    "href": "posts/2023-05-23-youtube-caption-summaries/youtube-caption-summaries.html#conclusion",
    "title": "Building a YouTube Video Transcript Summarizer (with some help)",
    "section": "Conclusion",
    "text": "Conclusion\nSolving little problems with a bit of code and a really cheap API call is a lot of fun. I’m looking forward to seeing what other tools I can build with the OpenAI API."
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html",
    "title": "Electricity generation per capita",
    "section": "",
    "text": "🔥In this blog post, we will take a closer look at the evolution of electricity generation per capita across the world and examine how China has rapidly climbed above its neighbors in per capita terms📈. Through the use of data visualization📊, we will explore the decrease of electricity generation per capita in New Zealand over the past few decades and compare it to other countries in the region🌎. This analysis will provide a deeper understanding of the world’s energy landscape and the global energy market🌡️. So, let’s dive in and see how energy consumption has evolved over time!🔍"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#introduction",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#introduction",
    "title": "Electricity generation per capita",
    "section": "",
    "text": "🔥In this blog post, we will take a closer look at the evolution of electricity generation per capita across the world and examine how China has rapidly climbed above its neighbors in per capita terms📈. Through the use of data visualization📊, we will explore the decrease of electricity generation per capita in New Zealand over the past few decades and compare it to other countries in the region🌎. This analysis will provide a deeper understanding of the world’s energy landscape and the global energy market🌡️. So, let’s dive in and see how energy consumption has evolved over time!🔍"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#inspiration",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#inspiration",
    "title": "Electricity generation per capita",
    "section": "Inspiration",
    "text": "Inspiration\n🚀 I am inspired by the company Electricity Maps based in Copenhagen, who allow you to understand the electricity grid through data. They collect data from all around the world and compute the data in an hourly resolution, allowing you to gain granular insights accounting for the variance of the grid.💡 While we won’t use their data in this blog post, we will take a look at some great annual statistics compiled by the team at Our World in Data. This organization is dedicated to providing data-driven insights on global issues and their data on electricity generation per capita is a valuable resource for understanding how different countries are progressing in terms of energy production.📊"
  },
  {
    "objectID": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#visualization",
    "href": "posts/2023-01-29-electricity-generation-per-capita/electricity-generation-per-capita.html#visualization",
    "title": "Electricity generation per capita",
    "section": "Visualization",
    "text": "Visualization\n\n\n\n\n\n\n\n\n\n\nNow we can see that South Africa has climbed the rankings from 18th place in 2000 up to 7th in 2020. Interesting!\n\nWhat about the other continents?\nHave a look at each below, and then give my Shiny app a gander (TODO).\n\n\nAmericasAsiaEuropeOceania"
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html",
    "href": "posts/2021-07-06-students-speak/students-speak.html",
    "title": "Students Speak",
    "section": "",
    "text": "Hi Johan - here are some visualizations I have put together from your students’ diary entries. I think that they tell quite a nice story. I hope that some are useful. I’ve done them in black and white - I’m not sure where you want to publish them in the end. If you’d like some colour I can add it easily.\nI’ve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\n\n\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\n\n\n\nThe students’ diary entries have been ingested to form a dataset such that each row is one student’s observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\n\n\n\n\n\n\n\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\n\n\n\nWe start with a word cloud which shows the words used by the students in their diary entries.\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\n\n\n\n\n\nWord cloud\n\n\n\n\nWe can see that common positive words include “support”, “privileged”, “healthy”, “productive”, and “excited”. Common negative words are dominated by “virus”, followed by “difficult”, “struggling”, and “infected”.\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students’ diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\nTable @ref(tab:excluded) shows some common words in the students’ diary entries which are excluded from the wordcloud in Figure @ref(fig:wordcloud).\n\n\n\nCommon words excluded from wordcloud\n\n\nWord\nNumber of uses\n\n\n\n\nLockdown\n251\n\n\nPeople\n211\n\n\nDay\n183\n\n\nTime\n172\n\n\nFamily\n111\n\n\nFeel\n92\n\n\nSouth\n90\n\n\nHome\n80\n\n\nWorld\n78\n\n\nDays\n71\n\n\nAfrica\n61\n\n\nLife\n54\n\n\nOnline\n51\n\n\n19\n50\n\n\nFriends\n50\n\n\n\n\n\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure @ref(fig:image-grobs).\n\n\n\nConventional word cloud and comparison cloud\n\n\n\n\n\nFigure @ref(fig:sentiment) below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students’ diary entries in a particular week.\n\n\n\n\n\nEvolution of sentiment\n\n\n\n\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn Årup Nielsen in 2009-2011. An example of the scores assigned to words in the students’ diary entries is shown in Table @ref(tab:afinn) below.\n\n\n\nAFINN sentiment scores\n\n\nWord\nSentiment score\n\n\n\n\nBullshit\n-4\n\n\nCatastrophic\n-4\n\n\nPanic\n-3\n\n\nFake\n-3\n\n\nWorse\n-3\n\n\nFunny\n4\n\n\nFun\n4\n\n\nWonderful\n4\n\n\nThrilled\n5\n\n\n\n\n\n\n\n\nThe wordcloud in Figure @ref(fig:wordcloud) showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\nTable @ref(tab:words) below shows the words most specific to each week.\n\n\n\nWeek specific words calculated with weighted log odds\n\n\n\n\n\n\n\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\n\n\n\n\nAfrican leaders\nLockdown starts\nPrivilege\nEaster Sunday\nZoom\nClothing bank\n\n\nChurch\nVirus\nConspiracy theories\nPayment\nR500 billion\nLockdown restrictions\n\n\nAirports\nTownships\nAn obligation\nHot Cross buns\nSUN Learn\nLevel 5\n\n\nNSFAS\nCases recorded\nContinues to rise\nExtension\nEconomic stimulus\nLivelihoods\n\n\n\n\n\nNice! We can see that we capture some elements of the experience in each week of lockdown.\n\n\n\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students’ sentiment across the weeks.\n\n\n\n\n\nSentiment and week-words figure\n\n\n\n\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\n\n\n\nFigure @ref(fig:context) shows the evolution of the sentiment of the students’ diary entries beside the rising Covid-19 case numbers in South Africa.\n\n\n\n\n\nComparison figure\n\n\n\n\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students’ reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\n\n\n\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\nI think option two conveys the message clearly and without clutter."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#students-speak",
    "href": "posts/2021-07-06-students-speak/students-speak.html#students-speak",
    "title": "Students Speak",
    "section": "",
    "text": "Hi Johan - here are some visualizations I have put together from your students’ diary entries. I think that they tell quite a nice story. I hope that some are useful. I’ve done them in black and white - I’m not sure where you want to publish them in the end. If you’d like some colour I can add it easily.\nI’ve written up the process mostly so that I can remember. The visualizations are at the bottom of the post.\n\n\nThe Stellenbosch students of Economic History 281 were encouraged to keep a diary during the lockdown as the Covid-19 pandemic overtook the world in March 2020. This post is a short text analysis of the content of their diary entries.\n\n\n\nThe students’ diary entries have been ingested to form a dataset such that each row is one student’s observation on one day. Additional columns specify the date and the week of the log. There are 333 observations in total. Three examples are shown in the table below.\n\n\n\n\n\n\n\nThese data were supplemented to include the number of Covid-19 cases in South Africa, the number of deaths, and the number of tests performed. These may provide some context around the change in content of the diary entries over time.\n\n\n\nWe start with a word cloud which shows the words used by the students in their diary entries.\nThe size of the word is correlated to how frequently it is used. The sentiment of the word is scored with the bing sentiment lexicon, a general purpose English sentiment lexicon that categorizes words in a binary fashion, either positive or negative.\n\n\n\n\n\nWord cloud\n\n\n\n\nWe can see that common positive words include “support”, “privileged”, “healthy”, “productive”, and “excited”. Common negative words are dominated by “virus”, followed by “difficult”, “struggling”, and “infected”.\nThis is slightly more informative than a generic word cloud showing word frequency. However, it should be noted that the words must occur in both the students’ diary entries and the bing sentiment lexicon in order to be shown in the word cloud.\nTable @ref(tab:excluded) shows some common words in the students’ diary entries which are excluded from the wordcloud in Figure @ref(fig:wordcloud).\n\n\n\nCommon words excluded from wordcloud\n\n\nWord\nNumber of uses\n\n\n\n\nLockdown\n251\n\n\nPeople\n211\n\n\nDay\n183\n\n\nTime\n172\n\n\nFamily\n111\n\n\nFeel\n92\n\n\nSouth\n90\n\n\nHome\n80\n\n\nWorld\n78\n\n\nDays\n71\n\n\nAfrica\n61\n\n\nLife\n54\n\n\nOnline\n51\n\n\n19\n50\n\n\nFriends\n50\n\n\n\n\n\nWe can also include a conventional word cloud beside the comparison cloud, and shown in Figure @ref(fig:image-grobs).\n\n\n\nConventional word cloud and comparison cloud\n\n\n\n\n\nFigure @ref(fig:sentiment) below shows the change in sentiment of the student responses over the course of the lockdown. It requires some explanation: the words used by the students are grouped by week, scored according to a sentiment lexicon, the score is averaged across the week. The points on the graph represent the average sentiment of the students’ diary entries in a particular week.\n\n\n\n\n\nEvolution of sentiment\n\n\n\n\nWe can see that at the outset, sentiment is poor, this improves, and then drops dramatically at the end of the period. It is noteworthy that the average sentiment is negative for the entirety of the period, highlighted by the dotted line at zero.\nThis can be explained by the choice of sentiment lexicon used to score the words. The AFINN-111 dataset is a lexicon of English words rated for valence with an integer between minus five and plus five. The words were manually labelled by Finn Årup Nielsen in 2009-2011. An example of the scores assigned to words in the students’ diary entries is shown in Table @ref(tab:afinn) below.\n\n\n\nAFINN sentiment scores\n\n\nWord\nSentiment score\n\n\n\n\nBullshit\n-4\n\n\nCatastrophic\n-4\n\n\nPanic\n-3\n\n\nFake\n-3\n\n\nWorse\n-3\n\n\nFunny\n4\n\n\nFun\n4\n\n\nWonderful\n4\n\n\nThrilled\n5\n\n\n\n\n\n\n\n\nThe wordcloud in Figure @ref(fig:wordcloud) showed the most common words. What if we want to see the words that are most specific to each week of the diary entries? We can use the tidylo package that provides the weighted log odds ratio for each word across the weeks of diary entries. This provides a quantification of how specific each word is to the week that it is used in. For more information see Fightin’ Words: Lexical Feature Selection and Evaluation for Identifying the Content of Political Conflict by Monroe, Colaresi, and Quinn (2008).\nTable @ref(tab:words) below shows the words most specific to each week.\n\n\n\nWeek specific words calculated with weighted log odds\n\n\n\n\n\n\n\n\n\n\nWeek 1\nWeek 2\nWeek 3\nWeek 4\nWeek 5\nWeek 6\n\n\n\n\nAfrican leaders\nLockdown starts\nPrivilege\nEaster Sunday\nZoom\nClothing bank\n\n\nChurch\nVirus\nConspiracy theories\nPayment\nR500 billion\nLockdown restrictions\n\n\nAirports\nTownships\nAn obligation\nHot Cross buns\nSUN Learn\nLevel 5\n\n\nNSFAS\nCases recorded\nContinues to rise\nExtension\nEconomic stimulus\nLivelihoods\n\n\n\n\n\nNice! We can see that we capture some elements of the experience in each week of lockdown.\n\n\n\nThis figure superimposes the week-specific words above the line graph that shows the evolution of the students’ sentiment across the weeks.\n\n\n\n\n\nSentiment and week-words figure\n\n\n\n\nI think it captures a bit of the experience - at the outset there was anxiety about the lockdown, difficulties with internet access and a worry about the rise in cases. This was followed by conspiracy theories and discussions of obligation and privilege. The collective mood improved toward Easter, and was further buoyed by the announcement of a large stimulus package by the government. Finally there was exasperation about the state of employment and livelihoods.\n\n\n\nFigure @ref(fig:context) shows the evolution of the sentiment of the students’ diary entries beside the rising Covid-19 case numbers in South Africa.\n\n\n\n\n\nComparison figure\n\n\n\n\nIt is difficult to conclude about a relationship between the number of cases and the sentiment of the students’ reflections. While there appears to be a relationship between average sentiment and number of tests at the outset of the lockdown, I think this is statistical noise rather than some sort of correlation.\n\n\n\nThe purpose of this selection of figures is to emphasize that the diary entries were recorded at the outset of the pandemic in South Africa. The number of cases was relatively low compared to the steep increase in cases which followed in winter of 2020.\nThe figures below compare the period of diary entries to the number of cases and deaths in the first year of the pandemic.\nI think option two conveys the message clearly and without clutter."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-1",
    "title": "Students Speak",
    "section": "Option 1",
    "text": "Option 1\nHere we have a two panel plot of the Covid-19 statistics and number of diary entries recorded by the students."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-2",
    "title": "Students Speak",
    "section": "Option 2",
    "text": "Option 2\nNext we have a single panel with the period of diary entries superimposed on the Covid-19 statistics."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-3",
    "title": "Students Speak",
    "section": "Option 3",
    "text": "Option 3\nAlternatively we can annotate a thick line to show where the diary entries occur."
  },
  {
    "objectID": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "href": "posts/2021-07-06-students-speak/students-speak.html#option-4",
    "title": "Students Speak",
    "section": "Option 4",
    "text": "Option 4\nAlternatively we can have a legend variant of option 2."
  },
  {
    "objectID": "content/teaching.html",
    "href": "content/teaching.html",
    "title": "Teaching",
    "section": "",
    "text": "Check out my site for a set of Stata labs I taught in the Spring of 2023"
  },
  {
    "objectID": "content/teaching.html#regional-development-and-growth",
    "href": "content/teaching.html#regional-development-and-growth",
    "title": "Teaching",
    "section": "",
    "text": "Check out my site for a set of Stata labs I taught in the Spring of 2023"
  },
  {
    "objectID": "content/teaching.html#screencasts",
    "href": "content/teaching.html#screencasts",
    "title": "Teaching",
    "section": "Screencasts",
    "text": "Screencasts\nHave a look at a set of screencasts I am working on for learning R."
  },
  {
    "objectID": "content/teaching.html#data-visualization",
    "href": "content/teaching.html#data-visualization",
    "title": "Teaching",
    "section": "Data visualization",
    "text": "Data visualization\nI am working on adapting a course from Claus Wilke for presentation in Lund."
  },
  {
    "objectID": "content/research/Wired-for-success-abstract.html",
    "href": "content/research/Wired-for-success-abstract.html",
    "title": "Wired for Success - Sweden’s Electrical Innovators in the Age of Industrial Transformation",
    "section": "",
    "text": "This paper delves into the intersection of technological innovation and economic leveling in Sweden during the first half of the 20th century, with a particular focus on the electrification revolution. As electricity transitioned from a luxury to an essential component of production and lifestyle, it not only improved the marginal product of labor, but also led to more equitable income distribution and job creation. Central to this transition were the individuals who championed and adopted this new technology.\nThe purpose of this study is to meticulously trace the career trajectories of these innovators and early adopters, by exploring their personal backgrounds, professional connections, and global linkages. Leveraging an unconventional data source, the Vem är vem? (Who is who?) series, this paper curates a comprehensive database of biographical information on approximately 70,000 notable Swedes from 1945-1950 and 1962-1968. We make use of OpenAI’s GPT-3.5 turbo API to unabbreviate the biographical text (from mid-20th-century Swedish), and structure it into a format that is amenable to network analysis (the Schema.org/Person schema). The resulting dataset contains information on the individuals education, employment, and social connections, as well as their geographical location and industry affiliation.\nBy systematically analyzing this rich data, we reconstruct a multi-layered network of individuals, their geographical loci, the companies they were associated with, and the industries they were part of. This innovative approach elucidates the dynamic interplay between technological progress, economic development, and social mobility, while shedding light on the critical role played by human agents in technological adoption and diffusion. The study enhances our understanding of the historical roots of Sweden’s economic leveling and can provide insights into the complex processes underlying similar technological transitions globally."
  },
  {
    "objectID": "content/research/Wired-for-success-abstract.html#abstract",
    "href": "content/research/Wired-for-success-abstract.html#abstract",
    "title": "Wired for Success - Sweden’s Electrical Innovators in the Age of Industrial Transformation",
    "section": "",
    "text": "This paper delves into the intersection of technological innovation and economic leveling in Sweden during the first half of the 20th century, with a particular focus on the electrification revolution. As electricity transitioned from a luxury to an essential component of production and lifestyle, it not only improved the marginal product of labor, but also led to more equitable income distribution and job creation. Central to this transition were the individuals who championed and adopted this new technology.\nThe purpose of this study is to meticulously trace the career trajectories of these innovators and early adopters, by exploring their personal backgrounds, professional connections, and global linkages. Leveraging an unconventional data source, the Vem är vem? (Who is who?) series, this paper curates a comprehensive database of biographical information on approximately 70,000 notable Swedes from 1945-1950 and 1962-1968. We make use of OpenAI’s GPT-3.5 turbo API to unabbreviate the biographical text (from mid-20th-century Swedish), and structure it into a format that is amenable to network analysis (the Schema.org/Person schema). The resulting dataset contains information on the individuals education, employment, and social connections, as well as their geographical location and industry affiliation.\nBy systematically analyzing this rich data, we reconstruct a multi-layered network of individuals, their geographical loci, the companies they were associated with, and the industries they were part of. This innovative approach elucidates the dynamic interplay between technological progress, economic development, and social mobility, while shedding light on the critical role played by human agents in technological adoption and diffusion. The study enhances our understanding of the historical roots of Sweden’s economic leveling and can provide insights into the complex processes underlying similar technological transitions globally."
  },
  {
    "objectID": "content/research/Warwick-poster.html",
    "href": "content/research/Warwick-poster.html",
    "title": "Warwick Poster",
    "section": "",
    "text": "I am delighted to present a poster at the EHS conference in Warwick this year!\nYou can have have a look at the poster on this page below. I also include the slides for a presentation of the paper on the same theme which I am writing along with my two supervisors, Kerstin Enflo and Jakob Molinder."
  },
  {
    "objectID": "content/publications.html",
    "href": "content/publications.html",
    "title": "Publications",
    "section": "",
    "text": "I have one paper that I’m proud to have published in World Development with my master’s supervisor Johan Fourie. Have a look at it below."
  },
  {
    "objectID": "content/data.html",
    "href": "content/data.html",
    "title": "Academic Data",
    "section": "",
    "text": "I have done a fair bit of web scraping to get data into usable formats.\n\n\n\n\n\n\n  \n    \n      Available Data\n    \n    \n      From my GitHub repos\n    \n  \n  \n    \n      Title and link\n      Description\n    \n  \n  \n    \nCar Scraper ZA\n      \nAn automated scraper that gets adverts for vehicles off of Gumtree in South Africa every day and stores them on this GitHub repo. Have a look at my Shiny App to understand the kind of data collected.\n\n    \nBicycle Advert Scraper\n      \nSimilar to the car scraper, this repo autmotically scrapes adverts for bicycles from Bikehub and Gumtree.\n\n    \nSwedish Job adverts\n      \nThis repo contains a one percent sample of the enriched jobtech data from jobtechdev.se. They provide a one percent sample of all jobs advertised in Sweden from 2016 to 2022Q2 to help you get to grips with the data. I simply put it into excel format from json\n\n    \nSwedish Agriculture and Livestock\n      \nThis repo is for the digitization of the SCB reports on Agriculture and Animal Husbandry. They span from 1865 to 1911. You can have a look at my Shiny app to get a better understanding\n\n    \nSwedish patent data\n      \nI have scraped Google Patents for 10,000 patents that were registered in Sweden, and collected PDF data from The Swedish Patent Registration Authority for further analysis.\n\n    \nHot Jobs in Sweden from LinkedIn\n      \nThis repo has a set of data from LinkedIn's collaboration with the World Bank on talent migration. I've written a short report on the data here"
  },
  {
    "objectID": "content/data.html#available-data",
    "href": "content/data.html#available-data",
    "title": "Academic Data",
    "section": "",
    "text": "I have done a fair bit of web scraping to get data into usable formats.\n\n\n\n\n\n\n  \n    \n      Available Data\n    \n    \n      From my GitHub repos\n    \n  \n  \n    \n      Title and link\n      Description\n    \n  \n  \n    \nCar Scraper ZA\n      \nAn automated scraper that gets adverts for vehicles off of Gumtree in South Africa every day and stores them on this GitHub repo. Have a look at my Shiny App to understand the kind of data collected.\n\n    \nBicycle Advert Scraper\n      \nSimilar to the car scraper, this repo autmotically scrapes adverts for bicycles from Bikehub and Gumtree.\n\n    \nSwedish Job adverts\n      \nThis repo contains a one percent sample of the enriched jobtech data from jobtechdev.se. They provide a one percent sample of all jobs advertised in Sweden from 2016 to 2022Q2 to help you get to grips with the data. I simply put it into excel format from json\n\n    \nSwedish Agriculture and Livestock\n      \nThis repo is for the digitization of the SCB reports on Agriculture and Animal Husbandry. They span from 1865 to 1911. You can have a look at my Shiny app to get a better understanding\n\n    \nSwedish patent data\n      \nI have scraped Google Patents for 10,000 patents that were registered in Sweden, and collected PDF data from The Swedish Patent Registration Authority for further analysis.\n\n    \nHot Jobs in Sweden from LinkedIn\n      \nThis repo has a set of data from LinkedIn's collaboration with the World Bank on talent migration. I've written a short report on the data here"
  },
  {
    "objectID": "content/data.html#academic-data-processing",
    "href": "content/data.html#academic-data-processing",
    "title": "Academic Data",
    "section": "Academic data processing",
    "text": "Academic data processing\nI have been happy to help a number of my colleagues with scraping data from public websites or processing geographic data. I link these GitHub repositories here, in case you’re looking for inspiration on how to structure a scraping project, or perhaps want to access data on soil suitability or Europe’s urban populations.\n\nScraping a genealogy wiki of Sweden’s noble families\nHere I scraped and structured a database of more than 120,000 individuals belonging to various branches of Sweden’s nobility.\n\n\n\n\n\n\n\nScraping tax registers from the Stockholm City Archive\nHere I scraped an index of records from 1800 to 1880 for residents of Sweden’s most populous city, Stockholm.\n\n\n\n\n\n\n\nSoil suitability calculation\nIn this project, I helped acquire data on soil suitability for wheat cultivation across Europe, and aggregate the raster spatial data to NUTS 2 and NUT 3 regions for use as control variables in a study of persistence.\n\n\n\n\n\n\n\nEuropean population aggregations\nIn this project, I helped aggregate the population from cities across Europe to NUTS2 and NUTS3 regions, to show the growth of urban populations across time."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Jonathan Jayes",
    "section": "",
    "text": "“I enjoy some of the old and I enjoy the new. And if I can find a balance between it, that’s where I find my satisfaction.”\n– Anderson .Paak - The Dreamer (feat. Talib Kweli & Timan Family Choir)"
  },
  {
    "objectID": "about.html#bio",
    "href": "about.html#bio",
    "title": "Jonathan Jayes",
    "section": "",
    "text": "“I enjoy some of the old and I enjoy the new. And if I can find a balance between it, that’s where I find my satisfaction.”\n– Anderson .Paak - The Dreamer (feat. Talib Kweli & Timan Family Choir)"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Jonathan Jayes",
    "section": "Education",
    "text": "Education\n\n\n\n\n\n\n\n\n\n\n\n\nLund University\nPhD, Economic History\n 2023-now \n Under prof. Kerstin Enflo \n\n\n\nLund University\nMaster's degree, Economics\n 2021 \n Exchange semester \n\n\n\nStellenbosch University\nMaster's degree, Economics\n 2020 \n Grade: cum laude \n\n\n\nUniversity of Cape Town\nHonours degree, Economics\n 2018 \n Grade: 2:1 \n\n\n\nUniversity of Cape Town\nBachelor's degree, Economics and Economic History\n 2015-2017 \n Grade: 2:1"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Jonathan Jayes",
    "section": "Experience",
    "text": "Experience\nCodera Analytics | Data Science Intern at Codera Analytics| December 2023 - March 2023\nFathom Data | Data Science Intern at Fathom Data| Feb 2022 - April 2022\nLaboratory for the Economics of Africa’s Past | Researcher under Prof Johan Fourie| January 2020 - July 2021\nFTI Consulting | Junior Economist in Economic and Financial Consulting Practice | March 2019 - January 2020"
  },
  {
    "objectID": "content/consulting.html",
    "href": "content/consulting.html",
    "title": "Consulting",
    "section": "",
    "text": "📊👨💻🎨💡🤝📈📚\nI am keen to work as consultant in data analysis and visualization, as well as web design.\nAs a data analyst with experience in both medical and historical data, I offer expertise in tidying, analyzing, and reporting on complex data sets. I specialize in crafting engaging visualizations that help organizations understand their data and make informed decisions.\nMy experience includes working with a not-for-profit medical office, where I was responsible for analyzing and reporting on a complex database of medical records. Additionally, I have experience crafting more than 30 data visualizations for Our Long Walk to Economic Freedom by Prof. Johan Fourie, a non-fiction book about the economic history of South Africa, which has subsequently been turned into a textbook.\nFurthermore, I have designed, built, and maintained the website for the 15th Swedish Economic History Meeting for the Swedish Economic History Association.\nIf you’re looking for a consultant to help you make sense of your data, please don’t hesitate to get in touch through the links on the about me page. I’d be happy to discuss your needs and see if we’re a good fit."
  },
  {
    "objectID": "content/phd-planning.html",
    "href": "content/phd-planning.html",
    "title": "PhD planning",
    "section": "",
    "text": "I need to commit to more structure and deadlines in order to complete my PhD on time.\nMy PhD consists of four papers and coursework."
  },
  {
    "objectID": "content/phd-planning.html#papers",
    "href": "content/phd-planning.html#papers",
    "title": "PhD planning",
    "section": "Papers",
    "text": "Papers\n\nShocked by Electricity: The impact of new technology on individuals and local labour markets. Co authored with Kerstin Enflo and Jakob Molinder\nWired for Success: Sweden’s Electrical Innovators in the Age of Industrial Transformation\n\nHere is a chart showing the remaining time in my PhD and what is left to do.\n\n\n\n\n\ngantt\n    dateFormat  YYYY-MM-DD\n    title       Paper Timeline\n    excludes    weekends\n\n    section Paper Drafts\n    Complete draft of first paper          :active, 2023-09-11, 19d\n    Complete draft of second paper         :        2023-09-30, 2023-12-16\n    Complete draft of third paper          :        2024-01-17, 2024-04-15\n    Complete draft of fourth paper         :        2024-08-16, 2024-12-16\n    \n    section Paper Revisions\n    Revise second paper                    :        2024-04-16, 2024-05-16\n    Revise third paper                     :        2024-05-17, 2024-06-15    \n    Revise fourth paper                    :        2025-01-24, 2025-03-15    \n\n\n    section Seminars & Submission\n    Midway seminar                         :crit,     2024-01-24, 1d\n    Final seminar                          :crit,     2025-04-01, 1d\n    Intense working on the Kappa           :          2025-05-01, 2025-06-30\n    Submit Ph.D. for typesetting           :milestone,2025-07-01, 0d\n\n    section Defense\n    Defend thesis                          :milestone, 2025-10-01, 0d\n\n\n\n\n\n\n\n\nUpcoming deadlines and feedback\nPresent Wired for Success data processing and initial results at Swedish Economic History Meeting on 29th of September.\nComplete draft of Shocked by Electricity on 2 October 2023."
  },
  {
    "objectID": "content/phd-planning.html#coursework",
    "href": "content/phd-planning.html#coursework",
    "title": "PhD planning",
    "section": "Coursework",
    "text": "Coursework\nMandatory courses (52.5 credit points):\n\nEHEH001 Designing a PhD dissertation: Completed - 7.5 credits.\nEHEH002 Foundations of Economic History: 1/3 papers done . Assuming linear progression and that each paper is worth an equal amount of credits, I’ve completed 1/3 of 15 credits = 5 credits.\nEEH002F research seminars: Ongoing . 0 credits for the purpose of this calculation.\nPhilosophy of Science: Not chosen yet - 0 credits.\nResearch Ethics: Not chosen yet - 0 credits.\nQuantitative course (SDU Summer School): Complete - 7.5 credits.\nQualitative Course (Archives, Primary Sources, Dataset - EEH033F): Complete - 4.5 credits.\n\nTotal for Mandatory : 7.5 + 5 + 0 + 0 + 0 + 7.5 + 4.5 = 24.5 credits\nOptional courses (22.5 credit points):\n1. EEH005F Economic Growth over Time and Space: Completed - 7.5 credits.\nTotal for Optional : 7.5 credits\nIn summary: - I’ve completed 24.5 credits of the mandatory coursework. - I’ve completed 7.5 credits of the optional coursework.\nThis means I’ve completed a total of 32 credits out of the 75 credits I need."
  },
  {
    "objectID": "content/phd-planning.html#conferences-and-presentations",
    "href": "content/phd-planning.html#conferences-and-presentations",
    "title": "PhD planning",
    "section": "Conferences and presentations",
    "text": "Conferences and presentations\nEHS Conference 2023: Warwick\nEHES Conference 2023: Vienna\nSEHM 2023: Lund"
  },
  {
    "objectID": "content/research.html",
    "href": "content/research.html",
    "title": "Interlude One",
    "section": "",
    "text": "Wired for Success - Sweden’s Electrical Innovators in the Age of Industrial Transformation\n\n\n\n\n\n\n\n\n\n\n\n\nJun 7, 2023\n\n\n\n\n\n\n  \n\n\n\n\nRatio Seminar: Hur påverkar AI-utvecklingen jobben?\n\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWired for Success: Sweden’s Electrical Innovators in the Age of Industrial Transformation\n\n\n\n\n\n\n\n\n\n\n\n\nMay 15, 2023\n\n\n\n\n\n\n  \n\n\n\n\nWarwick Poster\n\n\n\n\n\n\n\n\n\n\n\n\nMar 31, 2023\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "content/research/Ratio-seminar.html",
    "href": "content/research/Ratio-seminar.html",
    "title": "Ratio Seminar: Hur påverkar AI-utvecklingen jobben?",
    "section": "",
    "text": "I’m thrilled to present a summary of my research on the impact of electrification on the labour market at the Ratio seminar on May 23rd 2023. The seminar will look at the impact of AI on the labour market, and I am providing some context on a previous technological revolution. The prompt for the seminar is as follows:\nNu går det fort. Utvecklingen av AI rusar fram och med det dess effekter på arbetsmarknad och övrigt samhälle, och det går fortare än något tidigare. Vad betyder det? Vilka yrken kommer att påverkas mest – respektive minst av AI-utvecklingen? Och vad krävs för att de samhälleliga och socioekonomiska följderna av framtidens AI ska bli bra? Vad kan vi lära av historien?\nHave a look at my slides below."
  },
  {
    "objectID": "content/research/Wired-for-success.html",
    "href": "content/research/Wired-for-success.html",
    "title": "Wired for Success: Sweden’s Electrical Innovators in the Age of Industrial Transformation",
    "section": "",
    "text": "I am working on a paper about the individuals who helped usher in an age of great technical progress and economic levelling in Sweden, by making use of new technology associated with electrification.\nIn the first half of the 20th century, electricity went from a luxury to an essential input into the production process. This new technology was used to power machines, light homes, and heat buildings. It was also used to power the new communications technology of the telephone and radio.\nThis new technology raised the marginal product of labour, and put workers in a better bargaining position to argue for higher wages and better working conditions. It resulted in higher incomes, a more equal income distribution, and the creation of new kinds of jobs in areas that electrified first.\nWhat I seek to do in this paper is trace the career paths of innovators and early adopters of electrification technology in Sweden. I want to know who they were, where they came from, and what they did. I want to know how they were connected to each other, and how they were connected to the rest of the world.\nTo accomplish this, I make use of a fun data source, an archive of biographical information from a series of books called Vem är vem? (Who is who?). These books were published in two editions in Sweden between 1945-1950 and 1962-1968, and contain biographical information about prominent Swedes. Eight of the ten volumes have been digitized and are available on the Projekt Runeberg website.\nI have scraped the biographical information from these books, and used it to create a database of biographical information about nearly 70,000 individuals. I plan to use this database to create a network of individuals, and to create a network of the places they were born, lived, travelled and worked. I will also use the database to create a network of the companies they worked for, and the industries they worked in.\nIt is still a work in progress, but you can see some slides about the project which I was glad to present at the CBS Strategy and Innovation Graduate Seminar in May 2023."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Interlude One",
    "section": "",
    "text": "A modern question and answer site: SCB Users Group\n\n\n\n\n\nA post about creating a question and answer site for users of Statistics Sweden data\n\n\n\n\n\n\nNov 7, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nGeocoding: two ways\n\n\n\n\n\nA post about geocoding place names using R and python to access the OSM and Google Maps geocoding APIs.\n\n\n\n\n\n\nOct 2, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nTransforming National Budget Documents into Queryable Knowledge\n\n\n\n\n\nA post about automating the data retrieval process from PDF documents using LangChain and the OpenAI API.\n\n\n\n\n\n\nJul 5, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nThe Economics of Instant Hot Water: Quooker vs Kettle\n\n\n\n\n\nHow long will it take for an instant hot water tap to save some money?\n\n\n\n\n\n\nJul 4, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nBuilding a YouTube Video Transcript Summarizer (with some help)\n\n\n\n\n\nA post on what it means to be a data scientist in the age of such capable generative AI.\n\n\n\n\n\n\nMay 23, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT as translator and text wizard\n\n\n\n\n\nA post showing how ChatGPT can be used to create structred data from text.\n\n\n\n\n\n\nMar 10, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nMap projections in R\n\n\n\n\n\nA post on how to change projections for mapping and save out the new shapefiles.\n\n\n\n\n\n\nJan 31, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nElectricity generation per capita\n\n\n\n\n\nA post on electricity generation per capita across the world since 2000.\n\n\n\n\n\n\nJan 30, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nOur World In Data choropleth\n\n\n\n\n\nA post on how recreate the Our World In Data characteristic choropleth in R.\n\n\n\n\n\n\nJan 29, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT cooks me dinner\n\n\n\n\n\n🍲 Master the kitchen like a pro with the help of ChatGPT! 🍴 I tried it out and the results will blow your mind! 🤯 Find out what happened when I asked a language model for recipe ideas and cooking instructions. 😲 You won’t believe how delicious my Gammon and Vegetable Stew with Mint turned out! 🍽️ Don’t miss out on this must-read blog post! 👀\n\n\n\n\n\n\nJan 20, 2023\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nChatGPT fills in for me as a blog post author\n\n\n\n\n\nI ask ChatGPT to write a blog post in the same manner as one I wrote last week. The results will amaze you!\n\n\n\n\n\n\nDec 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nCoefficient plots in ggplot\n\n\n\n\n\nDrawing coefficient plots in R and ggplot.\n\n\n\n\n\n\nDec 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nLinkedIn style tables for personal websites\n\n\n\n\n\nA post on how to us the {gt} table package to make tables for the about page of a pesonal website or blog.\n\n\n\n\n\n\nMay 20, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nProgrammatically generate tabs in Rmarkdown\n\n\n\n\n\nA walk-though for writing code that writes it’s own code.\n\n\n\n\n\n\nMar 4, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nFlavours of cocoa\n\n\n\n\n\nAn exploration of chocolate bar reviews\n\n\n\n\n\n\nFeb 15, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nR vs Python? R & Python!\n\n\n\n\n\nMining the twitter archive of Fathom Data\n\n\n\n\n\n\nJan 17, 2022\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nStudents Speak\n\n\n\n\n\nText analysis of students’ diary entries during the Covid-19 lockdown in South Africa\n\n\n\n\n\n\nJul 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nResources for Learning R and Mastering ggplot\n\n\n\n\n\nA collection of videos and resources to get you started using R and the tidyverse for exploratory data analysis and visualization.\n\n\n\n\n\n\nMay 21, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nMapping Sweden’s Regional GDP\n\n\n\n\n\nVisualizing the relationship between population and regional GDP share in 1571, based on data from Enflo and Missiaia (2018).\n\n\n\n\n\n\nMay 6, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nShipping and Scraping - Part 1 in a Series on Shipping\n\n\n\n\n\nIn this post I walk through scraping data on cargo ships from Wikipedia as part of a series on shipping. I make use of R, the rvest package for webscraping and the SelectorGadget tool for CSS selection.\n\n\n\n\n\n\nApr 8, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nSwedish trade in goods and exchange students\n\n\n\n\n\nI wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund Univesity from there, this semester.\n\n\n\n\n\n\nFeb 20, 2021\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nSouth African Car Colour Exploration\n\n\n\n\n\nIn this document I walk through creating a stacked area chart showing the evolution of car colour over time in South Africa. The data comes from nearly 45,000 adverts for cars that I scraped from Gumtree.\n\n\n\n\n\n\nNov 24, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nCar Price Comparison Tool\n\n\n\n\n\nThis web app helps South Africans gauge the median price for a second hand car. You can compare price by odometer reading, make and model, and province. Ad links are included in a table beside the output.\n\n\n\n\n\n\nOct 10, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nRecreating Our World In Data’s Beautiful Transistor Count Graphic\n\n\n\n\n\nScraping data from Wikipedia and recreating OWID’s figure with ggplot2.\n\n\n\n\n\n\nApr 8, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\n  \n\n\n\n\nWelcome to Interlude One\n\n\n\n\n\n“Like all good fruit, the balance of life is in the ripe and ruin”\n\n\n\n\n\n\nMar 13, 2020\n\n\nJonathan Jayes\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html",
    "title": "Our World In Data choropleth",
    "section": "",
    "text": "I really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, “make progress against the world’s largest problems”.\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser last year, included below.\n\n\n\n\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#purpose",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#purpose",
    "title": "Our World In Data choropleth",
    "section": "",
    "text": "I really look up to Max Roser and the team at Our World in Data. They have some of the best short form articles condensing a wealth of academic literature to, in their words, “make progress against the world’s largest problems”.\nThe mission is summed up well in a lecture given at Stellenbosch University by Max Roser last year, included below.\n\n\n\n\nIn this tutorial I want to walk through recreating one of their classic chart types in R, the world map choropleth with an overlayed line graph for each coutnry. A typical example shown below."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#finished-product",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#finished-product",
    "title": "Our World In Data choropleth",
    "section": "Finished product",
    "text": "Finished product\nHere is the finished product that we will create in the tutorial"
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#context",
    "title": "Our World In Data choropleth",
    "section": "Context",
    "text": "Context\nThere is a lot of information about the OWID grapher tool. You can have a look at their github repo and an older reddit AMA if you are interested. It’s a custom system that allows for systematic changes across their website, drawing on data from a central database."
  },
  {
    "objectID": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "href": "posts/2022-10-17-our-world-in-data-choropleth/our-world-in-data-choropleth.html#components",
    "title": "Our World In Data choropleth",
    "section": "Components",
    "text": "Components\nWhat are the parts I want to recreate? The map has:\n\na base map, where the colour fill of each country indicates it’s position in a specific measure in a particular year.\na simple line chart that appears when you hover over a country, showing how the measure has changed within a country over time.\na clear legend\na note specifying the source of the data\n\nI walk through creating each of these below.\n\nThe world map\nThe base map is sourced from the maps package. I add a three letter country code from the english name of the country using the countrycode package and filter out Antarctica, Greenland and the French Southern and Antarctic Lands.\n\n\nCode\n# preamble\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n# load map\nmap &lt;- st_as_sf(maps::map(database=\"world\", plot = FALSE, fill = TRUE))\n\n# create code to match coutnry to data with\nlibrary(countrycode)\nmap &lt;- map %&gt;% \n  mutate(code = countrycode(ID, \"country.name\", \"iso3c\"))\n\n# remove clutter from map\ncountry_to_remove &lt;- c(\n  'Antarctica','Greenland', 'French Southern and Antarctic Lands'\n)\n\nmap &lt;- map %&gt;% \n  filter(!ID %in% country_to_remove)\n\n\nThe base map is projected with the Web Mercator or WGS 84 projection, the same one Google Maps uses.\n\n\nCode\nmap %&gt;% \n  ggplot() +\n  geom_sf()\n\n\n\n\n\n\n\nData\nI source the data from the Our World in Data website (the WHO collects the data and the World Bank distributes it). We read in the data as a CSV file, and tidy up the column names so that they are in snake case with the clean_names() command from the very helpful janitor package.\n\n\nCode\ndf &lt;- read.csv(\"data/share-of-adults-who-smoke.csv\")\n\ndf &lt;- df %&gt;% \n  as_tibble() %&gt;% \n  dplyr::rename(value = Prevalence.of.current.tobacco.use....of.adults.) %&gt;% \n  janitor::clean_names()\n\n\nNext we remove the summary groups which we cannot show on the map, including the World Bank country income groupings.\n\n\nCode\ndf %&gt;% \n  filter(!code %in% map$code) %&gt;% \n  distinct(entity)\n\n\n# A tibble: 16 × 1\n   entity                      \n   &lt;chr&gt;                       \n 1 East Asia and Pacific       \n 2 Europe and Central Asia     \n 3 European Union              \n 4 High income                 \n 5 Latin America and Caribbean \n 6 Low and middle income       \n 7 Low income                  \n 8 Lower middle income         \n 9 Middle East and North Africa\n10 Middle income               \n11 North America               \n12 South Asia                  \n13 Sub-Saharan Africa          \n14 Tuvalu                      \n15 Upper middle income         \n16 World                       \n\n\nCode\ndf &lt;- df %&gt;% \n  filter(code %in% map$code)\n\n\n\n\nCreate a colour palette\nWhat we want to do is use the scale_color_viridis_c() palette. We have to map it to the min and max of the values in our dataset for smoking so that we get a nice mapping across the colour palette.\n\n\nCode\ndf %&gt;%\n  summarise(\n    min = min(value),\n    max = max(value)\n  )\n\n\n# A tibble: 1 × 2\n    min   max\n  &lt;dbl&gt; &lt;dbl&gt;\n1   3.5  68.5\n\n\nCode\nlibrary(viridisLite) \n\nvir_10 &lt;- viridis(n = 10)\n\nsmoking_hex &lt;- scales::gradient_n_pal(\n  colours = vir_10,\n  values = seq(0, 50, by = 5)\n)\n\nscale_smoking &lt;- function() {\n  scale_color_gradientn(\n    colours = vir_10,\n    values = seq(0, 50, by = 5) / 50,\n    limits = c(0, 68.5),\n    name = \"value\"\n  )\n}\n\n\n\n\nHow to plot the line graph?\nThe line graph that appears when you hover over OWID map is very simple. It has just the starting value on the y-axis, and the first and last years on the x-axis, and a line that changes colour in accordance with the scale of the choropleth. The hover window which contains the graph also shows the country name, and the value of the measure in the most recent year.\nTo recreate it, we need store these four values, and draw the coloured line.\n\n\n\nA function for plotting the graph\nFirst we write a function to plot the line that is a very minimal ggplot, removing lots of superfluous elements with the theme() command.\n\n\nCode\n# Here the function to plot the line takes only one argument, `cd` the country code \nplot_line &lt;- function(cd) {\n  # get axis marks\n  label_y &lt;- df %&gt;%\n    filter(code == cd) %&gt;%\n    mutate(\n      min_year = min(year),\n      max_year = max(year)\n    ) %&gt;%\n    filter(year == min(year))\n  \n  # plot the line\n  df %&gt;%\n    filter(code == cd) %&gt;%\n    ggplot(aes(year, value)) +\n    geom_point(cex = 3) +\n    # mapping the colour of the line segment to the value is done here\n    geom_line(aes(colour = value), cex = 2, alpha = .7) +\n    # this scale is created above, with bounds appropriate to this data\n    scale_smoking() +\n    scale_y_continuous(\n      # specifying the break on the y-axis creates the axis text\n      breaks = c(label_y$value),\n      labels = scales::percent_format(scale = 1, accuracy = .1),\n      # the limits argument here ensures the y-axis starts at zero\n      limits = c(0, NA)\n    ) +\n    scale_x_continuous(\n      # x-axis needs only two years, the first and last\n      breaks = c(label_y$min_year, label_y$max_year)) +\n    theme(\n      # removing the axis ticks and lines clears the graph of clutter\n      axis.ticks.y = element_blank(),\n      panel.grid.major.x = element_blank(),\n      panel.grid.minor.x = element_blank(),\n      legend.position = \"none\",\n      text = element_text(size = 20)\n    ) +\n    labs(\n      x = NULL,\n      y = NULL\n    )\n}\n\n# Test the function with India.\nplot_line(\"IND\")\n\n\n\n\n\n\n\nNext to make the table that displays the most recent value\nWe write a function to make a gt() table that contains just two cells. In the left cell of the table is our line graph from above that shows the evolution of smoking rates in each country from 2000 onwards. In the right cell is the most recent value for the metric.\n\n\nCode\nlibrary(gt)\nlibrary(gtExtras)\n\nmake_table &lt;- function(cd) {\n  message(\"Making table for \", cd)\n  vars &lt;- df %&gt;%\n    filter(code == cd) %&gt;%\n    filter(year == max(year))\n\n  plot &lt;- plot_line(cd)\n\n  # the `ggplot_image` command outputs an image that can easily be put into a gt table when formatted as markdown\n  plot &lt;- gt::ggplot_image(plot, height = px(250), aspect_ratio = 1.6)\n\n  tbl &lt;- tibble(plot = plot, value = vars$value, context = glue::glue(\"in {vars$year}\"))\n\n  gt(tbl) %&gt;%\n    fmt_markdown(columns = c(plot)) %&gt;%\n    fmt_percent(value, scale_values = F, decimals = 1) %&gt;% \n    # the `merge_stack` command joins the value and the year in one cell\n    # the `smoking_hex` function we created above makes the text coloured appropriately\n    gt_merge_stack(col1 = value, col2 = context, palette = c(smoking_hex(tbl$value), \"grey\")) %&gt;%\n    tab_style(\n      style = cell_text(size = \"xx-large\"),\n      locations = cells_body(\n        columns = c(value)\n      )\n    ) %&gt;%\n    tab_header(\n      # title table with coutnry name\n      title = md(glue::glue(\"**{vars$entity}**\"))) %&gt;%\n    tab_options(column_labels.hidden = TRUE) %&gt;% \n    as_raw_html(inline_css = F)\n}\n\n# Test on South Africa\nmake_table(\"ZAF\")\n\n\n\n\n  \n  \n\n\n\n\n\n\n\nSouth Africa\n\n\n\n\n\n\n\n\n20.3%\n\n\nin 2020\n\n\n\n\n\n\n\n\n\n\nCreating the plots for each country\nHere we use the purrr::map command to make the table in raw HTML for each country and save it inside a tibble. The output shows an HTML list in the column called gt.\n\n\nCode\ngt_tables &lt;- df %&gt;% \n  distinct(code) %&gt;%\n  mutate(gt = purrr::map(code, make_table))\n\ngt_tables\n\n\nWe then create a tibble called df_map that selects the most recent year for each country from the dataset and joins it to the map by the country code variable we created above. Finally we join this to the tibble of tables called gt_tables.\n\n\nCode\ndf_map &lt;- df %&gt;% \n  group_by(entity) %&gt;% \n  filter(year == max(year)) %&gt;% \n  ungroup() %&gt;% \n  left_join(map, by = c(\"code\"))\n\ndf_map &lt;- df_map %&gt;% \n  inner_join(gt_tables)\n\n\n\n\nCreating the interactive figure\nNow we are ready to create the interactive figure!\nWe begin by drawing a static map in grey, with data from the original map. Next we overlay the interactive choropleth. The grey static map will show through all the countries we don’t have data on in the dataset.\n\n\nCode\ng &lt;- df_map %&gt;%\n  ggplot(aes(geometry = geom)) +\n  geom_sf(data = map, fill = \"grey80\") +\n  geom_sf_interactive(aes(fill = value, tooltip = gt)) +\n  scale_fill_binned(type = \"viridis\", labels = scales::percent_format(scale = 1)) +\n  cowplot::theme_minimal_grid() +\n  theme(legend.position = \"bottom\") +\n  guides(fill = guide_colourbar(barwidth = 20, barheight = .5, title.position = \"top\", label = TRUE)) +\n  labs(\n    fill = \"Share of adults who smoke, 2020\",\n    caption = \"Source: World Health Organization (via World Bank)\"\n  ) +\n  theme(\n    plot.background = element_rect(fill = \"white\", color = NA),\n    plot.title = element_text(hjust = 0.5, family = \"marker\", size = 50),\n    plot.subtitle = element_markdown(size = 20, family = \"open\", lineheight = 0.5),\n    plot.caption = element_markdown(size = 12, family = \"open\"),\n    axis.text = element_blank(),\n    axis.title = element_blank(),\n    panel.grid.major = element_line(color = \"grey80\", size = 0.1),\n    legend.title.align = .5\n  )\n\n\n\n\n\n\n\n\n\nShow off the interactive figure\nWow! Have a look at that! Pretty neat, and similar to the OWID explorer. I might make this more extendable in the future that you can upload your own data, but that’s for another day.\n\n\nCode\n# customizing the CSS makes the hover box easier to read.\ntooltip_css &lt;- \"background-color:gray;color:white;padding:10px;border-radius:5px;text-align:center;\"\n\nggiraph(\n  ggobj = g,\n  options = list(\n    opts_tooltip(css = tooltip_css),\n    opts_sizing(width = 1)\n  )\n)\n\n\n\n\n\n\n\n\n\nWhich African country has seen the largest reduction in smoking?\nFinally, we can plot the evolution of smoking behaviour among adults in Africa, highlighting my home nation of South Africa.\n\n\nCode\nget_ranking &lt;- function(continent_in) {\n  message(\"Getting ranking for \", continent_in)\n\n  continent_out &lt;- df %&gt;%\n    mutate(continent = countrycode(code, \"iso3c\", \"continent\")) %&gt;%\n    filter(continent == continent_in)\n\n  df_rank &lt;- continent_out %&gt;%\n    select(year, country_name = entity, value) %&gt;%\n    distinct()\n\n  df_rank &lt;- df_rank %&gt;%\n    group_by(year) %&gt;%\n    mutate(rank = rank(desc(value), ties.method = \"first\")) %&gt;%\n    ungroup()\n\n  labels_left &lt;- df_rank %&gt;%\n    filter(year == min(year)) %&gt;%\n    mutate(\n      left_rank = rank,\n      left_value = value\n    ) %&gt;%\n    select(country_name, left_rank, left_value)\n\n  labels_right &lt;- df_rank %&gt;%\n    filter(year == max(year)) %&gt;%\n    mutate(\n      right_rank = rank,\n      right_value = value\n    ) %&gt;%\n    select(country_name, right_rank, right_value)\n\n  df_rank &lt;- df_rank %&gt;%\n    inner_join(labels_left) %&gt;%\n    inner_join(labels_right)\n  \n  return(df_rank)\n}\n\nafrica_rank &lt;- get_ranking(\"Africa\")\n\n\n\n\nCode\nplot_levels &lt;- function(continent_in, highlight_country_in){\n  \n  tbl &lt;- get_ranking(continent_in)\n  \n  midpoint_in &lt;- tbl %&gt;% \n    summarise(mean(value)) %&gt;% \n    pull()\n  \n  tbl %&gt;%\n  mutate(across(contains(\"value\"), ~ round(.x, digits = 1))) %&gt;%\n  ggplot(aes(year, value, colour = right_value, group = country_name)) +\n  geom_line(size = 2.8, aes(year, value, group = country_name), colour = \"black\") +\n  geom_line(size = 2) +\n  geom_line(size = 3, colour = \"black\", data = tbl %&gt;% filter(country_name == highlight_country_in)) +\n  geom_text(aes(\n    x = 2000,\n    y = left_value,\n    label = paste0(country_name, \" \", left_value, \" %\")\n  ),\n  check_overlap = T,\n  colour = \"grey20\",\n  hjust = 1.1,\n  cex = 3\n  ) +\n  geom_text(aes(\n    x = 2020,\n    y = right_value,\n    label = paste0(country_name, \" \", right_value, \" %\")\n  ),\n  check_overlap = T,\n  colour = \"grey20\",\n  hjust = 0,\n  cex = 3\n  ) +\n  # scale_y_reverse() +\n  scale_color_gradient2(\n    low = \"#3C5488\",\n    high = \"#00A087\",\n    mid = \"#4DBBD5\",\n    midpoint = midpoint_in\n  ) +\n  scale_x_continuous(breaks = c(2000, 2005, 2010, 2015, 2018, 2019, 2020)) +\n  coord_cartesian(xlim = c(1995, 2025)) +\n  theme(\n    legend.position = \"none\",\n    panel.grid.major.y = element_blank(),\n    panel.grid.minor.y = element_blank(),\n    axis.ticks.y = element_blank(),\n    axis.text.y = element_blank()\n  ) +\n  labs(\n    x = NULL,\n    y = \"Percentage of adults who smoke\",\n    title = glue::glue(\"Evolution of percentage of adults who smoke in {continent_in}\"),\n    subtitle = glue::glue(\"{highlight_country_in} is highlighted in black\"),\n    caption = \"Data: WHO via World Bank\\nGraphic: @JonathanJayes\"\n  )\n  \n}\n\n\n# jpeg(\n#   filename = \"posts/2022-10-17-our-world-in-data-choropleth/images/Africa_levels.jpeg\",\n#   height = 10,\n#   width = 8,\n#   units = \"in\",\n#   res = 1000\n# )\n\nplot_levels(continent_in = \"Africa\",\n            highlight_country_in = \"South Africa\")\n\n\n\n\n\nCode\n# dev.off()\n\n\nHmm, South Africa has seen a drop in smoking since 2000, but the drop is relatively small in comparison to the progress that other African nations have made in the last two decades.\nWe can also plot the evolution of the ranking of who smokes the most in Africa.\n\n\nCode\nplot_ranking &lt;- function(continent_in, highlight_country_in){\n  \n  tbl &lt;- get_ranking(continent_in)\n  \n  midpoint_in &lt;- tbl %&gt;% \n    distinct(country_name) %&gt;% \n    count() %&gt;% \n    pull() / 2\n  \n  tbl %&gt;%\n    mutate(across(contains(\"value\"), ~ scales::percent(.x, scale = 1, accuracy = 1))) %&gt;%\n    ggplot(aes(year, rank, colour = right_rank, group = country_name)) +\n    geom_line(size = 2.8, aes(year, rank, group = country_name), colour = \"black\") +\n    geom_line(size = 2) +\n    geom_line(size = 3, colour = \"black\", data = tbl %&gt;% filter(country_name == highlight_country_in)) +\n    geom_text(aes(\n      x = 2000,\n      y = left_rank,\n      label = paste0(left_rank, \". \", country_name, \" \", left_value)\n    ),\n    colour = \"grey20\",\n    hjust = 1.1,\n    cex = 3\n    ) +\n    geom_text(aes(\n      x = 2020,\n      y = right_rank,\n      label = paste0(right_rank, \". \", country_name, \" \", right_value)\n    ),\n    colour = \"grey20\",\n    hjust = 0,\n    cex = 3\n    ) +\n    scale_y_reverse() +\n    scale_color_gradient2(\n      low = \"blue\",\n      high = \"red\",\n      mid = \"pink\",\n      midpoint = midpoint_in\n    ) +\n    scale_x_continuous(breaks = c(2000, 2005, 2010, 2015, 2018, 2019, 2020)) +\n    coord_cartesian(xlim = c(1995, 2025)) +\n    theme(\n      legend.position = \"none\",\n      panel.grid.major.y = element_blank(),\n      panel.grid.minor.y = element_blank(),\n      axis.ticks.y = element_blank(),\n      axis.text.y = element_blank()\n    ) +\n    labs(\n      x = NULL,\n      y = \"Ranking of adults who smoke\",\n      title = glue::glue(\"Evolution of ranking of adults who smoke in {continent_in}\"),\n      subtitle = glue::glue(\"{highlight_country_in} is highlighted in black\"),\n      caption = \"Data: WHO via World Bank\\nGraphic: @JonathanJayes\"\n    )\n}\n\n# jpeg(\n#   filename = \"posts/2022-10-17-our-world-in-data-choropleth/images/Africa_ranking.jpeg\",\n#   height = 10,\n#   width = 8,\n#   units = \"in\",\n#   res = 1000\n# )\n\nplot_ranking(\"Africa\", \"South Africa\")\n\n\n\n\n\nCode\n# dev.off()\n\n\nNow we can see that South Africa has climbed the rankings from 18th place in 2000 up to 7th in 2020. Interesting!\n\n\nWhat about the other continents?\nHave a look at each below, and then give my Shiny app a gander (TODO).\n\n\nCode\nmake_tab &lt;- function(continent_in, highlight_country_in) { # function to make the tabs\n  cat(\"##\", continent_in) # Label tab\n  cat(\"\\n\") # Close tab\n  p &lt;- plot_levels(\n    continent_in,\n    highlight_country_in\n  ) # Create plot in levels\n  print(p) # Display plot\n  cat(\"\\n\") # Space\n  q &lt;- plot_ranking(\n    continent_in,\n    highlight_country_in\n  ) # Create plot in levels\n  print(q)\n  cat(\"\\n\") # Close tab\n}\n\n\n\nAmericasAsiaEuropeOceania\n\n\n\n\n\n\n\n\n\nCode\nmake_tab(\"Asia\", \"China\")\n\n\n\n\n\n\n\n\n\n\nCode\nmake_tab(\"Europe\", \"Sweden\")\n\n\n\n\n\n\n\n\n\n\nCode\nmake_tab(\"Oceania\", \"New Zealand\")"
  },
  {
    "objectID": "posts/2023-01-31-map-projections/Map-projections.html",
    "href": "posts/2023-01-31-map-projections/Map-projections.html",
    "title": "Map projections in R",
    "section": "",
    "text": "Transform two shapefiles to the same coordinate system and plot them together.\n\n\nWe load the tidyverse for data manipulation, the sf package fo working with maps and set the ggplot theme to a light one for more attractive plots.\n\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n\n\n\nWe read in our shapefiles with the st_read() function, and ask for output of the crs with the st_crs() function so that we can see what it is.\nFor Sweden we can see we are told in he last line Projected CRS: SWEREF99 TM.\nSWEREF99 TM a projection favoured by the statistics body in Sweden.\n\nsweden_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\",\n                                 \"sweden_map.shp\"))\n\nReading layer `sweden_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\sweden_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 278651.2 ymin: 6134621 xmax: 917514.5 ymax: 7668755\nProjected CRS: SWEREF99 TM\n\n\nFor Norway it is WGS 84, a very common projection with full name World Geodetic System 1984.\n\nnorway_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\", \n                                 \"norway_map.shp\"))\n\nReading layer `norway_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\norway_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -9.098877 ymin: 58.02095 xmax: 33.6293 ymax: 80.47783\nGeodetic CRS:  WGS 84\n\n\nLet’s have a quick look at them side by side.\n\nggplot()+ \n  geom_sf(data = sweden_map) +\n  labs(title = \"Sweden\",\n       subtitle = \"CRS = SWEREF99\")\nggplot()+ \n  geom_sf(data = norway_map) +\n  labs(title = \"Norway\",\n       subtitle = \"CRS = WGS84\")\n\n\n\n\n\n\nSweden\n\n\n\n\n\n\n\nNorway\n\n\n\n\n\n\n\n\n\nWe’ll use EPSG 4326 or WGS84. We can use the st_transform() function, specifying the crs that we want for our new map object, which we name sweden_map_4326 to avoid confusion.\nThis is what the CRS now looks like for Sweden - the same as the one for Norway.\n\nsweden_map_4326 &lt;- st_transform(sweden_map, crs = 4326)\n\nsweden_map_4326 %&gt;% \n  st_crs()\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nWe can plot the together and choose some nice colours from the national flags for the fill aesthetic.\n\nggplot() +\n  geom_sf(data = sweden_map_4326, fill = \"#03558A\") +\n  geom_sf(data = norway_map, fill = \"#B40C2E\")\n\n\n\n\nMap of Sweden and Norway\n\n\n\n\n\n\n\nWe can save the shapefile for Sweden with the st_write() command if we want to keep the maps separate.\n\nsweden_map_4326 %&gt;% \n  st_write(here::here(\"posts\", \n                      \"2023-01-31-map-projections\", \n                      \"data\", \n                      \"sweden_map_4326.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting layer `sweden_map_4326' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting 1 features with 63 fields and geometry type Multi Polygon.\n\n\n\n\n\nAlternatively we can join together the maps as one object and then save one shapefile with the maps combined.\nTo do this, we first bind the two objects together using bind_rows() command and write it out with the same st_write() command.\n\ncombined_map &lt;- bind_rows(norway_map, sweden_map_4326)\n\ncombined_map %&gt;% st_write(here::here(\"posts\", \n                                     \"2023-01-31-map-projections\", \n                                     \"data\", \n                                     \"shapefile_combined.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting layer `shapefile_combined' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting 2 features with 63 fields and geometry type Multi Polygon.\n\n\nIf we want to get rid of the borders between our objects, we can use the st_union() command:\n\nunion_map &lt;- st_union(norway_map, sweden_map_4326)\n\nWe can compare the outputs here:\n\n# Combined map\nggplot() +\n  geom_sf(data = combined_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"#B40C2E\", \"#03558A\")) +\n  labs(fill = \"Country\")\n# Union map\nggplot() +\n  geom_sf(data = union_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"darkgreen\")) +\n  labs(fill = \"Country\")\n\n\n\n\n\n\nOutput from bind_rows\n\n\n\n\n\n\n\nOutput from st_union\n\n\n\n\n\n\nWhile st_union() is not useful in this particular instance, it may be of use to you in the future!\n\n\n\nHere we learned how to make it possible to plot maps with two different projections together by making use of the st_transform() command in R. We learned how to save the two maps as separate features in a shapefile, as well as how to combine them together. We can import the shapefile into any other software for plotting."
  },
  {
    "objectID": "posts/2023-01-31-map-projections/Map-projections.html#purpose",
    "href": "posts/2023-01-31-map-projections/Map-projections.html#purpose",
    "title": "Map projections in R",
    "section": "",
    "text": "Transform two shapefiles to the same coordinate system and plot them together.\n\n\nWe load the tidyverse for data manipulation, the sf package fo working with maps and set the ggplot theme to a light one for more attractive plots.\n\nlibrary(tidyverse)\nlibrary(sf)\ntheme_set(theme_light())\n\n\n\n\nWe read in our shapefiles with the st_read() function, and ask for output of the crs with the st_crs() function so that we can see what it is.\nFor Sweden we can see we are told in he last line Projected CRS: SWEREF99 TM.\nSWEREF99 TM a projection favoured by the statistics body in Sweden.\n\nsweden_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\",\n                                 \"sweden_map.shp\"))\n\nReading layer `sweden_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\sweden_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: 278651.2 ymin: 6134621 xmax: 917514.5 ymax: 7668755\nProjected CRS: SWEREF99 TM\n\n\nFor Norway it is WGS 84, a very common projection with full name World Geodetic System 1984.\n\nnorway_map &lt;- st_read(here::here(\"posts\", \n                                 \"2023-01-31-map-projections\", \n                                 \"data\", \n                                 \"norway_map.shp\"))\n\nReading layer `norway_map' from data source \n  `C:\\Users\\User\\Documents\\Recon\\interlude-one\\posts\\2023-01-31-map-projections\\data\\norway_map.shp' \n  using driver `ESRI Shapefile'\nSimple feature collection with 1 feature and 63 fields\nGeometry type: MULTIPOLYGON\nDimension:     XY\nBounding box:  xmin: -9.098877 ymin: 58.02095 xmax: 33.6293 ymax: 80.47783\nGeodetic CRS:  WGS 84\n\n\nLet’s have a quick look at them side by side.\n\nggplot()+ \n  geom_sf(data = sweden_map) +\n  labs(title = \"Sweden\",\n       subtitle = \"CRS = SWEREF99\")\nggplot()+ \n  geom_sf(data = norway_map) +\n  labs(title = \"Norway\",\n       subtitle = \"CRS = WGS84\")\n\n\n\n\n\n\nSweden\n\n\n\n\n\n\n\nNorway\n\n\n\n\n\n\n\n\n\nWe’ll use EPSG 4326 or WGS84. We can use the st_transform() function, specifying the crs that we want for our new map object, which we name sweden_map_4326 to avoid confusion.\nThis is what the CRS now looks like for Sweden - the same as the one for Norway.\n\nsweden_map_4326 &lt;- st_transform(sweden_map, crs = 4326)\n\nsweden_map_4326 %&gt;% \n  st_crs()\n\nCoordinate Reference System:\n  User input: EPSG:4326 \n  wkt:\nGEOGCRS[\"WGS 84\",\n    DATUM[\"World Geodetic System 1984\",\n        ELLIPSOID[\"WGS 84\",6378137,298.257223563,\n            LENGTHUNIT[\"metre\",1]]],\n    PRIMEM[\"Greenwich\",0,\n        ANGLEUNIT[\"degree\",0.0174532925199433]],\n    CS[ellipsoidal,2],\n        AXIS[\"geodetic latitude (Lat)\",north,\n            ORDER[1],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n        AXIS[\"geodetic longitude (Lon)\",east,\n            ORDER[2],\n            ANGLEUNIT[\"degree\",0.0174532925199433]],\n    USAGE[\n        SCOPE[\"Horizontal component of 3D system.\"],\n        AREA[\"World.\"],\n        BBOX[-90,-180,90,180]],\n    ID[\"EPSG\",4326]]\n\n\n\n\n\nWe can plot the together and choose some nice colours from the national flags for the fill aesthetic.\n\nggplot() +\n  geom_sf(data = sweden_map_4326, fill = \"#03558A\") +\n  geom_sf(data = norway_map, fill = \"#B40C2E\")\n\n\n\n\nMap of Sweden and Norway\n\n\n\n\n\n\n\nWe can save the shapefile for Sweden with the st_write() command if we want to keep the maps separate.\n\nsweden_map_4326 %&gt;% \n  st_write(here::here(\"posts\", \n                      \"2023-01-31-map-projections\", \n                      \"data\", \n                      \"sweden_map_4326.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting layer `sweden_map_4326' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/sweden_map_4326.shp' using driver `ESRI Shapefile'\nWriting 1 features with 63 fields and geometry type Multi Polygon.\n\n\n\n\n\nAlternatively we can join together the maps as one object and then save one shapefile with the maps combined.\nTo do this, we first bind the two objects together using bind_rows() command and write it out with the same st_write() command.\n\ncombined_map &lt;- bind_rows(norway_map, sweden_map_4326)\n\ncombined_map %&gt;% st_write(here::here(\"posts\", \n                                     \"2023-01-31-map-projections\", \n                                     \"data\", \n                                     \"shapefile_combined.shp\"), delete_dsn = TRUE)\n\nDeleting source `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting layer `shapefile_combined' to data source \n  `C:/Users/User/Documents/Recon/interlude-one/posts/2023-01-31-map-projections/data/shapefile_combined.shp' using driver `ESRI Shapefile'\nWriting 2 features with 63 fields and geometry type Multi Polygon.\n\n\nIf we want to get rid of the borders between our objects, we can use the st_union() command:\n\nunion_map &lt;- st_union(norway_map, sweden_map_4326)\n\nWe can compare the outputs here:\n\n# Combined map\nggplot() +\n  geom_sf(data = combined_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"#B40C2E\", \"#03558A\")) +\n  labs(fill = \"Country\")\n# Union map\nggplot() +\n  geom_sf(data = union_map, aes(fill = sovereignt)) +\n  scale_fill_manual(values = c(\"darkgreen\")) +\n  labs(fill = \"Country\")\n\n\n\n\n\n\nOutput from bind_rows\n\n\n\n\n\n\n\nOutput from st_union\n\n\n\n\n\n\nWhile st_union() is not useful in this particular instance, it may be of use to you in the future!\n\n\n\nHere we learned how to make it possible to plot maps with two different projections together by making use of the st_transform() command in R. We learned how to save the two maps as separate features in a shapefile, as well as how to combine them together. We can import the shapefile into any other software for plotting."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html",
    "href": "posts/2023-03-10/chatgpt-as-translator.html",
    "title": "ChatGPT as translator and text wizard",
    "section": "",
    "text": "Data is the lifeblood of modern economic history. It courses through our models and charts the path of history. Yet data is often born unruly, its words jumbled and disorganized, its meaning buried under mountains of text. For the economic historian, this is an all too familiar challenge. We need to extract structure from chaos, to transform raw text into organized data that we can analyze.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts a handshake between a robot arm and a human arm\n\n\n\n\nBut doing so by hand is a laborious and time-consuming task, one that demands hours of tedious work and a heightened attention to detail. Luckily, we live in an age of powerful AI tools, such as ChatGPT, that can help us wrangle our data and make sense of the world."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#introduction",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#introduction",
    "title": "ChatGPT as translator and text wizard",
    "section": "",
    "text": "Data is the lifeblood of modern economic history. It courses through our models and charts the path of history. Yet data is often born unruly, its words jumbled and disorganized, its meaning buried under mountains of text. For the economic historian, this is an all too familiar challenge. We need to extract structure from chaos, to transform raw text into organized data that we can analyze.\n\n\n\n\n\n\nAn oil painting in the style of Franz Marc that depicts a handshake between a robot arm and a human arm\n\n\n\n\nBut doing so by hand is a laborious and time-consuming task, one that demands hours of tedious work and a heightened attention to detail. Luckily, we live in an age of powerful AI tools, such as ChatGPT, that can help us wrangle our data and make sense of the world."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#a-typical-problem",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#a-typical-problem",
    "title": "ChatGPT as translator and text wizard",
    "section": "A typical problem",
    "text": "A typical problem\nI am interested in a source of Swedish biographical data called Vem är Vem? (or Who is Who?), part of Projekt Runeberg, a volunteer effort to create free electronic editions of classic Scandinavian literature.\nIn particular I am interested in the occupational trajectories of the individuals - what job titles do they hold through the course of their lives. By extracting information on the career paths of notable Swedes, we can gain insights into the labour market dynamics of Sweden and how they have evolved over time. With this structured data, economic historians can explore questions such as the influence of education and social networks on career trajectories, the impact of technological change on the labour market, and much more.\nCareful work by two librarians in Uppsala has resulted in a large selection of the source being scanned and digitized. Though the OCR engine is not perfect, the resulting text is largely legible. A typical example of the source is shown below.\nThe city librarian in Uppsala Paul Harnesk was the initiator and chief editor of the work Who is Who? which was published in two editions, each comprising five parts. He died in 1965 and the work was completed by the first librarian at Uppsala University Library, associate professor Åke Davidsson.\n\n\n\nAn example of the source, note the highlighted abbreviations wherein “f.” means “född” and “Med. kand.” means “medicine kandidat”.**"
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#planning",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#planning",
    "title": "ChatGPT as translator and text wizard",
    "section": "Planning",
    "text": "Planning\nExtracting occupational trajectories from the Swedish biographical source “Vem är Vem?” comprises a two-part process. The first part involves translating the Swedish text, which is full of abbreviations and contractions, into English using the natural language processing capabilities of ChatGPT. This step is crucial for making the data accessible and understandable to researchers who may not be familiar with Swedish (like myself!).\nOnce the text has been translated, the second part of the process demands structuring the data into year and occupation pairs. This involves identifying mentions of job titles and dates in the text and extracting them in a structured format."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-translator",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-translator",
    "title": "ChatGPT as translator and text wizard",
    "section": "ChatGPT as translator",
    "text": "ChatGPT as translator\nI simply ask ChatGPT:\nCan you convert the following text from Swedish into English? There are many abbreviations, for instance, “f.” means “född” or born, “Med. kand.” means “medicine kandidat”.\nIt is a biography about a man:\nAnd I paste in the OCR output:\nAbramson, Ernst Leopold, professor, Djursholm, f. i Grava, Värml. L, 12/3 /96 av godsäg. Hugo A. o. Anna Salén. Med. kand. Sthlm 18, med. lic. o. dr 27, doc. i fysiol. 27, labor. i fysiol. 30-38, prof. o. förest, v. statrs inst. f. folkhälsan fr. 38. Batalj :läk. i f alt-läk rkår. res. 27, lär. i fysiol, o. rörel-sefysiol. v. gymn. centr anst. 22-30, led. av arméförv :s sjukv rstyr. vetensk. råd 36, medl. av riksidr rförb. öv rstyr. 24-36, skolidr :förb. läk. 28, led. av direkt, öv. gymn. centr :inst. 33, ordf. där 36, led. o. sekr. i näringsråd. 36, ordf. i 36 års livsmedelslagstiftn :-sakk. Utg. ett 50-tal arb. särsk. rör. näringsfysiol., fysiol, mek., muskel-fysiol. RNO.\n\n\n\nScreenshot of input\n\n\nChatGPT returns to me the translated text, free from abbreviations and contractions:\n\nAbramson, Ernst Leopold, professor, Djursholm, born in Grava, Värmland, on March 12, 1896, to estate owner Hugo A. and Anna Salén. He obtained a medicine kandidat degree in Stockholm in 1918, followed by a medical licentiate and doctorate in 1927. He was a docent in physiology in 1927 and worked as a laboratory assistant in physiology from 1930 to 1938. From 1938, he was a professor and director at the State Institute for Public Health. He served as a battalion doctor and reserve officer from 1927 and was a teacher in physiology and movement physiology at the Gymnastics Central Institute from 1922 to 1930. He was a member of the Army Medical Corps and Reserve Force from 1924 to 1936, a member of the Scientific Council of the Army Medical Service from 1936, and a member of the School Management Association of Doctors from 1928. He was also a member of the Directorate of the Gymnastics Central Institute from 1933, its chairman from 1936, and a member and secretary of the Nutrition Council from 1936. In addition, he was the chairman of the 1936 Food Legislation Committee. He authored around 50 works, primarily related to nutrition physiology, physiology, mechanics, and muscle physiology. He was awarded the Royal Order of the North Star (RNO).\n\nIt surprises me that ChatGPT is able to extrapolate correctly from just “RNO” (an abbreviation of the Swedish Nordstjärneorden) in the final part of the description to:\n\nHe was awarded the Royal Order of the North Star (RNO).\n\nFantastic!\n\n\n\nScreenshot of text output\n\n\nNext I ask ChatGPT to return to us the occupational information in a structured format."
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-interpreter",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#chatgpt-as-interpreter",
    "title": "ChatGPT as translator and text wizard",
    "section": "ChatGPT as interpreter",
    "text": "ChatGPT as interpreter\nStrucutred data is our goal. Again I use a simple instruction to ask:\nGreat, thank you, can you extract the occupations he held and the dates for which he held them, and return the output in an R tibble with a column for start year, a column for end year (if any), a column for the Swedish description of the occupation, and a column for the English translation.\nIt returns the following R code:\n\nlibrary(tidyverse)\n\noccupations &lt;- tibble(\n  start_year = c(1918, 1927, 1927, 1930, 1938, 1927, 1922, 1924, 1936, 1928, 1933, 1936),\n  end_year = c(NA, NA, NA, 1938, NA, 1936, 1930, 1936, NA, NA, NA, NA),\n  swedish_occupation = c(\n    \"Med. kand.\", \"Med. lic. o. dr\", \"Docent i fysiol.\", \"Labor. i fysiol.\",\n    \"Prof. o. förest, v. stats inst. f. folkhälsan\", \"Batalj :läk. i f alt-läk rkår.\",\n    \"Lär. i fysiol, o. rörel-sefysiol. v. gymn. centr anst.\", \"Led. av arméförv :s sjukv rstyr.\",\n    \"Medl. av riksidr rförb.\", \"Skolidr :förb. läk.\", \"Led. av direkt, öv. gymn. centr :inst.\",\n    \"Ordf. i 36 års livsmedelslagstiftn :-sakk.\"\n  ),\n  english_occupation = c(\n    \"Medicine kandidat\", \"Medical licentiate and doctorate\", \"Docent in physiology\",\n    \"Laboratory assistant in physiology\", \"Professor and director at the State Institute for Public Health\",\n    \"Battalion doctor and reserve officer\", \"Teacher in physiology and movement physiology at the Gymnastics Central Institute\",\n    \"Member of the Army Medical Corps and Reserve Force\", \"Member of the Scientific Council of the Army Medical Service\",\n    \"Member of the School Management Association of Doctors\", \"Member of the Directorate of the Gymnastics Central Institute\",\n    \"Chairman of the 1936 Food Legislation Committee.\"\n  )\n)"
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#displaying-the-output",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#displaying-the-output",
    "title": "ChatGPT as translator and text wizard",
    "section": "Displaying the output",
    "text": "Displaying the output\n\nTable\nI can easily take this code and turn the output into a table with a hyperlink to the source:\n\n\nCode\nlibrary(gt)\n\noccupations %&gt;%\n  arrange(start_year) %&gt;%\n  gt() %&gt;%\n  tab_header(title = md(\"**Ernst Leopold Abramson's career trajectory**\")) %&gt;%\n  cols_label(\n    start_year = md(\"Start\"),\n    end_year = md(\"End\"),\n    swedish_occupation = md(\"Swedish\"),\n    english_occupation = md(\"English\")\n  ) %&gt;%\n  sub_missing(columns = end_year, missing_text = \"\") %&gt;%\n  tab_spanner(md(\"**Duration**\"), columns = c(start_year, end_year)) %&gt;%\n  tab_spanner(md(\"**Occupation**\"), columns = c(swedish_occupation, english_occupation)) %&gt;%\n  tab_source_note(md(\"Data source: [Projekt Runeberg](http://runeberg.org/vemarvem/sthlm45/0018.html)\"))\n\n\n\n\n\n\n  \n    \n      Ernst Leopold Abramson's career trajectory\n    \n    \n  \n  \n    \n      \n        Duration\n      \n      \n        Occupation\n      \n    \n    \n      Start\n      End\n      Swedish\n      English\n    \n  \n  \n    1918\n\nMed. kand.\nMedicine kandidat\n    1922\n1930\nLär. i fysiol, o. rörel-sefysiol. v. gymn. centr anst.\nTeacher in physiology and movement physiology at the Gymnastics Central Institute\n    1924\n1936\nLed. av arméförv :s sjukv rstyr.\nMember of the Army Medical Corps and Reserve Force\n    1927\n\nMed. lic. o. dr\nMedical licentiate and doctorate\n    1927\n\nDocent i fysiol.\nDocent in physiology\n    1927\n1936\nBatalj :läk. i f alt-läk rkår.\nBattalion doctor and reserve officer\n    1928\n\nSkolidr :förb. läk.\nMember of the School Management Association of Doctors\n    1930\n1938\nLabor. i fysiol.\nLaboratory assistant in physiology\n    1933\n\nLed. av direkt, öv. gymn. centr :inst.\nMember of the Directorate of the Gymnastics Central Institute\n    1936\n\nMedl. av riksidr rförb.\nMember of the Scientific Council of the Army Medical Service\n    1936\n\nOrdf. i 36 års livsmedelslagstiftn :-sakk.\nChairman of the 1936 Food Legislation Committee.\n    1938\n\nProf. o. förest, v. stats inst. f. folkhälsan\nProfessor and director at the State Institute for Public Health\n  \n  \n    \n      Data source: Projekt Runeberg\n    \n  \n  \n\n\n\n\n\n\nVisualization\nAnd we can take the output and make it into a nice figure that visualizes Ernst’s career trajectory.\n\n\nCode\noccupations %&gt;%\n  arrange(start_year) %&gt;%\n  mutate(end_year_imputed = case_when(\n    is.na(end_year) ~ lead(start_year),\n    is.na(lead(start_year)) ~ start_year,\n    TRUE ~ end_year\n  )) %&gt;%\n  mutate(\n    english_occupation = str_squish(str_remove_all(english_occupation, \"[:punct:]\")),\n    english_occupation = fct_reorder(english_occupation, start_year, .desc = TRUE)\n  ) %&gt;%\n  # pivot_longer(c(start_year, end_year_imputed)) %&gt;%\n  # ggplot(aes(value, english_occupation, colour = name)) +\n  ggplot(aes(xmin = start_year, xmax = end_year_imputed, y = english_occupation)) +\n  geom_linerange(colour = \"gray50\", size = 2) +\n  geom_point(aes(x = end_year_imputed), colour = \"#be5d2b\", size = 4) +\n  geom_point(aes(x = start_year), colour = \"#2B8CBE\", size = 4) +\n  labs(\n    title = \"Ernst Leopold Abramson's career trajectory\",\n    subtitle = \"Showing &lt;span style = 'color:#2B8CBE;'&gt;start dates&lt;/span&gt; and &lt;span style = 'color:#be5d2b;'&gt;end dates&lt;/span&gt; for his various roles\",\n    x = NULL,\n    y = NULL\n  ) +\n  theme(\n    legend.position = \"none\",\n    plot.subtitle = element_markdown(),\n    plot.title.position = \"plot\",\n    text = element_text(family = \"ibm\", size = 28)\n  )"
  },
  {
    "objectID": "posts/2023-03-10/chatgpt-as-translator.html#conclusion",
    "href": "posts/2023-03-10/chatgpt-as-translator.html#conclusion",
    "title": "ChatGPT as translator and text wizard",
    "section": "Conclusion",
    "text": "Conclusion\nThank you for reading along. I have demonstrated how we can use unstructured text data to reconstruct career trajectories for notable Swedes, translating and structuring the biographical text data through the ChatGPT web interface.\nNext week I’ll share a post on how to take this process and scale it by leveraging OpenAI’s API.\nI hope that it might prove useful in your own research!"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "",
    "text": "One of the claims of generative AI is that it will free humans from labourious tasks which are currently tedious to do manually and challenging to automate. In this article, we’ll explore how we can use LangChain to automate the process of extracting information from PDF documents.\nThe National Treasury in South Africa produces a fantastically informative budget review each year. It includes headline figures like GDP growth, inflation, revenue and expenditure, as well as more detailed information about the budget for each government department. In addition, the budget speech is delivered by the Minister of Finance, and contains a wealth of information about the budget, including the government’s priorities for the year, and the reasoning behind the budget allocations.\nIf we want to get specific information from these documents, we can read through them and extract the information manually, searching for specific terms with the control + F functionality. This is a time consuming process, and it’s easy to make mistakes. What if we could ask a computer to do it for us?\n\n\n\n\n\n\nNote\n\n\n\nThere are, in addition to the PDF documents a series of excel files that National Treasury releases too. These files do contain the information that we want, but are not in an easily machine readable format.1 Useful tools like EconData from Codera Analytics make it much easier to access this data programatically."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-challenge",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-challenge",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "",
    "text": "One of the claims of generative AI is that it will free humans from labourious tasks which are currently tedious to do manually and challenging to automate. In this article, we’ll explore how we can use LangChain to automate the process of extracting information from PDF documents.\nThe National Treasury in South Africa produces a fantastically informative budget review each year. It includes headline figures like GDP growth, inflation, revenue and expenditure, as well as more detailed information about the budget for each government department. In addition, the budget speech is delivered by the Minister of Finance, and contains a wealth of information about the budget, including the government’s priorities for the year, and the reasoning behind the budget allocations.\nIf we want to get specific information from these documents, we can read through them and extract the information manually, searching for specific terms with the control + F functionality. This is a time consuming process, and it’s easy to make mistakes. What if we could ask a computer to do it for us?\n\n\n\n\n\n\nNote\n\n\n\nThere are, in addition to the PDF documents a series of excel files that National Treasury releases too. These files do contain the information that we want, but are not in an easily machine readable format.1 Useful tools like EconData from Codera Analytics make it much easier to access this data programatically."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-solution-a-vector-database-and-a-large-language-model",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#the-solution-a-vector-database-and-a-large-language-model",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "The solution: a vector database and a Large Language Model",
    "text": "The solution: a vector database and a Large Language Model\nLangChain is a framework that allows us to connect a large language model to our own data. In this case, it allows to transform the South African National Budget Reviews and Budget Speeches into an interactive, queryable knowledge base. The final output in this case is a table of summary statistics that would have taken a human hours to compile, but which we can now generate in seconds.\nIn essence, to transform budget documents into a queryable database, we read in the PDFs, segment them into chunks, and store the text in a database. To query this database, we use a simple similarity calculation to find the most relevant chunks of text, which are then fed to a large language model to answer our questions.\nHere is a diagram of the process:\n\n\n\n\n\ngraph LR\n    A((Budget Documents PDF)) --&gt;|Chunking&lt;br&gt;Vectorization| B{Chroma DB}\n    C[User Query] --&gt;|Vectorization| D{Chroma DB}\n    D --&gt; E[Top-n Similar Chunks]\n    E --&gt; F{Large Language &lt;br&gt;Model Context Window}\n    F --&gt; G((Answer))\n    style A fill:#ece7f2,stroke:#333,stroke-width:2px\n    style B fill:#a6bddb,stroke:#333,stroke-width:2px\n    style C fill:#ece7f2,stroke:#333,stroke-width:2px\n    style D fill:#a6bddb,stroke:#333,stroke-width:2px\n    style E fill:#2b8cbe,stroke:#333,stroke-width:2px\n    style F fill:#a6bddb,stroke:#333,stroke-width:2px\n    style G fill:#2b8cbe,stroke:#333,stroke-width:2px"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#an-example-what-was-consumer-price-inflation-in-2022",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#an-example-what-was-consumer-price-inflation-in-2022",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "An example: what was Consumer Price Inflation in 2022?",
    "text": "An example: what was Consumer Price Inflation in 2022?\nSay for instance that we want to know what Consumer Price Inflation was in 2022 in South Africa. We can ask the question in a natural language query, and the system will return the answer. Here is the query and the answer, along with the source documents that the system used to answer the question:\n\n\n\n\n\n\n\n\nQuery\nResult\nSource documents\n\n\n\n\nWhat was Consumer Price Inflation inflation in 2022?\n6.9%\nDocument(page_content=to moderate in 2023, before rising gradually over the medium term . inflation peaked in the third quarter and averaged 6.9 per cent during 2022 . is projected is estimated to ease to 5.3 per cent in 2023. Global crude oil and food prices remain sources of inflationary pressure. Electricity prices have been up by 4.6 per cent over the medium term compared to the 2022 MTBPS and are’, metadata={‘source’: ‘c:\\Users\\User\\Documents\\Recon\\langchain-budget-speech-demo\\budget-documents\\budget-review-2023.pdf’, ‘page’: 39}),\n\n\n\n\nDocument(page_content=‘CHAPTER 1 NAVIGATING AN UNEVEN ECONOMIC RECOVERY Real percentage growth 2022 2023 2024 2025 Estimate Forecast Household consumption 2.8 1.0 1.5 1.8 Gross fixed-capital formation 4.2 1.3 3.8 3.5 Exports 8.8 1.0 2.2 2.9 Imports 14.0 1.1 2.3 2.9 Real GDP growth 2.5 0.9 1.5 1.8 Consumer price index (CPI) inflation 6.9 5.3 4.9 4.7 Current account balance (% of GDP) -0.4 -1.8 -2.0 -2.1’, metadata={‘source’: ‘c:\\Users\\User\\Documents\\Recon\\langchain-budget-speech-demo\\budget-documents\\budget-review-2023.pdf’, ‘page’: 27}),\n\n\n\nIn the table above, we can see two of the chunks of text that are most similar in terms of content to our query, from the ChromaDB database that contains the 2023 Budget Review and Budget Speech broken into 2000 token chunks. The first chunk in our table is from page 39 of the budget review, and the second chunk is from page 27. Both of these chunks contain the information that we are looking for, and the system has correctly identified them as the most relevant chunks.\nThese chunks are then passed to the OpenAI GPT-3.5 large language model, which is able to take in the question and the context, and then return to us the answer. In this case, the answer is 6.9%, which is correct, as we can see in the table from the budget review below (which is the same as the second source document in the markdown table above).\n\n\n\nScreenshot of table from 2023 Budget Review"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-is-going-on-in-the-background",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-is-going-on-in-the-background",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "What is going on in the background?",
    "text": "What is going on in the background?\nYou can have a look at my GitHub repository for the full code, but I’ll walk through the main steps here.\n\nSet Up: The first step is to set up LangChain using an OpenAI key. This example uses OpenAI for both the language model and the embeddings.\nLoad Documents: LangChain can load multiple documents at once. In this example, PDF files containing the 2023 budget review and 2023 budget speech are loaded.\nSplit Data: The data is then split into chunks. Each chunk corresponds to a portion of information from a specific page of the budget documents.\nCreate a Database: A vector store database is created to store and index the document embeddings, which are vector representations of the text chunks. The embeddings are generated using OpenAI and stored in a database using a tool named ChromaDB.\nRetrieve Relevant Documents: A retriever is then set up to query the vector database and return relevant documents. The number of documents returned can be adjusted according to the user’s needs. In this case, I used fewer chunks with larger number of tokens per chunk compared to the default as it worked best.\nLanguage Model Chain: After setting up the retriever, a retrieval-question-answering chain is created using OpenAI. This chain is capable of answering queries based on the contents of the chunks that are passed to it, depending on the words in the query.\nFormat Output: A function is created to present the results and source documents neatly. This is what we show in the markdown table above.\nQuerying: The system is now ready for queries. A query is input and the system returns the most relevant response based on the indexed documents. In this case, we worked through each years’ budget documents from 2010 to 2022 to find values for seven data series.\n\nHave a look at the output in the table below.\n\n\n\n\n\n\n  \n    \n      Selected series from Budget Reviews and Budget Speeches (2010-2022)\n    \n    \n  \n  \n    \n      Key\n      2010\n      2011\n      2012\n      2013\n      2014\n      2015\n      2016\n      2017\n      2018\n      2019\n      2020\n      2021\n      2022\n    \n  \n  \n    Budget balance (% of GDP)\n-5.30\n-4.80\n5.2\n-4.00\n-3.9\n-3.9\n-3.40\n-4.30\n-3.70\n-6.30\n-14.00\n-5.70\n-4.2\n    GDP (current ZAR[billions])\n2.67\n2.94\n\n3.39\n3.8\n\n4.32\n\n4.96\n5.09\n4.93\n6.17\n6.6\n    GDP growth (%)\n3.40\n3.10\n2.5\n1.80\n1.4\n1.3\n0.50\n0.01\n0.70\n0.30\n-7.20\n4.80\n2.5\n    General government debt (% of GDP)\n25.70\n33.50\n41.8\n34.90\n40.8\n44.8\n45.70\n53.30\n55.60\n61.60\n80.30\n69.50\n71.1\n    General government expenditure (% of GDP)\n30.80\n32.60\n32.9\n33.20\n32.0\n33.9\n32.80\n33.20\n32.90\n29.40\n41.70\n33.20\n32.6\n    General government revenue (% of GDP)\n28.30\n27.70\n27.7\n29.20\n28.1\n30.0\n29.40\n28.80\n28.80\n29.40\n27.70\n27.50\n28.5\n    Inflation (CPI) (%)\n4.30\n5.00\n5.4\n5.70\n5.3\n4.6\n6.40\n5.30\n4.70\n4.10\n3.30\n4.80\n6.9\n  \n  \n    \n      Source: National Treasury of South Africa"
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#visualising-the-data",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#visualising-the-data",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "Visualising the data",
    "text": "Visualising the data\nWe can also make a plot of our selected series. Mouse over the points to see the exact values."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-do-we-learn",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#what-do-we-learn",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "What do we learn?",
    "text": "What do we learn?\nThere are some missing values, particularly in the series GDP (current ZAR[billions]) and Budget balance (% of GDP).\nIn addition, in 2019, the government expenditure figure is incorectly plucked from the PDF document as 29.40% of GDP, when in reality, this was 35.70% of GDP. This is a significant error, and it is not clear why this happened. It is possible that the PDF document was not correctly parsed, or that the OpenAI API made an error extracting this value from the text chunk.\nIn order to overcome these kinds of errors, experimenting with multiple pipelines and chunk sizes is probably the best way to go. This was just a demonstration, and there is certainly a possiblity to make the pipline more robust and less error prone."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#conclusion",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#conclusion",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "Conclusion",
    "text": "Conclusion\nThough we have used budget data from South Africa in this case, the same approach can be used for any kind of data. LangChain has document loaders for text, PDF, and HTML, and can be used to extract information from many kinds of document."
  },
  {
    "objectID": "posts/2023-07-05-langchain-demo/langchain-demo.html#footnotes",
    "href": "posts/2023-07-05-langchain-demo/langchain-demo.html#footnotes",
    "title": "Transforming National Budget Documents into Queryable Knowledge",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nThere are ideosyncracies between each year that mean ingesting this data into a tidy format is time consuming↩︎"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html",
    "href": "posts/2023-10-02-geocoding/geocoding.html",
    "title": "Geocoding: two ways",
    "section": "",
    "text": "Locating places of interest on a map is an important task in many economic history projects. This tutorial will show you how to geocode a list of addresses.\nI will use both R and python to show you how to do this, depending on your preference and what you are most comfortable with.\n\n\n\n\n\n\nA beautiful oil painting of a mosaic of a map or a city plan in the unique texture and color palette of Gustav Caillebotte"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#getting-started",
    "href": "posts/2023-10-02-geocoding/geocoding.html#getting-started",
    "title": "Geocoding: two ways",
    "section": "Getting started",
    "text": "Getting started\nThere are many engines that you can use to geocode place names or addresses to coordinates, some popular engines include the Open Street Map Nominatim API and the Google Maps API.\nConceptually, there is a distinction between the code that you use to call the engine which gets the coordinates (R or python), and then there is the engine itself (OSM or Google Maps).\n\n\n\n\n\n\n\n\n\nFramework\nR (tidygeocoder) & Open Street Map Nominatim API\nPython (requests) & Google Maps Geocoding API\n\n\n\n\nGeocoding Engine\nOpen Street Map’s Nominatim\nGoogle Maps Geocoding\n\n\nRequest Method\nUses the tidygeocoder R package to send requests.\nUses the requests Python library to send HTTP requests.\n\n\nCost\nFree and open-source, but has usage policies to prevent heavy traffic.\nOffers limited free requests, billing is required for extensive usage.\n\n\n\n\nI use the tidygeocoder package in R, which is free, and the Google Maps Geocoding API in python, which is also free (up to some threshold), but requires an API key from Google and a credit card to sign up.1\nYou can read about getting an API key here."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#data",
    "href": "posts/2023-10-02-geocoding/geocoding.html#data",
    "title": "Geocoding: two ways",
    "section": "Data",
    "text": "Data\nFor the purpose of this tutorial, I have generated 20 place names to geocode and a column of arbitrary values for visualization.\n\n\n\nPlace Name\nArbitrary Value\n\n\n\n\nStockholm\n35\n\n\nGothenburg\n12\n\n\nMalmö\n27\n\n\nUppsala\n19\n\n\nVästerås\n22\n\n\nÖrebro\n15\n\n\nLinköping\n20\n\n\nHelsingborg\n24\n\n\nNorrköping\n28\n\n\nJönköping\n17\n\n\nLund\n23\n\n\nUmeå\n10\n\n\nGävle\n18\n\n\nBorås\n14\n\n\nEskilstuna\n16\n\n\nSödertälje\n13\n\n\nKarlstad\n21\n\n\nTäby\n29\n\n\nVäxjö\n11\n\n\nHalmstad\n26"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#r-and-tidygeocoder",
    "href": "posts/2023-10-02-geocoding/geocoding.html#r-and-tidygeocoder",
    "title": "Geocoding: two ways",
    "section": "R and tidygeocoder",
    "text": "R and tidygeocoder\nJesse Cambon has kindly made a package called tidygeocoder available on github.\nYou can install it on your computer using the following code:\n\ninstall.packages('tidygeocoder')\n\nHere is how you geocode a list of addresses using the tidygeocoder package.\nNote that lat and long here will be the column names for the coordinates that are returned.\nThe line method = \"osm\" specifies that we are using the Open Street Map Nominatim API. Here you can switch to another engine if you prefer.\n\nlibrary(tidyverse) # for data manipulation\nlibrary(tidygeocoder) # for geocoding\n\n# Read in data\nlibrary(readxl) # for reading excel files\nplaces &lt;- read_excel(\"data/data_to_geocode.xlsx\")\n\n# Geocode places\nplaces_geocoded &lt;- places %&gt;%\n    geocode(place_name,\n        method = \"osm\",\n        lat = latitude,\n        long = longitude\n    )\n\nThis what the output looks like when the geocoding is complete.\n\nPassing 20 addresses to the Nominatim single address geocoder [===================================] 20/20 (100%) Elapsed: 20s Remaining: 0s\n\nWe can visualize the results using the ggplot2 package.\n\n# Find bounds to trim map, adding some padding\nbounds &lt;- places_geocoded %&gt;%\n    summarise(xmin = min(longitude) - 0.1,\n              xmax = max(longitude) + 0.1,\n              ymin = min(latitude) - 0.1,\n              ymax = max(latitude) + 0.1)\n\n# Plot places\nggplot(places_geocoded, aes(longitude, latitude), color = \"grey99\") +\n    borders() +\n    geom_point(aes(size = arbitrary_value), colour = \"midnightblue\") +\n    ggrepel::geom_label_repel(aes(label = place_name)) +\n    theme_void() +\n    coord_cartesian(xlim = c(bounds$xmin, bounds$xmax), \n                    ylim = c(bounds$ymin, bounds$ymax)) +\n    labs(size = \"Arbitrary value\")\n\n\n\n\nGreat! We have successfully geocoded our list of addresses."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#geocoding-in-python",
    "href": "posts/2023-10-02-geocoding/geocoding.html#geocoding-in-python",
    "title": "Geocoding: two ways",
    "section": "Geocoding in python",
    "text": "Geocoding in python\nI have created a Colab notebook that shows you how to geocode addresses using the Google Maps Geocoding API in python. You can simply run the code in the notebook to get the coordinates of your own data by providing your own API key and data.\nYou can have a look at the Colab notebook here"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#real-world-use-case",
    "href": "posts/2023-10-02-geocoding/geocoding.html#real-world-use-case",
    "title": "Geocoding: two ways",
    "section": "Real world use case",
    "text": "Real world use case\nIn reality, it will likely not be as simple as running the above code once and getting all the coordinates you need.\nYou will likely need to do multiple passes, modify addresses, and evaluate the results.\nMultiple passes are necessary to maximize the accuracy and success rate of geocoding. The initial pass may not successfully geocode all addresses due to inaccuracies, ambiguities, or limitations of the geocoding tool. By modifying the problematic addresses and doing subsequent passes, you can resolve these issues and geocode more addresses correctly.\nFor example, an address might fail to geocode because it contains a typo, is too specific, or too ambiguous. By modifying the address to correct the typo or by making it more general or more specific, it might geocode successfully in a subsequent pass.\nThe term “out of bounds” in the context of geocoding refers to the geocoded results that do not fit within a predefined geographical area or are not reasonable based on prior knowledge. For instance, if you are geocoding addresses in Sweden, any result that lies outside the borders of Sweden would be considered “out of bounds.”\nIn practical terms, if you know the general area where the addresses should be located (like a specific country or city), you can compare the geocoded results against this area. If the geocoded location falls outside this area, it is considered out of bounds, and you might need to modify the address and try geocoding it again, or use a different geocoding engine that might provide more accurate results.\nHere is a diagram that shows the real world process of geocoding in more detail.\n\n\n\n\nstateDiagram-v2\n    [*] --&gt; FirstPass\n    FirstPass --&gt; StoreResults: In bounds\n    FirstPass --&gt; ModifyAddresses: Out of bounds\n    \n    ModifyAddresses --&gt; SecondPass\n    \n    SecondPass --&gt; StoreSecondPassResults: In bounds\n    SecondPass --&gt; Evaluate: Out of bounds\n    \n    StoreResults --&gt; Evaluate\n    StoreSecondPassResults --&gt; Evaluate\n    \n    Evaluate --&gt; ManualGeocode: Handful left\n    Evaluate --&gt; TryDifferentEngine: Many left\n    \n    ManualGeocode --&gt; FinalResults\n    TryDifferentEngine --&gt; FinalResults\n    \n    FinalResults --&gt; [*]\n\n\n\n\n\n\n\n1. First Pass\n\nObjective: To attempt an initial conversion of place names or addresses into geographical coordinates.\nExample: If you have a list of street addresses in Sweden, you use a geocoding tool to convert these addresses into geographical coordinates.\nOutcome: Some addresses will be successfully geocoded within the reasonable bounds (i.e., within Sweden), and some may not be (i.e., places outside Sweden).\n\n\n\n2. Modify Addresses\n\nObjective: To modify the addresses that were not successfully geocoded in the first pass.\nExample: If an address, “123 Main St, Stockholm,” fails in the first pass, you might modify it to just “Stockholm” or append the country, “123 Main St, Stockholm, Sweden.”\nOutcome: The modified addresses are ready for the second pass.\n\n\n\n3. Second Pass\n\nObjective: To attempt the conversion again with the modified addresses.\nOutcome: More addresses will be successfully geocoded within the reasonable bounds.\n\n\n\n4. Evaluate Results\n\nObjective: To assess the number of addresses that are left un-geocoded after the second pass.\nOutcome: Determine whether manual geocoding or trying a different geocoding engine is needed.\n\n\n\n5. Manual Geocode or Try Different Engine\n\nObjective: To geocode the remaining un-geocoded addresses.\nExample: If there are only a few addresses left, you might geocode them by hand using online maps. If there are many addresses left, you might switch to a different geocoding engine, e.g., from Open Street Map to Google Maps.\nOutcome: All addresses are successfully geocoded.\n\n\n\n6. Final Results\n\nObjective: To obtain a complete list of geographical coordinates corresponding to the initial list of place names or addresses.\nOutcome: A successfully geocoded list is ready for further use or analysis."
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#footnotes",
    "href": "posts/2023-10-02-geocoding/geocoding.html#footnotes",
    "title": "Geocoding: two ways",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nYou can also use the Google Maps Geocoding API in R through the tidygeocoder package, but you will still need to sign up for an API key from Google.↩︎"
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html",
    "title": "Swedish trade in goods and exchange students",
    "section": "",
    "text": "In a presentation last week organized by the Laboratory for the Economics of Africa’s Past I learned about the persistence of economic connectivity between areas from the Roman era until today. It was fascinating. The presenter, Prof. Erik Hornung, mentioned offhand that the choice of where to spend a semester exchange was likely a function of the economic and social connections between your home country and your host country – whether you know someone there, have family, or are familiar due to cultural similarities.\nI am in Sweden this semester, on an exchange at Lund University, in the South of the country. This is due in part to my supervisor’s connections with the economic historians here.\nI wanted to see if I could back up this assertion about exchange students with some data. More specifically, I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund from there, this semester.\nAs you see in the graphic below, there appears to be a strong positive correlation between Swedish imports of goods and imports of exchange students at Lund.\n\ndf_plotly &lt;- read_rds(\"data/Sweden_trade_exchange/plotly_data.rds\")\n\nc &lt;- df_plotly %&gt;% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent)) +\n  geom_smooth(group = 1, method = \"lm\", se = F) +\n  geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(title = \"Correlation between Sweden's imports of goods and exchange students\",\n       x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"Population\")\n\nc\n\n\n\n\nThe most populous European nations of Germany and France lead the rankings, with 46 and 37 exchange students respectively. The largest non-European senders are China and India, followed by the United States. South Africa has the largest representation for Africa, with 4 students.\nRead on to see the creation of the graphic."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#purpose",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#purpose",
    "title": "Swedish trade in goods and exchange students",
    "section": "",
    "text": "In a presentation last week organized by the Laboratory for the Economics of Africa’s Past I learned about the persistence of economic connectivity between areas from the Roman era until today. It was fascinating. The presenter, Prof. Erik Hornung, mentioned offhand that the choice of where to spend a semester exchange was likely a function of the economic and social connections between your home country and your host country – whether you know someone there, have family, or are familiar due to cultural similarities.\nI am in Sweden this semester, on an exchange at Lund University, in the South of the country. This is due in part to my supervisor’s connections with the economic historians here.\nI wanted to see if I could back up this assertion about exchange students with some data. More specifically, I wanted to see if there was an association between the volume of trade between a country and Sweden, and the number of exchange students that have come to Lund from there, this semester.\nAs you see in the graphic below, there appears to be a strong positive correlation between Swedish imports of goods and imports of exchange students at Lund.\n\ndf_plotly &lt;- read_rds(\"data/Sweden_trade_exchange/plotly_data.rds\")\n\nc &lt;- df_plotly %&gt;% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent)) +\n  geom_smooth(group = 1, method = \"lm\", se = F) +\n  geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(title = \"Correlation between Sweden's imports of goods and exchange students\",\n       x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"Population\")\n\nc\n\n\n\n\nThe most populous European nations of Germany and France lead the rankings, with 46 and 37 exchange students respectively. The largest non-European senders are China and India, followed by the United States. South Africa has the largest representation for Africa, with 4 students.\nRead on to see the creation of the graphic."
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#data",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#data",
    "title": "Swedish trade in goods and exchange students",
    "section": "Data",
    "text": "Data\nTo answer my question, I link two pieces of data: trade statistics on Sweden’s imports, and the nationalities of the incoming exchange students at Lund.\n\nTrade data\nThe first I downloaded from an amazing trade tool called the Observatory of Economic Complexity (Linked here). It has wonderful visualizations of trade by type of goods and by country. Have a look at this tree map below for an example.\n\n\n\n\nTree Map\n\n\nThe tree map shows the origins of Sweden’s imports by value in 2018. Sweden cares about limiting carbon emissions, and so it makes sense that the majority of their imports are sourced within Europe.\n\n\n\nData on exchange students\nThe second data source is a rough proxy for the nationalities of my colleagues. I collected phone numbers from a big WhatsApp group called “Lund University ’21” and extract the international dialing codes. There are several shortcomings to this data source – self-selection into WhatsApp use may differ by country, as might the desire to be part of a large group. Further, WhatsApp groups are limited in size at 256 members, just more than half the total number of exchange students at Lund this semester. My sample is unlikely to perfectly represent my population of interest, but it is a good enough starting point.\n\nData processing\nIn the chunks of code below I scrape a list of international dialing codes from the web, along with other country level information including GDP per capita and land area.\n\n# website with data on dialing codes\nurl &lt;- \"https://countrycode.org/\"\n\n# scrape table with rvest\ntable &lt;- read_html(url) %&gt;% \n  html_nodes(\"table\") %&gt;% \n  html_table()\n\n# processing\ntable &lt;- table[[1]] %&gt;% as_tibble(.name_repair = \"minimal\") %&gt;% unnest()\n\ntable &lt;- table %&gt;%  as_tibble() %&gt;% janitor::clean_names()\n\n# cleaning names and formatting columns as numbers rather than characters\ntable_df &lt;- table %&gt;% \n  mutate(population = parse_number(population),\n         area_km2 = parse_number(area_km2),\n         gdp_usd = parse_number(gdp_usd),\n         gdp_usd = gdp_usd*10e9)\n\nHere is a table of the data scraped from countrycode.org.\n\n# display table\ndatatable(table_df)\n\n\n\n\n\n\nWe can show the regions of the world by the first digit of their dialing code in a neat map. Expand the chunk by clicking code to see how easy it is to make an interactive graphic with ggplotly.\n\n# packages for map plotting and matching countries.\np_load(rnaturalearth, countrycode)\n\n# extracting first digit of dialing code\ntable_df &lt;- table_df %&gt;% \n  mutate(iso_a3 = countrycode(country, origin = \"country.name\", destination = \"iso3c\")) %&gt;% \n  mutate(first_digit = substring(country_code, 1, 1))\n\n# creating dataframe with mapping geometry\nworld &lt;- ne_countries(scale = \"medium\", returnclass = \"sf\")\n\n# joining up to table of dialing codes\nmap_df &lt;- world %&gt;% as_tibble() %&gt;% left_join(table_df, by = \"iso_a3\")\n\n# creating plot\na &lt;- map_df %&gt;% \n  filter(!is.na(first_digit)) %&gt;% \n  ggplot(aes(geometry = geometry, fill = first_digit)) +\n  geom_sf() +\n  scale_fill_brewer(palette = \"Set3\") +\n  labs(title = \"Countries coloured by first digit of international dialing code\",\n       fill = \"First digit\")\n\n# display interactive plot\nggplotly(a)\n\n\n\n\n\n\n\nDialing codes\nThe trickiest part was matching a country name with an international dialing code from the number alone. I used Google’s open source library for parsing, formatting, and validating international phone numbers. It is written in Java but someone has kindly written a wrapper for R. I show the process in the code chunk below, but do not display the phone numbers themselves for privacy reasons.\n\n# Sys.setenv(JAVA_HOME='C:\\\\Program Files\\\\Java\\\\jre1.8.0_281')\n# install.packages(\"dialrjars\", INSTALL_opts = c(\"--no-multiarch\"))\n\nlibrary(dialrjars)\nlibrary(dialr)\n\ndf &lt;- read_excel(\"data/Sweden_trade_exchange/Whatsapp numbers.xlsx\")\n\n# uniform formatting of numbers\ndf &lt;- df %&gt;% \n  mutate(clean = ifelse(str_detect(raw, \"\\\\+.*\"), raw, str_c(\"+\", raw))) %&gt;% select(number = clean)\n\ndf &lt;- df %&gt;% \n  mutate(phone = phone(number, \"SE\"))\n\n# get the region from number\ndf &lt;- df %&gt;% \n  mutate(region = get_region(phone))\n\n# count number of phone numbers per country\ncounts &lt;- df %&gt;% count(region, sort = T)\n\n# match name of country to two letter country code\ncounts &lt;- counts %&gt;% \n  mutate(name = countrycode(region, origin = \"iso2c\", destination = \"country.name\"),\n         iso_a3 = countrycode(region, origin = \"iso2c\", destination = \"iso3c\"))\n\ndatatable(counts)\n\n\n\n\n\n\nA quick plot of counts. Wow! Look at that. Go Germany!\n\ndf_col &lt;- counts %&gt;% \n  left_join(world %&gt;% as_tibble(), by = \"iso_a3\") %&gt;% \n  rename(name = name.x)\n\ndf_col %&gt;%   \n  filter(name != \"Sweden\") %&gt;% \n  mutate(name = fct_reorder(name, n),\n         continent = factor(continent, levels = c(\"Europe\", \"Asia\", \"North America\", \"Africa\", \"South America\"))) %&gt;% \n  ggplot(aes(n, name, fill = continent)) +\n  geom_col() +\n  scale_fill_brewer(palette = \"Dark2\") +\n  labs(title = \"Number of exchange students in Lund Uni exchange WhatsApp group\",\n       subtitle = \"By country\",\n       y = \"\",\n       x = \"Number of exchange students\",\n       fill = \"Continent\")"
  },
  {
    "objectID": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#findings",
    "href": "posts/2021-03-16-swedish-trade-in-goods-and-exchange-students/swedish-trade-in-goods-and-exchange-students.html#findings",
    "title": "Swedish trade in goods and exchange students",
    "section": "Findings",
    "text": "Findings\n\nPlotting the correlation between imports of goods to Sweden and exchange students to Lund\nI plot the correlation between the value of Sweden’s imports in 2018 on the x-axis, and the number of exchange students on the y-axis. Both axes are on a log scale.\nBelow is an interactive version of the static plot above.\n\nc &lt;- df_plotly %&gt;% \n  ggplot(aes(trade_value, exchange_students)) +\n  geom_point(aes(size = pop_est, colour = Continent, text=paste(\"Country:\", name))) +\n  geom_smooth(group = 1, method = \"lm\", se = T) +\n  scale_y_log10() +\n  scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n  scale_size_continuous(labels = scales::comma_format()) +\n  scale_color_brewer(palette = \"Paired\") +\n  geom_hline(yintercept = 0, lty = 2) +\n  labs(x = \"Value of imports into Sweden in 2018 (USD)\",\n       y = \"Number of exchange students in Lund Uni Whatsapp group\",\n       size = \"\",\n       colour = \"\")\n\n\nggplotly(c)\n\n\n\n\n# static figure\n\n\n# map_df_short &lt;- map_df %&gt;%\n#   select(iso_a3, continent, pop_est, gdp_md_est)\n# \n# jpeg(\n#   filename= \"data/Sweden_trade_exchange/reg_2.jpg\",\n#   width = 8,\n#   height = 6,\n#   units = \"in\",\n#   res = 500\n# )\n# \n# df %&gt;%\n#   inner_join(map_df_short) %&gt;%\n#   filter(iso_a3 != \"SWE\",\n#          Continent != \"Antarctica\") %&gt;%\n#   rename(exchange_students = n) %&gt;%\n#   ggplot(aes(trade_value, exchange_students)) +\n#   geom_point(aes(size = pop_est, colour = Continent)) +\n#   geom_smooth(group = 1, method = \"lm\", se = F) +\n#   geom_text_repel(aes(trade_value, exchange_students, label = name), alpha = .7, cex = 3) +\n#   scale_y_log10() +\n#   scale_x_log10(labels = dollar_format(), limits = c(100000,NA)) +\n#   scale_size_continuous(labels = scales::comma_format()) +\n#   scale_color_brewer(palette = \"Paired\") +\n#   geom_hline(yintercept = 0, lty = 2) +\n#   labs(x = \"Value of imports into Sweden in 2018 (USD)\",\n#        y = \"Number of exchange students in Lund Whatsapp group\",\n#        size = \"Population\")\n# \n# dev.off()\n\nThank you for reading along!"
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html",
    "href": "posts/2021-04-03-container-ships/container-ships.html",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "This week we learned about a data leak at Facebook which took place in 2019, where more than 500 million phone numbers, email addresses and names were scraped from the site and leaked online. Then, on Thursday we heard about another 500 million records including names, email addresses and more personal details were scraped from Linkedin, though the company argues that this was not a data breach. If you want to learn more about scraping, and get in on the (non-nefarious) action yourself, read along. As a bonus you will learn about how cargo ships have become so large.\nIn this post I want to show how easy it is to scrape data from the internet. It is the first post in a series looking at ships. I walk through scraping data from Wikipedia, one of the best places on the internet to ingest tabular data from.\nBefore we begin with the scraping walkthrough, I want to visualize the data and show how cargo ships have become larger over time.\n\n\nThe small multiples plot below shows the evolution of cargo ship size from 1870 to today. On the x-axis is the length of the average cargo ship per decade from Wikipedia’s list of cargo ships1. On the y-axis is the average ship beam, or width at the widest point. The colour of the rectangle shows the deadweight tonnage of the average ship, or amount of cargo that the ship can carry.\nDeadweight tonnage or tons deadweight is a measure of how much weight a ship can carry. It is the sum of the weights of cargo, fuel, fresh water, ballast water, provisions, passengers, and crew.\n\n\n\n\n\nCargo ships have increased dramatically in size over time! The oldest ship in our dataset is the R. J. Hackett, one of the first Great Lakes freighters. It was just 63m long and 10m wide, with a wooden hull. According to historian Mark Thompson, the R. J. Hackett’s boxy hull, hatch-lined deck, and placement of the deckhouses meant the ship was ideally suited for moving cargo through inland waterways. This steamer greatly influenced the development of cargo ships which followed.\nToday, container ships like the Ever Given are nearly 400m long, 60m wide, and can carry more than 20,000 twenty-foot containers. That’s enough space for 745 million bananas!\nTEUs, or twenty-foot equivalent units, is the measure that shipping companies use to compare volume. A TEU is 6.1m long, 2.4m wide and usually 2.6m high. Source: Wikipedia\n\n\n\nIn the plots below we focus only on container ships built after 1970. This era saw the construction of the first ships purpose built to carry ISO containers, which could be loaded and unloaded rapidly at port, repacked and shipped onward on any compatible ship, truck or train. The ISO standard container transformed the shipping industry and replaced the prior status quo of break bulk carriers. One of the consequences was a dramatic reduction in demand for “stevedores” or “longshoremen”; workers who would manually unload cargo from break bulk carriers.\nIf you’re interested in containerization, I highly reccomend this episode from the podcast Trade Talks, and this eight-part series from Alexis Madrigal.\n\n\n\n\nHow have cargo ship deadweight tonnages, or how much cargo a ship can carry, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships can carry more cargo today than ever before. It’s hard to get my mind around 220 000 tons of cargo!\n\n\n\nHow has the length of cargo ships changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nThe Ever Given is among the longest container ships operating today at 400m in length. The linear fit line shows that there has been a steady increase in container ship length over time.\n\n\n\nHow have cargo ship beams, or widths of ships at their widest point, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships have also become wider, with lumping at beams of 32m, 40m and 59m.\n\n\n\n\nSo it certainly seems that cargo ships have been becoming larger over time. Interestingly, it appears that while the largest container ships continue to get larger and carry more cargo, there is still a need for relatively small ships. There are a significant number of container ships that can carry less than 50 000 tons launched since 2010, shown in the density plot below. We could say that there has been a bifurcation in ship capacity, with a few enormous ships, and a greater number of smaller ships operating in tandem today.\n\n\n\n\n\n\n\n\nEconomies of scale describes a decrease in the per unit cost as scale of an operation increases. This perfectly fits the shipping industry’s relentless path towards upsizing ships, cranes and ports. One of the reasons has to do with fluid dynamics. Hull resistance is one of the key factors impacting fuel consumption. For vessels in water, drag loss is less than proportional to increasing cargo carried. In other words, making a ship larger usually results in less fuel consumption per ton of cargo, holding all else constant. As we have seen in the figures above, container ships have become larger and larger as they carry more cargo per ship, in an effort to save fuel.\nOther methods of drag reduction include improved hull design, injecting air around the hull surface and reducing hull roughness from slime and weeds. See Resistence and powering of ships \nAccording to Marc Levinson, author of The Box: How the Shipping Container Made the World Smaller and the World Economy Bigger, the shippers applied this idea to every element of the industry. He says:\n\nBigger ships lowered the cost of carrying each container. Bigger ports with bigger cranes lowered the cost of handling each ship. Bigger containers — the 20-foot box, shippers’ favorite in the early 1970s, was yielding to the 40-footer — cut down on crane movements and reduced the time needed to turn a vessel around in port, making more efficient use of capital. A virtuous circle had developed: lower costs per container permitted lower rates, which drew more freight, which supported yet more investments in order to lower unit costs even more. If ever there was a business in which economies of scale mattered, container shipping was it.\n\nThe consequences of containerization are fascinating – including rapidly falling costs of trade, increasingly intermeshed global supply chains, a proliferation of robots at ports, and the environmental challenges associated with ships, trucks and trains meeting at transshipping nodes around the world.\nIn the remainder of this post I walk through scraping some of the data presented above."
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#tonnage-over-time",
    "href": "posts/2021-04-03-container-ships/container-ships.html#tonnage-over-time",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "How have cargo ship deadweight tonnages, or how much cargo a ship can carry, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships can carry more cargo today than ever before. It’s hard to get my mind around 220 000 tons of cargo!"
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#length-over-time",
    "href": "posts/2021-04-03-container-ships/container-ships.html#length-over-time",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "How has the length of cargo ships changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nThe Ever Given is among the longest container ships operating today at 400m in length. The linear fit line shows that there has been a steady increase in container ship length over time."
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#width-over-time",
    "href": "posts/2021-04-03-container-ships/container-ships.html#width-over-time",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "",
    "text": "How have cargo ship beams, or widths of ships at their widest point, changed over time? Mouse over a point to see the name of the ship.\n\n\n\n\n\n\nContainer ships have also become wider, with lumping at beams of 32m, 40m and 59m."
  },
  {
    "objectID": "posts/2021-04-03-container-ships/container-ships.html#footnotes",
    "href": "posts/2021-04-03-container-ships/container-ships.html#footnotes",
    "title": "Shipping and Scraping - Part 1 in a Series on Shipping",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nI focus on all cargo ships until the 1970s, whereafter I focus only on container ships. Additional data on container ships is scraped from VesselFinder.com↩︎"
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "",
    "text": "Data visualisation is super fun and a great tool for communication. I have put together some slides for talk on data visualisation for economic historians at the Laboratory for the Economics of Africa’s Past. These resources are collected here to help when you think ‘where do I start with R?’.\nI hope that you find these resources useful. They are divided up into different sections in the tabs below.\n\n\nThere are a multitude of different pieces of software and different workflows that you can use to create beautiful and intelligible data visualizations.\nI think R is a very good choice. There are three reasons I love R for data viz.\n\nggplot2 is a fantastic library for making data visualizations - it is customizable and logical.\nThe R community is very supportive and makes learning and improving a pleasure.\nThe entire process of importing and cleaning data, visualizing it, and then sharing it is made easy in Rstudio."
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#purpose",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#purpose",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "",
    "text": "Data visualisation is super fun and a great tool for communication. I have put together some slides for talk on data visualisation for economic historians at the Laboratory for the Economics of Africa’s Past. These resources are collected here to help when you think ‘where do I start with R?’.\nI hope that you find these resources useful. They are divided up into different sections in the tabs below.\n\n\nThere are a multitude of different pieces of software and different workflows that you can use to create beautiful and intelligible data visualizations.\nI think R is a very good choice. There are three reasons I love R for data viz.\n\nggplot2 is a fantastic library for making data visualizations - it is customizable and logical.\nThe R community is very supportive and makes learning and improving a pleasure.\nThe entire process of importing and cleaning data, visualizing it, and then sharing it is made easy in Rstudio."
  },
  {
    "objectID": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#resources",
    "href": "posts/2021-05-21-ggplot-resources/ggplot-resources.html#resources",
    "title": "Resources for Learning R and Mastering ggplot",
    "section": "Resources",
    "text": "Resources\nIn the tabs below, I have collected some resources that I hope will be beneficial on your journey.\n\n\nInstalling R\n\nGetting started\nBefore we can do the fun stuff, we need to install R and RStudio.\nThe Modern Dive textbook has a great primer on getting started in R with an explanation of the difference between R and RStudio:\n\n\n\nModern Dive analogy\n\n\nThe Rstudio Education team also have a great beginner guide to R that covers installation.\n\n\n\nLearning R\n\nVideos and textbooks for learning how to use R\n\nThe R bible\nThe definitive guide to using R in a modern manner is R for Data Science by Hadley Wickham and Garrett Grolemund. I am not sure that is the best place to start, but it is certainly a helpful resource to come back to once you’re up and running.\n\n\n\nThe R for Data Science textbook\n\n\n\n\nRStudio Education\nRstudio Education have put together a set of instructions for learning R at different levels of competency. These include great guides for installing R, R Studio and the tidyverse packages right at the beginning.\nThe beginner, intermediate and expert start guides are linked here.\nThe RStudio team have put a lot of effort into creating resources that are useful for many R users, from helping teachers take their course work online, to financial analysis, to creating a personal website and academic publishing. Have a look around their website for more.\n\n\n\nRstudio Education graphics\n\n\n\n\nOther great video resources\nI am a visual learner and as a consequence I like to watch how things are done and copy them. Because Youtube allows for pausing and recapping at your own pace, I think this is a great way to learn. I’ve collected some videos in these resources which I think are a good place to start.\nDavid Robinson’s Youtube Channel\nDavid Robinson does a weekly screencast where he shows how to do exploratory data analysis and makes plots in R.\nHere is a video about analysing text from reviews of ramen noodles. It has some useful tips for drawing tie-fighter plots and interpreting linear regressions.\n\n\nHe also has a talk about tricks in the tidyverse.\n\n\nJulia Silge’s blog\nJulia Silge’s blog is mostly about doing modelling in R, and text modelling in particular. It is a very useful place to look if you want to understand the tidymodels framework which provides access to a host of machine learning techniques in a very easy to use manner. She also has a Youtube channel.\nHere is a video from Julia looking at sentiment analysis in R:\n\n\nThe tidy text mining textbook\nWritten by Julia Silge and David Robinson, this book gives a lovely introduction to text mining in R. It has lots of examples and includes all the code necessary. As a bonus it is free!\n\n\n\nTidy Text Mining Textbook\n\n\nAndrew Couch’s Youtube Channel has a marvelous set of walk throughs of innovative and exciting packages in R. He’s a young data scientist who is creating fantastic content focussed on R and the tidyverse.\nHere is one of his videos on creating functions with the tidyverse.\n\n\n\n\nAnother resource collection!\nThe LearnR4Free website compiled by Mine Dogucu has a great set of resources that are free to use for the learner.\n\n\n\n\nDatasets\n\nUseful sources of data to practice your skills on\nHaving great datasets available helps to keep me interested in learning new skills. Here are three sources of interesting data:\nThe Tidy Tuesday Project\nTidy Tuesday is a project run by the R for Data Science foundation - each week they share a real world data set and encourage R users to practice their skills. It has a really fantastic community aspect and if you share your work on Twitter people are always helpful and kind. Others share their work and their code so you can see how their plots were made. In their words,\n\nThe intent of Tidy Tuesday is to provide a safe and supportive forum for individuals to practice their wrangling and data visualization skills independent of drawing conclusions.\n\n\n\n\nTidy Tuesday\n\n\nData is Plural\n“Data is Plural is a weekly newsletter of useful/curious datasets, published by Jeremy Singer-Vine”. This sums it up - really interesting stuff from all over the internet.\n\n\n\nData is plural\n\n\nNaked Data\n“Naked Data is a weekly newsletter which rounds up the best in data-driven storytelling created in or about the Global South.” Get the best bits of twitter without having to spend all day scrolling. Really great newsletter based in SA.\n\n\n\nNaked Data\n\n\n\n\n\nVisualization\n\nGreat places to get inspiration, figures and code from\n\nFundementals of data visualization\nThere are many books on the theory and practice of data visualization. I think Fundamentals of Data Visualization by Claus Wilke is an excellent choice to start with. The preface reads:\n\nIf you are a scientist, an analyst, a consultant, or anybody else who has to prepare technical documents or reports, one of the most important skills you need to have is the ability to make compelling data visualizations, generally in the form of figures. Figures will typically carry the weight of your arguments. They need to be clear, attractive, and convincing.\n\nThe book provides great advice for matching your data to a good chart type. For how to make the chosen chart in R, have a look at his course, linked below.\n\n\n\nThe best course in data viz\nClause Wilke’s Course on Data Visualization in R is a fantastic accompaniment to the book linked above. The description reads:\n\nIn this class, students will learn how to visualize data sets and how to reason about and communicate with data visualizations. A substantial component of this class will be dedicated to learning how to program in R. In addition, students will learn how to compile analyses and visualizations into reports, how to make the reports reproducible, and how to post reports on a website or blog.\n\nIt has wonderful sets of slides including this delightful primer on visualizing uncertainty, and accompnaying code that can be downloaded and worked through. Bonus: all of the slides are hosted on github such that you can see how they’re put together.\n\n\nThe ggplot2 bible\nThe definitive guide to ggplot2, The ggplot2 Book is a fantastic resource for novices and experts alike. It is easy to navigate and has helpful examples with code used to create each figure. (I love to consult the section on annotations as I never remember exactly how they work for some reason.)\n\n\n\nThe ggplot2 book\n\n\n\n\nOther great viz resources\nThe Storytelling with Data blog has an excellent selection of articles that give tips and tricks on how to design graphics that are easy to understand.\nTheir Chart Guide is also a fantastic starting point for how to pick a chart.\n\n\n\nSWD Chart Guide\n\n\nJonathan Tan’s exploration of Household Solar Energy in the US is an amazing data storytelling piece that shows all of the code required to create each figure. It makes great use of annotations to explain each component of the visualizations.\n\n\n\nJonathan Tan’s ggridge plot\n\n\nThe R Graph Gallery is a useful website which showcases different kinds of visualizations and provides the code that you need to use to recreate the figure yourself.\n\n\n\nR Graph Gallery\n\n\n\n\n\n\nCustom themes\n\nFor style that is consistent and graphics that are clear\nIf you want to create a style of graphics that communicate clearly to the general public (or are consistent across your organization), have a look at the BBC Visual and Data Journalism cookbook for R graphics. The guide is filled with fantastic examples of visualizations you might like to emulate, and code to do so. They also walk through customizing fonts, colours and annotations for consistency over time.\n\n\n\nBBC style graphics\n\n\nIf you want to create your own theme, Thomas Mock has a lovely guide on graphs that communicate well and look good.\nHe walks through a great example of where adding custom images on the plot makes sense:\n\n\n\nNFL team images example\n\n\n\n\n\nAesthetics\n\nIf you’re interested in learning more about how to make your charts look great.\nWilliam Chase’s talk on ‘The Glamour of Graphics’ is a fantastic resource on choosing colours and how to make your graphs pop.\n\n\nStephen Tracy’s breakdown of DataViz Sins You Should Never Commit is a walk-through on how not to confuse your audience. It explains the use of colour and secondary axes in Microsoft Excel plots.\nIf you are interested in learning more about fonts, Practical Typography is a great place to start. If you’re interested in choosing a font for a website, have a look at Google’s free fonts here.\n\n\n\nLato is a free Google font\n\n\nThis page from the R Graph Gallery gives a breakdown of the RcolorBrewer package and when to use its different sets of palettes.\n\nThe color brewer webpage gives fantastic advice on how to choose colours for maps, using the same theory as the package above. It also gives you the hex codes for each colour that you may copy to use yourself.\n\nThis video on colour theory is a useful primer on colour in data viz.\n\n\n\n\n\nEconometrics in R\n\nIf you’re looking to implement econometric techqniques in R\nGrant McDermott’s Data science for economists course is a great place to start. He has one lecture on regression that is super useful.\n\n\n\nGrant McDermott\n\n\n\n\n\nR Shiny\n\nIf you’re looking to make interactive web pages with R and R Markdown\nThe simplest way to get a quick and dirty interactive dashboard together is to use the flexdashboard package. Here is a link to the package. One of the great advantages of flexdashboard is that you can get amazing interactivity with just htmlwidgets, you do not need to use a shiny-runtime. In other words, you can create and host a static html page anywhere, for example on GitHub Pages.\n\n\n\nFlexdashboard\n\n\nIf you want more interactivity, David Robinson’s video on global crop yields shows a simple Shiny app, with code here.\n\n\nAnother video example of R Shiny comes from Andrew Couch’s YouTube channel.\n\n\nMastering Shiny is a more advanced authoritative book on Shiny, written by Hadley Wickham. It’s a great place to get an understanding of the core of the package.\n\n\n\nMastering Shiny\n\n\n\n\n\nRecreating Published Figures\n\nWhen you can’t get the authors of an article to share their data\nSometimes you may want to recreate a figure for your own paper from a journal article, but the data is not publicly accessible.\nOne option is to snip the image from the article and cite it in your own. The disadvantage of this is that the figure will not fit your colour scheme and may be horribly pixalated.\nAn alternative is to recreate the figure yourself by extracting the data from the image.\nA truly fantastic free tool for this purpose is WebPlotDigitizer. This web application allows you to upload a figure and grab the data from it.\nHave a look at a short video I made on how to use it here:"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Fathom data are an analytics and data science training firm which offers training in two of the most popular open source languages for statistical and general purpose programming: #Rstats and #Python. In their words:\n\n\nWe use 100% #opensource tools and we contribute to a number of Open Source projects. There's no doubt that Open Source adds value to @fathomdata, but it's hard to know what value we're creating. Some interesting perspectives on quantifying value.https://t.co/hcw5hylTwn\n\n— Fathom Data (@fathomdata) January 24, 2022\n\n\nSome of the staff state a preference for R, and I wanted to see if this preference was borne out in the tweets on their corporate twitter account.\n\n\nTo quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n# get the tweets!\ndf &lt;- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\n\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew’s account.\n\ndf %&gt;% \n    count(screen_name) %&gt;% \n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\n\n\n\n\nAccount\nNumber of tweets\n\n\n\n\ndatawookie\n3198\n\n\nfathomdata\n448\n\n\n\n\n\n\n\n\nLet’s start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\n\nhashtags &lt;- df %&gt;%\n  filter(screen_name == \"fathomdata\") %&gt;%\n  unnest(cols = c(hashtags)) %&gt;%\n  filter(!is.na(hashtags)) %&gt;%\n  mutate(hashtags = str_to_lower(hashtags)) %&gt;%\n  count(hashtags, is_retweet) %&gt;%\n  mutate(is_retweet = if_else(is_retweet == TRUE,\n    \"Retweet\",\n    \"@fathomdata tweet\"\n  ))\n\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\nI’ve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\n\nlibrary(tidytext)\n# ggsci package for nice blue fill scale\nlibrary(ggsci)\n\nhashtags %&gt;% \n  group_by(is_retweet) %&gt;% \n  slice_max(n, n = 12, with_ties = F) %&gt;% \n  ungroup() %&gt;% \n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %&gt;% \n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %&gt;% \n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\n  geom_col(show.legend = F) +\n  scale_fill_material(\"blue\") +\n  scale_color_manual(values = c(\"white\", \"red\")) +\n  scale_y_reordered() +\n  facet_wrap(~ is_retweet, scales = \"free\") +\n  labs(x = \"Number of tweets tagging\",\n       y = \"Hashtag\",\n       caption = \"Note: red outline on bar indicates language\")\n\n\n\n\nColumn chart of hashtag frequency by tweet type\n\n\n\n\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\n\nhashtags %&gt;%\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %&gt;%\n  mutate(hashtags = fct_reorder(hashtags, n),\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %&gt;%\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\n  geom_col(position = \"fill\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Share of tweets tagging langauge\",\n    fill = \"Language\",\n    y = NULL\n  ) +\n  scale_fill_npg(\n    guide = guide_legend(reverse = TRUE)\n  )\n\n\n\n\nStacked column chart comparing language hashtag frequency by tweet type\n\n\n\n\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\n\ndf %&gt;%\n  select(text) %&gt;%\n  mutate(\n    rstats = str_detect(text, \"rstats\"),\n    python = str_detect(text, \"python\")\n  ) %&gt;%\n  summarise(across(c(rstats, python), sum)) %&gt;%\n  pivot_longer(everything(),\n    names_to = \"language\",\n    values_to = \"n\"\n  ) %&gt;% \n    mutate(share = scales::percent(n / (496 + 134)))\n\n# A tibble: 2 × 3\n  language     n share\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;\n1 rstats     496 79%  \n2 python     134 21%  \n\n\n\n\n\nIn the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\n\ndf %&gt;%\n  filter(\n    str_detect(text, \"rstats\"),\n    screen_name == \"fathomdata\",\n    !is.na(retweet_screen_name)\n  ) %&gt;%\n  count(retweet_name, sort = T) %&gt;%\n  slice_head(n = 12) %&gt;%\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\n\n\n\n\nAccout name\nNumber of retweets\n\n\n\n\nAndrew B. Collier\n43\n\n\nR-bloggers\n19\n\n\nMegan Beckett\n16\n\n\nR posts you might have missed!\n10\n\n\nsatRday South Africa\n6\n\n\nMara Averick\n5\n\n\nAstrid Radermacher\n3\n\n\nRStudio\n3\n\n\nTom Mock\n3\n\n\nFathom Data\n2\n\n\nR-Ladies Cape Town\n2\n\n\nR Weekly Live\n2\n\n\n\n\n\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap.\n\n\n\nFollowing the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\n\n# taking out URLs\nremove_bits &lt;- \"&amp;|&lt;|&gt;\"\n\ntidy_tweets &lt;- df %&gt;%\n  filter(!str_detect(text, \"^RT\")) %&gt;%\n  mutate(text = str_remove_all(text, remove_bits)) %&gt;%\n  unnest_tokens(word, text, token = \"tweets\") %&gt;%\n    # remove stopwords\n  filter(\n    !word %in% stop_words$word,\n    !word %in% str_remove_all(stop_words$word, \"'\"),\n    str_detect(word, \"[a-z]\")\n  ) \n\nfrequency &lt;- tidy_tweets %&gt;% \n  group_by(screen_name) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(tidy_tweets %&gt;% \n              group_by(screen_name) %&gt;% \n              summarise(total = n())) %&gt;%\n  mutate(freq = n/total)\n\nfrequency &lt;- frequency %&gt;% \n  select(screen_name, word, freq) %&gt;% \n  pivot_wider(names_from = screen_name, values_from = freq) %&gt;%\n  arrange(desc(datawookie), fathomdata)\n\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew’s tweets.\n\nfrequency %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  theme(legend.position = \"none\") +\n    labs(x = \"Andrew Collier\",\n         y = \"Fathom Data\")\n\n\n\n\nComparing the frequency of words used by the Fathom Data account and Andrew Collier\n\n\n\n\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\n\nfrequency %&gt;% \n  mutate(highlight = word == \"@standardbankza\") %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\nHighlighting Standard Bank account mentions\n\n\n\n\nThis is clearly the case! Have a look at the top terms, haha!\n\nfrequency %&gt;%\n  filter(is.na(fathomdata)) %&gt;%\n  select(-fathomdata) %&gt;%\n  mutate(datawookie = scales::percent(datawookie)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\n\n\n\n\nWord\nRelative specificity to Andrew Collier’s tweets\n\n\n\n\nkak\n0.6723%\n\n\n#customerservice\n0.6666%\n\n\n@rainsouthafrica\n0.5759%\n\n\nmind\n0.5220%\n\n\nmbps\n0.4255%\n\n\nprepared\n0.3773%\n\n\nlinux\n0.3489%\n\n\n#itcanbe\n0.3262%\n\n\n#fixit\n0.2354%\n\n\nworst\n0.2241%\n\n\n\n\n\n\n\n\nIt appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats."
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#tweets",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#tweets",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "To quantify the references to each language, I used the rtweet package from Michael W. Kearney to mine the Fathom Data timeline, as well as those of founder Andrew Collier.\n\nlibrary(tidyverse)\nlibrary(rtweet)\n# get the tweets!\ndf &lt;- get_timelines(c(\"fathomdata\", \"datawookie\"), n = 3200)\n\nWe can see that we collected nearly 450 tweets on the Fathom Data account, and just shy of 3200 from Andrew’s account.\n\ndf %&gt;% \n    count(screen_name) %&gt;% \n    knitr::kable(col.names = c(\"Account\", \"Number of tweets\"))\n\n\n\n\nAccount\nNumber of tweets\n\n\n\n\ndatawookie\n3198\n\n\nfathomdata\n448"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#hashtags",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#hashtags",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Let’s start by looking at the hashtags used on the Fathom Data account, distinguishing between original tweets and retweets.\n\nhashtags &lt;- df %&gt;%\n  filter(screen_name == \"fathomdata\") %&gt;%\n  unnest(cols = c(hashtags)) %&gt;%\n  filter(!is.na(hashtags)) %&gt;%\n  mutate(hashtags = str_to_lower(hashtags)) %&gt;%\n  count(hashtags, is_retweet) %&gt;%\n  mutate(is_retweet = if_else(is_retweet == TRUE,\n    \"Retweet\",\n    \"@fathomdata tweet\"\n  ))\n\nIn the column chart below we can visualize the number of times the 12 most frequently used hashtags appear by type of tweet. Evidently original tweets include hashtags about both Rstats and Python, as well as SQL and CSS. The retweets skew heavily towards Rstats though.\nI’ve made use of the lovely reorder_within command in combination with the scale_y_reordered command, both from the tidytext package to reorder the hashtags by frequency within the type of tweet. This means that though the same terms appear across facets, they can be in a different order within each.\n\nlibrary(tidytext)\n# ggsci package for nice blue fill scale\nlibrary(ggsci)\n\nhashtags %&gt;% \n  group_by(is_retweet) %&gt;% \n  slice_max(n, n = 12, with_ties = F) %&gt;% \n  ungroup() %&gt;% \n  mutate(language = str_detect(hashtags, \"rstats|python|sql|css|markdown\")) %&gt;% \n  mutate(hashtags = reorder_within(hashtags, n, is_retweet)) %&gt;% \n  ggplot(aes(n, hashtags, fill = n, colour = language)) +\n  geom_col(show.legend = F) +\n  scale_fill_material(\"blue\") +\n  scale_color_manual(values = c(\"white\", \"red\")) +\n  scale_y_reordered() +\n  facet_wrap(~ is_retweet, scales = \"free\") +\n  labs(x = \"Number of tweets tagging\",\n       y = \"Hashtag\",\n       caption = \"Note: red outline on bar indicates language\")\n\n\n\n\nColumn chart of hashtag frequency by tweet type\n\n\n\n\nAnother way to compare these is the stacked column chart below, which shows the number of uses of hashtags for each language as a percentage of the total number of language tags. Rstats tags make us just less than half of the tweets posted to the Fathom Data account, but more than 90 percent of the retweet tags! Wow! It certainly seems like the person in charge of the twitter account enjoys sharing tweets about Rstats.\n\nhashtags %&gt;%\n  filter(hashtags %in% c(\"rstats\", \"python\", \"sql\", \"css\")) %&gt;%\n  mutate(hashtags = fct_reorder(hashtags, n),\n         is_retweet = fct_relevel(is_retweet, \"Retweet\")) %&gt;%\n  ggplot(aes(n, is_retweet, fill = hashtags)) +\n  geom_col(position = \"fill\") +\n  scale_x_continuous(labels = scales::percent_format()) +\n  labs(\n    x = \"Share of tweets tagging langauge\",\n    fill = \"Language\",\n    y = NULL\n  ) +\n  scale_fill_npg(\n    guide = guide_legend(reverse = TRUE)\n  )\n\n\n\n\nStacked column chart comparing language hashtag frequency by tweet type\n\n\n\n\nLooking at text instead of hashtags we see that Rstats make up 79 percent share of the langauge mentions of Rstats and Python.\n\ndf %&gt;%\n  select(text) %&gt;%\n  mutate(\n    rstats = str_detect(text, \"rstats\"),\n    python = str_detect(text, \"python\")\n  ) %&gt;%\n  summarise(across(c(rstats, python), sum)) %&gt;%\n  pivot_longer(everything(),\n    names_to = \"language\",\n    values_to = \"n\"\n  ) %&gt;% \n    mutate(share = scales::percent(n / (496 + 134)))\n\n# A tibble: 2 × 3\n  language     n share\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;\n1 rstats     496 79%  \n2 python     134 21%"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#who-is-being-retweeted",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#who-is-being-retweeted",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "In the table below we can see that several accounts which aggregate informative Rstats content feature in the top retweets on the Fathom Data feed. One of the reasons I love Rstats is that the community is so willing to share their knowledge. The Fathom Data feed is an example of this.\n\ndf %&gt;%\n  filter(\n    str_detect(text, \"rstats\"),\n    screen_name == \"fathomdata\",\n    !is.na(retweet_screen_name)\n  ) %&gt;%\n  count(retweet_name, sort = T) %&gt;%\n  slice_head(n = 12) %&gt;%\n  knitr::kable(col.names = c(\"Accout name\", \"Number of retweets\"))\n\n\n\n\nAccout name\nNumber of retweets\n\n\n\n\nAndrew B. Collier\n43\n\n\nR-bloggers\n19\n\n\nMegan Beckett\n16\n\n\nR posts you might have missed!\n10\n\n\nsatRday South Africa\n6\n\n\nMara Averick\n5\n\n\nAstrid Radermacher\n3\n\n\nRStudio\n3\n\n\nTom Mock\n3\n\n\nFathom Data\n2\n\n\nR-Ladies Cape Town\n2\n\n\nR Weekly Live\n2\n\n\n\n\n\nWe can also see that Fathom founder Andrew Collier appears at the top of the list of Rstats retweets from the Fathom account, with 43. The next section looks at how his feed and the Fathom Data feed overlap."
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#what-is-the-overlap-between-private-and-public",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#what-is-the-overlap-between-private-and-public",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "Following the fantastic resource Text Mining with R the code chunk below calculates the frequency with which each word used in the tweets of the Fathom Data accound and by Andrew Collier as a share of the total number of distinct words used.\n\n# taking out URLs\nremove_bits &lt;- \"&amp;|&lt;|&gt;\"\n\ntidy_tweets &lt;- df %&gt;%\n  filter(!str_detect(text, \"^RT\")) %&gt;%\n  mutate(text = str_remove_all(text, remove_bits)) %&gt;%\n  unnest_tokens(word, text, token = \"tweets\") %&gt;%\n    # remove stopwords\n  filter(\n    !word %in% stop_words$word,\n    !word %in% str_remove_all(stop_words$word, \"'\"),\n    str_detect(word, \"[a-z]\")\n  ) \n\nfrequency &lt;- tidy_tweets %&gt;% \n  group_by(screen_name) %&gt;% \n  count(word, sort = TRUE) %&gt;% \n  left_join(tidy_tweets %&gt;% \n              group_by(screen_name) %&gt;% \n              summarise(total = n())) %&gt;%\n  mutate(freq = n/total)\n\nfrequency &lt;- frequency %&gt;% \n  select(screen_name, word, freq) %&gt;% \n  pivot_wider(names_from = screen_name, values_from = freq) %&gt;%\n  arrange(desc(datawookie), fathomdata)\n\nThe visualization below shows the relative frequencies for terms used by both accounts. Words near the red line are used in about equal proportion by the Fathom account and by Andrew, while words above the line are more specific to the Fathom account and words below the line are more specific to Andrew.\nIn the top right of the plot we can see that #datascience, data and #Rstats are used frequently by both accounts, and roughly in equal proportions. Terms like courses, #sql and keynote are more specific to the Fathom account, while #linux and binance are more specific to Andrew’s tweets.\n\nfrequency %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  theme(legend.position = \"none\") +\n    labs(x = \"Andrew Collier\",\n         y = \"Fathom Data\")\n\n\n\n\nComparing the frequency of words used by the Fathom Data account and Andrew Collier\n\n\n\n\nInterestingly, standard bank stands out in the bottom right of the plot, highlighted in the version below. It seems that Andrew uses his personal account to call out poor customer service!\n\nfrequency %&gt;% \n  mutate(highlight = word == \"@standardbankza\") %&gt;% \n  ggplot(aes(datawookie, fathomdata)) +\n  geom_jitter(alpha = 0.1, size = 2.5, width = 0.25, height = 0.25) +\n  geom_text(aes(label = word, colour = highlight), check_overlap = TRUE, vjust = 1.5) +\n  scale_x_log10(labels = percent_format()) +\n  scale_y_log10(labels = percent_format()) +\n  geom_abline(color = \"red\") +\n  scale_color_manual(values = c(\"grey80\", \"midnightblue\")) +\n  theme(legend.position = \"none\")\n\n\n\n\nHighlighting Standard Bank account mentions\n\n\n\n\nThis is clearly the case! Have a look at the top terms, haha!\n\nfrequency %&gt;%\n  filter(is.na(fathomdata)) %&gt;%\n  select(-fathomdata) %&gt;%\n  mutate(datawookie = scales::percent(datawookie)) %&gt;%\n  slice_head(n = 10) %&gt;%\n  knitr::kable(col.names = c(\"Word\", \"Relative specificity to Andrew Collier's tweets\"))\n\n\n\n\nWord\nRelative specificity to Andrew Collier’s tweets\n\n\n\n\nkak\n0.6723%\n\n\n#customerservice\n0.6666%\n\n\n@rainsouthafrica\n0.5759%\n\n\nmind\n0.5220%\n\n\nmbps\n0.4255%\n\n\nprepared\n0.3773%\n\n\nlinux\n0.3489%\n\n\n#itcanbe\n0.3262%\n\n\n#fixit\n0.2354%\n\n\nworst\n0.2241%"
  },
  {
    "objectID": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#conclusion",
    "href": "posts/2022-01-24-fathom-twitter/fathom-twitter.html#conclusion",
    "title": "R vs Python? R & Python!",
    "section": "",
    "text": "It appears that Fathom has a good mix of open source languages in use, and on their twitter, with a slight skew towards Rstats."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html",
    "title": "LinkedIn style tables for personal websites",
    "section": "",
    "text": "Katherine asked me if she could include the university logo in her education section on the about page on her website. The about page is made with the postcards package. I didn’t know how to do it but thought that a table might make the most sense.\nI really liked the style of the LinkedIn education sections:\n\nknitr::include_graphics(\"images/linkedin_education.PNG\")\n\n\n\n\nLinkedIn screenshot\n\n\n\n\nIn this post I walk through recreating this table with the gt package in R."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#purpose",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#purpose",
    "title": "LinkedIn style tables for personal websites",
    "section": "",
    "text": "Katherine asked me if she could include the university logo in her education section on the about page on her website. The about page is made with the postcards package. I didn’t know how to do it but thought that a table might make the most sense.\nI really liked the style of the LinkedIn education sections:\n\nknitr::include_graphics(\"images/linkedin_education.PNG\")\n\n\n\n\nLinkedIn screenshot\n\n\n\n\nIn this post I walk through recreating this table with the gt package in R."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#where-to-begin",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#where-to-begin",
    "title": "LinkedIn style tables for personal websites",
    "section": "Where to begin?",
    "text": "Where to begin?\nThe education section in the LinkedIn table has six components for each entry. They are:\n\nA logo photo\nA hyperlink to the institution though the image\nName of the institution (in bold)\nTitle of the degree\nDate (start and end year)\nGrade or comment\n\nLet’s create a tibble with these components.\n\nlibrary(tidyverse)\n\neducation &lt;- tribble(\n  ~logo, ~link, ~institution, ~degree, ~date, ~grade_comment,\n  # LU\n  \"images/lu.jfif\", \"https://ehl.lu.se/ekonomisk-historia\", \"Lund University\", \"Master's degree, Economics\", \"2021\", \"Exchange semester\",\n  # SU\n  \"images/su.jfif\", \"https://www.ekon.sun.ac.za/\", \"Stellenbosch University\", \"Master's degree, Economics\", \"2020\", \"Grade: &lt;em&gt;cum laude&lt;/em&gt;\",\n  # uct hnours\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Honours degree, Economics\", \"2018\", \"Grade: 2:1\",\n  # uct undergrad\n  \"images/uct.jfif\", \"http://www.economics.uct.ac.za/\", \"University of Cape Town\", \"Bachelor's degree, Economics and Economic History\", \"2015-2017\", \"Grade: 2:1\",\n)"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#building-the-table",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#building-the-table",
    "title": "LinkedIn style tables for personal websites",
    "section": "Building the table",
    "text": "Building the table\nNow lets start on the table structure. What we’re gonna do is take the individual pieces and format them to html, then stick the text together in one cell and the logos/links in another.\n\neducation &lt;- education %&gt;%\n  mutate(\n    logo = glue::glue(\"&lt;a href = {link}&gt;\n                        &lt;img src='{logo}' width='50' height='50'&gt;\n                      &lt;/a&gt;\"),\n    institution = glue::glue(\"&lt;b&gt;{institution}&lt;/b&gt;\"),\n    date = glue::glue(\"&lt;span style='opacity: .7;'&gt; {date} &lt;/span&gt;\"),\n    grade_comment = glue::glue(\"&lt;span style='opacity: .7;'&gt; {grade_comment} &lt;/span&gt;\"),\n  )\n\nIn the cell above we create an image tag for the logo, putting a hyperlink tag for the link to the university website inside it. Here we use the glue package which is useful for working with strings constructed from other columns. You can think of it as gluing together the html tag parts and the content from our dataframe. Next we put the institution in bold, and grey out the dates and grades with the html style tag, 'opacity: .7;'.1\nNow we can stick these text components together into one cell (just called text) and keep only the logo and text columns.2 Finally we use the map function from the purrr package to transform the character strings into html. Printing the tibble shows that it now contains html elements inside each cell.\n\neducation &lt;- education %&gt;%\n  mutate(text = str_c(\n    institution, \"&lt;br&gt;\",\n    degree, \"&lt;br&gt;\",\n    date, \"&lt;br&gt;\",\n    grade_comment\n  )) %&gt;%\n  select(logo, text) %&gt;%\n  mutate(\n    logo = purrr::map(logo, gt::html),\n    text = purrr::map(text, gt::html)\n  )\n\neducation\n\n# A tibble: 4 × 2\n  logo       text      \n  &lt;list&gt;     &lt;list&gt;    \n1 &lt;html [1]&gt; &lt;html [1]&gt;\n2 &lt;html [1]&gt; &lt;html [1]&gt;\n3 &lt;html [1]&gt; &lt;html [1]&gt;\n4 &lt;html [1]&gt; &lt;html [1]&gt;\n\n\nNow we create the table by calling the gt() command.\n\nlibrary(gt)\n\neducation %&gt;% \n    gt()\n\n\n\n\n\n\n\n\n\n\n\n\nlogo\ntext\n\n\n\n\n\nLund University\nMaster's degree, Economics\n 2021 \n Exchange semester \n\n\n\nStellenbosch University\nMaster's degree, Economics\n 2020 \n Grade: cum laude \n\n\n\nUniversity of Cape Town\nHonours degree, Economics\n 2018 \n Grade: 2:1 \n\n\n\nUniversity of Cape Town\nBachelor's degree, Economics and Economic History\n 2015-2017 \n Grade: 2:1 \n\n\n\n\n\n\n\nGreat, there it is!"
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#finalizing-the-table",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#finalizing-the-table",
    "title": "LinkedIn style tables for personal websites",
    "section": "Finalizing the table",
    "text": "Finalizing the table\nWe still want to make some changes to tidy it up a bit:\n\nAdding a table header\nAligning the text to the left\nRemoving the column labels\n\n\neducation %&gt;%\n  gt() %&gt;%\n  tab_header(md(\"**Education**\")) %&gt;%\n  cols_align(align = c(\"left\"), columns = text) %&gt;%\n  cols_label(\n    logo = \"\",\n    text = \"\",\n  )\n\n\n\n\n\n  \n    \n      Education\n    \n    \n  \n  \n    \n      \n      \n    \n  \n  \n    \n  \n\nLund UniversityMaster's degree, Economics 2021  Exchange semester \n    \n  \n\nStellenbosch UniversityMaster's degree, Economics 2020  Grade: cum laude \n    \n  \n\nUniversity of Cape TownHonours degree, Economics 2018  Grade: 2:1 \n    \n  \n\nUniversity of Cape TownBachelor's degree, Economics and Economic History 2015-2017  Grade: 2:1 \n  \n  \n  \n\n\n\n\nLovely! Now we have the table we can use in the about page of a blog or personal website, with nice clickable logos that take you to the website of the institution."
  },
  {
    "objectID": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#footnotes",
    "href": "posts/2022-06-01-linkedin-style-about-info/linkedin-style-about-info.html#footnotes",
    "title": "LinkedIn style tables for personal websites",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nWe opt to use the &lt;span&gt; tag rather than the &lt;p&gt; tag because &lt;span&gt; does not introduce any extra space.↩︎\nWe use str_c from the stringr package for its nice syntax, we could have used glue here again↩︎"
  },
  {
    "objectID": "posts/2022-12-15-coef-plots/coef-plots.html",
    "href": "posts/2022-12-15-coef-plots/coef-plots.html",
    "title": "Coefficient plots in ggplot",
    "section": "",
    "text": "Recently a colleague asked how they could quickly draw a coefficient plot in R. In Stata, this is relatively simple - just run your regression and use the command coefplot afterwards. This produces a graphic that is perfectly acceptable for exploratory data analysis, but leaves something to be desired if you want to use it in a publication.\nThis post shows you how to draw coefficient plots in R and ggplot, and is extensible for use with regressions beyond the basic lm command.\n\nData\nFor this example we will use data from the lovely Palmer Penguins package from Allison Horst. The penguins dataset is a great toy dataset for exploration and visualization, based on genuine data collected by Dr. Kristen Gorman at the Palmer Station in Antarctica.\n\n\n\n\n\n\n\nBasic regression\nWe begin with a basic regression where our dependent variable is penguin body weight in grams, and independent variables are the dimensions of the penguins’ bills, flipper length, as well as species and sex.\n\nlibrary(tidyverse)\ntheme_set(theme_light())\n\n# read in data\ndf &lt;- palmerpenguins::penguins\n\n# basic regression\nbasic_reg &lt;- df %&gt;% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .)\n\nbasic_reg\n\n\nCall:\nlm(formula = body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + \n    species + sex, data = .)\n\nCoefficients:\n      (Intercept)     bill_length_mm      bill_depth_mm  flipper_length_mm  \n         -1460.99              18.20              67.22              15.95  \n speciesChinstrap      speciesGentoo            sexmale  \n          -251.48            1014.63             389.89  \n\n\nThis produces a rather messy output. It includes both the regression specification and the coefficients.\n\n\nTidy with broom package\nWe can use the broom package to return a tibble, a neat data object that is easy to work with.\n\nlibrary(broom)\n\ntidy_reg &lt;- df %&gt;% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %&gt;% \n    tidy()\n\ntidy_reg\n\n# A tibble: 7 × 5\n  term              estimate std.error statistic  p.value\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2\n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4\n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3\n6 speciesGentoo       1015.     130.        7.83 6.85e-14\n7 sexmale              390.      47.8       8.15 7.97e-15\n\n\nGreat! This output is much easier to deal with.\n\n\nCoefficient plot\nLet’s try and make a coefficient plot.\n\ntidy_reg %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot\"\n  )\n\n\n\n\nWe can see that relative to Adelie penguins (the base category), Gentoo penguins weigh more and Chinstrap penguins weigh less.\nFurther, male penguins weigh more than females.\n\n\nError bars\nTo get the error bars, we specify that we want a confidence interval when we use the tidy command from the broom package, like so: tidy(conf.int = TRUE)\n\ntidy_reg_conf_int &lt;- df %&gt;% \n    lm(body_mass_g ~ bill_length_mm + bill_depth_mm + flipper_length_mm + species + sex, data = .) %&gt;% \n    tidy(conf.int = TRUE)\n\ntidy_reg_conf_int\n\n# A tibble: 7 × 7\n  term              estimate std.error statistic  p.value conf.low conf.high\n  &lt;chr&gt;                &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;\n1 (Intercept)        -1461.     571.       -2.56 1.10e- 2 -2585.      -337. \n2 bill_length_mm        18.2      7.11      2.56 1.09e- 2     4.22      32.2\n3 bill_depth_mm         67.2     19.7       3.40 7.45e- 4    28.4      106. \n4 flipper_length_mm     16.0      2.91      5.48 8.44e- 8    10.2       21.7\n5 speciesChinstrap    -251.      81.1      -3.10 2.09e- 3  -411.       -92.0\n6 speciesGentoo       1015.     130.        7.83 6.85e-14   760.      1270. \n7 sexmale              390.      47.8       8.15 7.97e-15   296.       484. \n\n\nNow in our tibble we get columns called conf.low and conf.high.\nTo plot these, we use an additional geometry in our ggplot, called geom_errorbarh. Here the h at the end specifies we want it in the horizontal direction. We map the conf.low and conf.high variables to xmin and xmax respectively.\n\ntidy_reg_conf_int %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\"\n  )\n\n\n\n\nBecause none of the error bars cross the dotted line at zero, we conclude that the point estimates of the effects are significantly different from zero at the five percent level.\n\n\nCategories of predictor\nFinally, let’s facet the variables by type.\nWe use the case_when command from dplyr to call any variable containing mm a numeric variable, and the others categorical. We colour our bars and points by variable type, and facet_wrap to make a small multiple chart. This makes it easy to differentiate between the different types of variables.\n\ntidy_reg_conf_int %&gt;%\n  filter(term != \"(Intercept)\") %&gt;%\n  # create a type variable with case_when\n  mutate(type = case_when(\n    str_detect(term, \"mm\") ~ \"Numeric variable\",\n    TRUE ~ \"Categorical variable\"\n  )) %&gt;%\n  # reorder the coefficients so that the largest is at the top of the plot\n  mutate(term = fct_reorder(term, estimate)) %&gt;%\n  ggplot(aes(estimate, term, colour = type)) +\n  geom_point() +\n  geom_errorbarh(aes(xmin = conf.low, xmax = conf.high)) +\n  # add in a dotted line at zero\n  geom_vline(xintercept = 0, lty = 2) +\n  # remove the legend as the facet show that information\n  theme(legend.position = \"none\") +\n  # facet by type\n  facet_wrap(~type, scales = \"free_y\") +\n  labs(\n    x = \"Estimate of effect of variable on body mass (in grams)\",\n    y = NULL,\n    title = \"Coefficient plot with error bars\",\n    subtitle = \"By variable type\"\n  )\n\n\n\n\nIn this way we can see that the scaling of the variables can have an impact on how we perceive the results. Because the bill and flipper dimensions are measured in mm, and the body mass in grams, we are seeing the effect of an additional mm of bill depth, for example, on body mass in grams, which appears quite small.\n\n\nConclusion\nGreat - I hope that this was useful! It’s possible to easily customise your coefficient plots in ggplot, adding labels and colours to help your reader understand your regression results in a glance."
  },
  {
    "objectID": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "href": "posts/2023-01-20-chatgpt-cooks-me-dinner/chatgpt-cooks-me-dinner.html",
    "title": "ChatGPT cooks me dinner",
    "section": "",
    "text": "Intro\nI ask ChatGPT to help my cook dinner based on the ingredients I had in the fridge - it was a great success!\nHave a look below at how it went!\nI also asked it to write this blog post, the output for which I include below.\n\n\nProcess\n\n\n  \nVideo\n\n\n\n\n\n\n\n\nVideo\n\n\nSuggested blog post text\nCooking is always a fun and creative experience, but sometimes it can be challenging to come up with new and exciting recipes. That’s why having a helpful tool like ChatGPT can make a big difference in the kitchen.\nRecently, I had the opportunity to put ChatGPT to the test as a cooking assistant. I provided a list of ingredients that I had on hand and asked for a recipe suggestion. ChatGPT quickly came up with a recipe for a Gammon and Vegetable Stew with Mint, along with detailed instructions for cooking the dish.\nI was impressed with the speed and accuracy of ChatGPT’s suggestion, and even more impressed with the quality of the recipe. The stew turned out to be delicious, hearty, and comforting - exactly what I was looking for on a cold winter day.\nBut ChatGPT’s assistance didn’t stop there. I asked if I could add a splash of Chardonnay to the stew and ChatGPT not only gave me a positive response, but also provided instructions on when and how to add the wine, along with a reminder to use a good quality Chardonnay to enhance the overall taste of the dish.\nOverall, my experience with ChatGPT as a cooking assistant was extremely positive. It was able to provide me with a recipe and instructions that resulted in a delicious and satisfying meal. I was impressed with its speed, accuracy, and helpful suggestions, and I would definitely use ChatGPT again for recipe ideas and cooking instructions.\n\n\nConclusion\nWhat an amazing tool!"
  },
  {
    "objectID": "posts/2023-10-02-geocoding/geocoding.html#introduction",
    "href": "posts/2023-10-02-geocoding/geocoding.html#introduction",
    "title": "Geocoding: two ways",
    "section": "",
    "text": "Locating places of interest on a map is an important task in many economic history projects. This tutorial will show you how to geocode a list of addresses.\nI will use both R and python to show you how to do this, depending on your preference and what you are most comfortable with.\n\n\n\n\n\n\nA beautiful oil painting of a mosaic of a map or a city plan in the unique texture and color palette of Gustav Caillebotte"
  },
  {
    "objectID": "posts/2023-11-07-q-and-a/q-and-a.html",
    "href": "posts/2023-11-07-q-and-a/q-and-a.html",
    "title": "A modern question and answer site: SCB Users Group",
    "section": "",
    "text": "Emails limits knowledge to the sender and receivers. Researchers grappling with Statistics Sweden (SCB) datasets often exchange insights that vanish into inboxes. We are building the SCB Users Group to change that. It’s a platform where shared questions yield communal answers, accessible to everyone online.\n\n\n\n\n\n\nDigital artwork by ChatGPT with DALL·E 3, depicting a lively forum discussion, inspired by the detail-oriented style of painter Gustave Caillebotte.\n\n\n\n\n\n\nThe concept is simple: think StackOverflow, but with a focus on SCB data. Researchers ask, answer, and, in the process, craft a self-sustaining knowledge base.\nWe have built an alpha version and are looking to get a small amount of money for a custom domain and hosting costs.\nThe current version of the site is accessible here. I include a screenshot of the homepage below.\n\n\n\nHomepage\n\n\n\n\n\nWe chose Google Firebase Firestore for its simple storage and Firebase for authentication. The upshot? A secure, efficient backend without the headache of password management or the risk of exposing passwords inadvertently. There is also a generous free tier offered for storage on Firebase.\nThe Firestore nosql database has a simple set up with a collection for users with a unique id, usernames and emails. The questions collection has a unique id for each question, the information pertaining to each question like title and body and date, as well as a subcollection for the number of upvotes and id of the user who upvoted to prevent multiple upvotes from the same user. Each answer to the question belongs to a sub collection of the question called ‘answers’ in the nosql database.\n\n\n\nSvelteKit powers our frontend; its dynamic nature is perfect for crafting reactive user interfaces. SvelteFire slots let us tailor content based on user status, enhancing interaction without clutter.\nFor example, compare the below in which logged in users can upvote questions and answers and add the their own answers with the view of logged out users who can just view the content.\n\n\n\n\n\n\n\n(a) Logged in\n\n\n\n\n\n\n\n(b) Logged out\n\n\n\n\nFigure 1: Question page.\n\n\n\n\n\nThe interface is intuitive. If you’re logged in, you’re part of the conversation. If not, you’re still in the know.\n\n\n\nNetlify hosts our site, chosen for its scalability and future-proofing abilities. It’s a platform that grows with us, promising smooth custom domain integration down the line.\n\n\n\nMarkdown rendering is next on our list. It’s about enriching the discourse with equations, links, and well-formatted content.\n\n\n\nOur secret weapon during development? ChatGPT-4 with vision. It’s like a tireless coding partner, turning visual and textual prompts into Svelte components that click into place.\n\n\n\nThe project is public on GitHub in this repo. Feel free to add a comment in the discussions tab with a suggestion about how we can improve before launch."
  },
  {
    "objectID": "posts/2023-11-07-q-and-a/q-and-a.html#introduction",
    "href": "posts/2023-11-07-q-and-a/q-and-a.html#introduction",
    "title": "A modern question and answer site: SCB Users Group",
    "section": "",
    "text": "Emails limits knowledge to the sender and receivers. Researchers grappling with Statistics Sweden (SCB) datasets often exchange insights that vanish into inboxes. We are building the SCB Users Group to change that. It’s a platform where shared questions yield communal answers, accessible to everyone online.\n\n\n\n\n\n\nDigital artwork by ChatGPT with DALL·E 3, depicting a lively forum discussion, inspired by the detail-oriented style of painter Gustave Caillebotte.\n\n\n\n\n\n\nThe concept is simple: think StackOverflow, but with a focus on SCB data. Researchers ask, answer, and, in the process, craft a self-sustaining knowledge base.\nWe have built an alpha version and are looking to get a small amount of money for a custom domain and hosting costs.\nThe current version of the site is accessible here. I include a screenshot of the homepage below.\n\n\n\nHomepage\n\n\n\n\n\nWe chose Google Firebase Firestore for its simple storage and Firebase for authentication. The upshot? A secure, efficient backend without the headache of password management or the risk of exposing passwords inadvertently. There is also a generous free tier offered for storage on Firebase.\nThe Firestore nosql database has a simple set up with a collection for users with a unique id, usernames and emails. The questions collection has a unique id for each question, the information pertaining to each question like title and body and date, as well as a subcollection for the number of upvotes and id of the user who upvoted to prevent multiple upvotes from the same user. Each answer to the question belongs to a sub collection of the question called ‘answers’ in the nosql database.\n\n\n\nSvelteKit powers our frontend; its dynamic nature is perfect for crafting reactive user interfaces. SvelteFire slots let us tailor content based on user status, enhancing interaction without clutter.\nFor example, compare the below in which logged in users can upvote questions and answers and add the their own answers with the view of logged out users who can just view the content.\n\n\n\n\n\n\n\n(a) Logged in\n\n\n\n\n\n\n\n(b) Logged out\n\n\n\n\nFigure 1: Question page.\n\n\n\n\n\nThe interface is intuitive. If you’re logged in, you’re part of the conversation. If not, you’re still in the know.\n\n\n\nNetlify hosts our site, chosen for its scalability and future-proofing abilities. It’s a platform that grows with us, promising smooth custom domain integration down the line.\n\n\n\nMarkdown rendering is next on our list. It’s about enriching the discourse with equations, links, and well-formatted content.\n\n\n\nOur secret weapon during development? ChatGPT-4 with vision. It’s like a tireless coding partner, turning visual and textual prompts into Svelte components that click into place.\n\n\n\nThe project is public on GitHub in this repo. Feel free to add a comment in the discussions tab with a suggestion about how we can improve before launch."
  }
]